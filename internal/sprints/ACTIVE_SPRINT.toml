# Ã†therLight Unified Sprint: Protection + Self-Configuration + Template System
# Sprint Goal: Stabilize v0.16.7 + Build sprint template system + Self-configuration system
# Timeline: 8-10 weeks | Target: v1.0.0
# Created: 2025-11-05

[metadata]
version = "1.0.0"
sprint_number = 3
start_date = "2025-11-05"
target_completion = "2026-01-15"
status = "pending"

[metadata.focus]
primary = "Stabilize codebase with protection system + Build infrastructure for platform vision"
secondary = "Lock down stable code, build sprint template normalization, enable self-configuration"
tertiary = "Ship v1.0.0 with stable base + automation + adaptability"

[metadata.priority_order]
phase_0 = "Protection & Stabilization - v0.16.7 lockdown (BLOCKS ALL)"
phase_1 = "Build Sprint Template System - Normalization infrastructure (HIGH)"
phase_2 = "Foundation - Variable substitution & config generation (CRITICAL)"
phase_3 = "Detection - Tech stack, tools, workflow inference (HIGH)"
phase_4 = "Interview - CLI prompts & config generation (HIGH)"
phase_5 = "Migration - Upgrade command for existing users (MEDIUM)"
phase_6 = "Testing - Integration, E2E, performance, docs (MEDIUM)"
phase_7 = "Execute Normalization - AUTO-GENERATED by template system (FINAL)"

[metadata.progression]
previous_sprint = "archive/ACTIVE_SPRINT_v0.16.0_SPRINT_PLAN_UPDATE.toml"
next_sprint = "TBD"
triggers_hotfix_release = false
git_branch = "feature/unified-v1.0-sprint"

[metadata.team]
team_size = 1
default_engineer = "engineer_1"

[[metadata.team.engineers]]
id = "engineer_1"
name = "BB_Aelor"
expertise = ["typescript", "rust", "cli", "project-detection", "infrastructure"]
available_agents = ["infrastructure-agent", "api-agent", "documentation-agent", "testing-agent"]
max_parallel_tasks = 2
daily_capacity_hours = 8

[notes]
strategy = """
THREE-PHASE APPROACH:

Phase 0 (BLOCKING): Stabilize v0.16.7
- Manual test all features
- Lock down passing code (@protected, @immutable annotations)
- Build protection enforcement (pre-commit hooks)
- Fix failing features as bugs
- BLOCKS all other work until complete

Phase 1: Build Sprint Template System
- Create SPRINT_TEMPLATE.toml (normalized tasks)
- Enhance sprint-plan skill with template injection
- Update validation/enforcement
- TEST by generating Phase 7 tasks for THIS sprint

Phase 2-6: Self-Configuration System
- Platform vision: Ã†therLight adapts to ANY project
- Variable substitution, detection, interview, migration
- Enables community domain creation
- Ships software-dev domain initially

Phase 7 (AUTO-GENERATED): Execute Normalization
- sprint-plan skill injects normalized tasks
- Validates template system works
- Completes sprint with quality assurance

Constraints:
- Phase 0 BLOCKS Phase 1-6 (stable foundation required)
- Backward compatible (preserve user customizations)
- Performance (<5s init time, <200ms activation)
- TDD enforced (90% coverage for infrastructure)
"""

priority_order = """
Phase 0 is BLOCKING - nothing else starts until complete.
Phase 1 builds infrastructure used by Phase 7.
Phase 2-6 can run after Phase 0 (self-config work).
Phase 7 generated after Phase 1 complete (validates template system).
"""

# =============================================================================
# PHASE 0: PROTECTION & STABILIZATION (v0.16.7 lockdown) - BLOCKING
# =============================================================================

[tasks.PROTECT-000]
id = "PROTECT-000"
name = "Build task prompt export system for AI delegation"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "completed"
description = "Create terminal-based system to export TOML tasks as AI-enhanced prompts. Uses VS Code Terminal API to communicate with Claude Code (active session or launches new). AI analyzes current project state (git diff, completed tasks, modified files) and enhances TOML baseline with temporal context. No IPC/webhooks/files - pure terminal communication. Displays enhanced prompt in text area for user to copy to any external terminal/AI system."
estimated_lines = 600
estimated_time = "6-8 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "TaskPromptExporter.ts service (terminal-based communication with Claude Code)",
    "AI enhancement logic (analyzes git diff, completed tasks, temporal drift)",
    "Terminal detection (find active aetherlight/clod terminal)",
    "Terminal launcher (create terminal â†’ aetherlight â†’ clod â†’ wait for init)",
    "Output capture system (read terminal buffer until end marker)",
    "Integration with Start Task commands (shows AI-enhanced prompt before execution)",
    "TaskPromptViewer webview component (displays enhanced prompt)",
    "Copy to clipboard functionality"
]

performance_target = "Terminal detection < 50ms, AI enhancement < 3s, total flow < 5s"

patterns = ["Pattern-TDD-001", "Pattern-CODE-001"]

files_to_modify = [
    "vscode-lumina/src/services/TaskPromptExporter.ts",
    "vscode-lumina/src/commands/sprintTaskStarter.ts",
    "vscode-lumina/src/webview/components/TaskPromptViewer.tsx",
    "vscode-lumina/test/services/taskPromptExporter.test.ts"
]

validation_criteria = [
    "Run: npm test -- taskPromptExporter.test.ts",
    "Output: All tests passing (90% coverage)",
    "Check: TaskPromptExporter.exportTask(taskId) generates formatted prompt",
    "Check: Prompt includes all fields: description, why, context, reasoning_chain, validation_criteria, deliverables, success_impact",
    "Check: Markdown formatting with clear sections and hierarchy",
    "Check: Start Next Task command shows prompt in text area before execution",
    "Check: Start This Task command shows prompt in text area before execution",
    "Check: Copy to clipboard works (one-click copy)",
    "Check: User can review prompt before sending to external terminal/AI",
    "Check: Prompt format matches BACKUP.toml enriched format"
]

why = """
User needs to delegate tasks to external AI systems (terminals, APIs, other Claude sessions).
Currently: Rich TOML context only available to internal Claude Code agent.
Gap: No way to export that context for external use.
Solution: Generate comprehensive prompt from TOML that user can copy/paste anywhere.
Use cases: Parallel execution (multiple AIs), team delegation, review before execution, external tools.
Without this: User manually copies/formats task context (error-prone, incomplete).
With this: One-click export of complete task context in standard format.
"""

context = """
THE PROBLEM: Temporal Context Drift
Tasks written at sprint start (Day 1) become stale by execution time (Day 14+).
- Files change: New files created, existing files modified
- Tests added: Coverage increases, new test cases
- Dependencies shift: Completed tasks create new context
- Git commits: 15+ commits between task creation and execution

Static TOML (Day 1) doesn't reflect reality (Day 14).
AI executing task with stale context â†’ bugs, missed files, incorrect assumptions.

SOLUTION: AI-Enhanced Prompts with Current State Analysis
When user requests task export, AI analyzes:
1. TOML baseline (original intent from Day 1)
2. Current project state (Day 14 reality):
   - Git diff since task written
   - Completed tasks (new context from finished work)
   - New/modified files
   - Test results
3. Enhanced prompt includes:
   - Original task intent
   - "Changes since task written" section
   - Updated file recommendations
   - Completed dependencies impact

ARCHITECTURE: Terminal-Based Communication (Option D)

**Scenario 1: Claude Code Already Running**
```
User clicks "Start Next Task" in VS Code
â†“
Extension: vscode.window.terminals.find(t => t.name.includes('aetherlight'))
â†“
Found active terminal âœ…
â†“
Extension: terminal.sendText("export prompt for PROTECT-001")
â†“
I (Claude Code) receive command with FULL CONTEXT:
  - Read TOML baseline
  - Run git diff since task created
  - Scan for completed tasks
  - Check modified files
  - Generate AI-enhanced prompt
â†“
I output enhanced prompt to terminal (markdown with end marker)
â†“
Extension: captureTerminalOutput() until "--- END PROMPT ---"
â†“
Extension: Display in TaskPromptViewer text area
â†“
User: Review + copy to external terminal
```

**Scenario 2: Claude Code NOT Running**
```
User clicks "Start Next Task" in VS Code
â†“
Extension: No active aetherlight terminal found âŒ
â†“
Extension: vscode.window.createTerminal('Ã†therLight Task Export')
â†“
Extension: terminal.sendText('aetherlight')
â†“
Extension: waitForPrompt() until "Windsurf launched" detected
â†“
Extension: terminal.sendText('clod')
â†“
Extension: waitForPrompt() until "Claude Code initialized" detected
â†“
Extension: terminal.sendText("export prompt for PROTECT-001")
â†“
New Claude Code instance analyzes with fresh context
â†“
I output enhanced prompt
â†“
Extension: Capture and display
```

**Key: No IPC, No Webhooks, No Files**
- âœ… VS Code Terminal API (vscode.window.terminals)
- âœ… terminal.sendText() to send commands
- âœ… Terminal buffer capture for output
- âœ… Wait for initialization prompts
- âŒ NO file-based IPC
- âŒ NO WebSocket/HTTP
- âŒ NO temp files

**AI Enhancement Logic (What I Do)**
When I receive "export prompt for PROTECT-001":
1. Read TOML from ACTIVE_SPRINT.toml
2. Extract task.created_date or estimate from git log
3. Run: git diff {created_date}..HEAD
4. Scan completed tasks: grep status = "completed"
5. Check files_to_modify: git status on each file
6. Run tests if mentioned: npm test
7. Generate enhanced prompt:
   - Original TOML sections
   - NEW: "Changes Since Task Written (14 days ago)"
   - NEW: "Completed Tasks Impact"
   - NEW: "Recommended File Updates"
   - NEW: "Current Test Status"

**Enhanced Prompt Format**
```markdown
## ðŸ“‹ AI-Enhanced Task Prompt for PROTECT-001
Generated: 2025-11-19 (Task written: 2025-11-05, 14 days ago)

### âš ï¸ Changes Since Task Written
- 15 commits in 14 days
- 3 new files created: TaskPromptExporter.ts, TaskPromptViewer.tsx, PromptFormatter.ts
- 2 files modified: extension.ts (+150 lines), SprintLoader.ts (+75 lines)
- 1 task completed: PROTECT-000 (affects this task's scope)

### Task Metadata (Original)
[TOML baseline...]

### Current Project State
âœ… PROTECT-000 completed (adds new files to annotate)
â³ Git status: Clean working directory
â³ Tests: 85% coverage, all passing

### Recommended Updates
Original task specified 6 files. Recommend adding 3 new files:
- vscode-lumina/src/services/TaskPromptExporter.ts (NEW - review for annotation)
- vscode-lumina/src/webview/components/TaskPromptViewer.tsx (NEW)
- vscode-lumina/src/services/PromptFormatter.ts (NEW)

[Rest of original TOML sections...]
```

Related tasks:
- All 40 sprint tasks benefit from AI-enhanced export
- Solves temporal drift problem (stale context)
- Enables confident task delegation
"""

reasoning_chain = [
    "1. Write tests first (TDD RED phase): findAetherlightTerminal(), launchNewSession(), captureTerminalOutput(), enhancePrompt()",
    "2. Implement terminal detection: vscode.window.terminals.find(t => t.name.includes('aetherlight' or 'clod'))",
    "3. Implement terminal launcher: createTerminal() â†’ sendText('aetherlight') â†’ sendText('clod')",
    "4. Implement wait mechanism: waitForPrompt() polls terminal buffer for 'initialized' marker",
    "5. Implement command sender: terminal.sendText('export prompt for PROTECT-001')",
    "6. Implement output capture: read terminal buffer until '--- END PROMPT ---' marker",
    "7. Design AI enhancement command interface (what I receive in terminal)",
    "8. Implement project state analyzer (git diff, completed tasks, modified files, test results)",
    "9. Implement AI prompt generation logic (baseline TOML + current state â†’ enhanced prompt)",
    "10. Add temporal drift detection (compare task.created_date to today, count days elapsed)",
    "11. Add 'Changes Since Task Written' section generator",
    "12. Add 'Completed Tasks Impact' analyzer (scan dependencies, check if fulfilled)",
    "13. Add 'Recommended Updates' generator (compare files_to_modify to current files)",
    "14. Implement end marker output ('--- END PROMPT ---' for capture)",
    "15. Create TaskPromptViewer webview component (React)",
    "16. Integrate with sprintTaskStarter.ts (intercept Start Next Task button)",
    "17. Test Scenario 1: Active terminal exists â†’ command sent â†’ prompt captured",
    "18. Test Scenario 2: No terminal â†’ launch â†’ wait â†’ command sent â†’ prompt captured",
    "19. Test AI enhancement: Create fake task, modify files, verify 'Changes Since' section",
    "20. Test with PROTECT-001 (first real task, verify all sections present)",
    "21. Verify copy-to-clipboard works (one-click copy)",
    "22. Document usage in CLAUDE.md (how to use Start Next Task with AI enhancement)"
]

success_impact = """
After PROTECT-000 complete:
âœ… Terminal-based communication with Claude Code (active session or new launch)
âœ… AI-enhanced prompts with temporal drift detection
âœ… "Changes Since Task Written" analysis (git diff, completed tasks, modified files)
âœ… Recommended file updates based on current project state
âœ… Start Next Task triggers AI enhancement automatically
âœ… Start This Task triggers AI enhancement automatically
âœ… TaskPromptViewer displays enhanced prompt in text area
âœ… Copy to clipboard enables delegation to external terminals
âœ… No IPC/webhooks/files complexity (pure terminal API)
âœ… All 40 sprint tasks exportable with living context

Enables:
- Temporal context awareness (tasks adapt to project changes)
- Confident task delegation (AI gets current reality, not stale Day 1 context)
- Parallel execution (multiple AIs with synchronized context)
- Bug prevention (AI aware of completed work, new files, modifications)
- Quality assurance (user validates AI understanding before execution)
- Transparency (user sees enhanced prompt before task starts)
- Reduces hallucination risk (AI has complete current picture)
"""

[tasks.PROTECT-001]
id = "PROTECT-001"
name = "Annotate passing code with protection levels"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Review v0.16.7 manual test results, annotate passing features with @protected/@immutable, create bug tasks for failures"
estimated_lines = 200
estimated_time = "2-4 hours"
dependencies = ["PROTECT-000"]
agent = "infrastructure-agent"
deliverables = [
    "All passing code annotated with protection levels",
    "CODE_PROTECTION_POLICY.md updated with locked file list",
    "Bug tasks created for failing features (Phase 0b UX Polish)"
]
performance_target = "Annotation completes < 4 hours"
patterns = ["Pattern-TRACKING-001"]
files_to_modify = [
    "vscode-lumina/src/extension.ts (add @protected annotation)",
    "vscode-lumina/src/commands/SprintLoader.ts (add @protected annotation)",
    "vscode-lumina/src/services/AutoTerminalSelector.ts (add @protected)",
    "vscode-lumina/src/services/TaskDependencyValidator.ts (add @protected)",
    "vscode-lumina/src/services/whisperClient.ts (add @immutable)",
    "vscode-lumina/src/commands/voicePanel.ts (add @protected to passing sections)",
    "docs/CODE_PROTECTION_POLICY.md (document locked files)"
]
validation_criteria = [
    "All passing features from MANUAL_TEST_v0.16.7.md annotated",
    "Protection levels correct (@immutable for APIs, @protected for features)",
    "CODE_PROTECTION_POLICY.md lists all locked files",
    "Bug tasks created for all failing features",
    "Each annotation includes lock date and test references"
]
why = """
User will manually test v0.16.7 and identify passing features.
Passing code gets locked down to prevent regression.
Protection levels: @immutable (never change), @protected (refactor only), @maintainable (bug fixes allowed).
Historical precedent: v0.13.23 native dependency broke extension (9 hours wasted).
"""
context = """
Manual testing completed 2025-11-05. Results documented in MANUAL_TEST_v0.16.7.md.

PASSING FEATURES (Lock Down with @protected):

1. Core Extension (vscode-lumina/src/)
   - extension.ts â†’ @protected (activation, command registration)
   - Auto-refresh FileSystemWatcher â†’ @protected (lines 891-947)

2. Sprint Management (vscode-lumina/src/commands/)
   - SprintLoader.ts â†’ @protected (TOML parsing, rich fields - FIXED)
   - SprintLoader.ts:504-534 â†’ @protected (parseTomlTasks with rich fields)

3. Services (vscode-lumina/src/services/)
   - AutoTerminalSelector.ts â†’ @protected (terminal integration)
   - TaskDependencyValidator.ts â†’ @protected (dependency blocking - BRILLIANT!)
   - whisperClient.ts â†’ @immutable (voice transcription API)

4. Voice Panel (vscode-lumina/src/commands/)
   - voicePanel.ts (partial):
     - Terminal list/dropdown logic â†’ @protected
     - Sprint dropdown â†’ @protected
     - Sprint refresh button â†’ @protected
     - Skills browser â†’ @protected (Test 5.5)
     - Settings UI â†’ @protected (Test 5.6)

5. WebView UI
   - Single unified screen (no tabs) â†’ @protected
   - Minimal spacing layout â†’ @protected

FAILING FEATURES will be added as UX Polish tasks (Phase 0b - after Template System).
"""
reasoning_chain = [
    "1. Review MANUAL_TEST_v0.16.7.md test results",
    "2. Identify all passing features (32 tests passed)",
    "3. For each passing feature, determine protection level:",
    "   - whisperClient.ts: @immutable (API contract)",
    "   - SprintLoader.ts: @protected (core feature)",
    "   - AutoTerminalSelector.ts: @protected (core feature)",
    "4. Add file-level annotations:",
    "   // @protected - Core functionality, refactor only",
    "   // Locked: 2025-11-05 (v0.16.7 manual test PASS)",
    "   // Tests: [test numbers]",
    "5. Document in CODE_PROTECTION_POLICY.md:",
    "   - List all locked files with protection levels",
    "   - Reference test numbers that verify functionality",
    "6. Create bug tasks for failing features:",
    "   - Test 2.5 FAIL â†’ UX-002 (enhance button broken)",
    "   - Test 4.6 FAIL â†’ UX-001 (task starter broken)",
    "   - Test 5.1-5.4 FAIL â†’ UX-001 (all enhancement buttons)",
    "7. Protection system prevents future regressions",
    "8. Pre-commit hook will enforce in PROTECT-002"
]
success_impact = """
After PROTECT-001 complete:
âœ… All passing code annotated with protection levels
âœ… CODE_PROTECTION_POLICY.md documents locked files
âœ… Bug tasks created for all failing features
âœ… Clear distinction between stable and broken code
âœ… Foundation for pre-commit hook enforcement (PROTECT-002)
âœ… Prevents repeat of v0.13.23 disaster (9 hours wasted)
âœ… Future developers know which code is locked
âœ… Audit trail of what works in v0.16.7
"""

[tasks.PROTECT-002]
id = "PROTECT-002"
name = "Build pre-commit protection enforcement"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create validate-protection.js script, enhance pre-commit hook to block protected code changes without explicit approval"
estimated_lines = 150
estimated_time = "2-3 hours"
dependencies = ["PROTECT-001"]
agent = "infrastructure-agent"
deliverables = [
    "scripts/validate-protection.js created and tested",
    "Pre-commit hook updated with protection enforcement",
    "Protection enforcement tested (6 scenarios)",
    "Audit trail in git log verified",
    "CI mode working (SKIP_PROTECTION_CHECK)"
]
performance_target = "Protection check completes < 200ms"
patterns = ["Pattern-CODE-PROTECTION-001", "Pattern-GIT-001"]
files_to_modify = [
    ".git/hooks/pre-commit (add protection enforcement)",
    "scripts/validate-protection.js (NEW - protection scanner)",
    "docs/CODE_PROTECTION_POLICY.md (document enforcement)"
]
validation_criteria = [
    "Modify @protected file â†’ Prompt shown",
    "Answer 'no' â†’ Commit blocked",
    "Answer 'yes' â†’ Commit succeeds with PROTECTED: message",
    "Modify non-protected file â†’ No prompt",
    "Git log shows approval trail",
    "CI mode works without prompts"
]
why = """
Annotations mean nothing without enforcement.
Pre-commit hook must block changes to @protected/@immutable files without approval.
Historical bug: v0.13.23 native dependency broke extension (9 hours to fix).
Protection system prevents repeat of costly regressions.
"""
context = """
Current state: Code annotations exist but no enforcement mechanism.
Developers can freely modify @protected files without warnings.

Problem: Without enforcement, protection system is aspirational only.
- @immutable files can be changed (breaks critical APIs)
- @protected files can have behavior changes (breaks locked functionality)
- No audit trail of who approved protected modifications

Solution: Git pre-commit hook with validation script.
- Scans staged files for @protected/@immutable annotations
- Prompts for explicit approval if protected code modified
- Logs approval in git commit message (audit trail)
- Blocks commit if approval denied
"""
reasoning_chain = [
    "1. Developer modifies SprintLoader.ts (marked @protected)",
    "2. Developer runs: git add . && git commit -m 'fix bug'",
    "3. Pre-commit hook triggers",
    "4. validate-protection.js scans staged files",
    "5. Finds SprintLoader.ts has @protected annotation",
    "6. Hook shows warning: 'âš ï¸  Protected file: SprintLoader.ts'",
    "7. Hook prompts: 'Modify protected code? (yes/no)'",
    "8. If 'no' â†’ Exit 1, commit blocked, file unstaged",
    "9. If 'yes' â†’ Add approval to commit message, commit proceeds",
    "10. Git log shows approval trail for auditing"
]
success_impact = """
After PROTECT-002 complete:
âœ… Cannot accidentally modify @protected/@immutable files
âœ… Explicit approval required for protected changes
âœ… Git log contains audit trail of approvals
âœ… Prevents repeat of v0.13.23 disaster (9 hours wasted)
âœ… Protection system enforced, not aspirational
âœ… Works with existing pre-commit checks (tests, TOML validation)
âœ… CI mode allows automated commits (SKIP_PROTECTION_CHECK)
"""

[tasks.PROTECT-003]
id = "PROTECT-003"
name = "Update CODE_PROTECTION_POLICY.md with actual state"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Transform CODE_PROTECTION_POLICY.md from aspirational to actual: add ACTIVE status banner, list all protected files, document enforcement mechanisms and override process"
estimated_lines = 400
estimated_time = "1-2 hours"
dependencies = ["PROTECT-001", "PROTECT-002"]
agent = "documentation-agent"
deliverables = [
    "CODE_PROTECTION_POLICY.md updated to ACTIVE status",
    "All protected files listed with lock dates and test references",
    "Enforcement mechanisms documented (pre-commit hook, validation script)",
    "Override process documented with clear examples",
    "Audit trail section added (git log commands)",
    "CI/CD integration documented"
]
performance_target = "Documentation update completes < 2 hours"
patterns = ["Pattern-DOCS-001"]
files_to_modify = [
    "docs/CODE_PROTECTION_POLICY.md (complete rewrite - aspirational â†’ actual)"
]
validation_criteria = [
    "STATUS banner shows âœ… ACTIVE",
    "All @protected files from PROTECT-001 are listed",
    "Pre-commit enforcement clearly explained",
    "Override process has examples",
    "Git log commands work",
    "Developers understand what's protected and why"
]
why = """
Existing CODE_PROTECTION_POLICY.md is aspirational (describes desired state, not actual).
Without accurate documentation, developers don't know what's protected or how enforcement works.
Need single source of truth for protection system.
"""
context = """
Current state: CODE_PROTECTION_POLICY.md exists but is aspirational.
- Lists files that SHOULD be protected (not actual protected files)
- Describes enforcement that DOESN'T YET EXIST
- No reference to pre-commit hooks or validation scripts
- Confuses developers ("Is this real or planned?")

After PROTECT-001 and PROTECT-002:
- Protection annotations applied to real files
- Pre-commit enforcement working
- Need to document ACTUAL system state

Problem: Developers need to know:
1. Which files are protected (and at what level)
2. How protection is enforced (pre-commit hook)
3. How to request override approval
4. What happens if they modify protected files
"""
reasoning_chain = [
    "1. PROTECT-001 annotates files with @protected/@immutable",
    "2. PROTECT-002 builds enforcement (pre-commit hook)",
    "3. Developer needs documentation to understand system",
    "4. PROTECT-003 updates CODE_PROTECTION_POLICY.md with reality",
    "5. Document lists ALL protected files (from PROTECT-001)",
    "6. Document explains enforcement (how pre-commit works)",
    "7. Document provides override process (when/how)",
    "8. Document includes examples (commit messages, git commands)",
    "9. Developer can now understand: What's protected, How it works, What to do",
    "10. Protection system is complete: Annotations + Enforcement + Documentation"
]
success_impact = """
After PROTECT-003 complete:
âœ… Developers know exactly which files are protected
âœ… Clear documentation of enforcement mechanism
âœ… Override process documented with examples
âœ… Audit trail accessible via git log
âœ… Protection system is complete (not aspirational)
âœ… Single source of truth for protection policy
âœ… Reduces confusion and support questions
âœ… Protection system ready for production use
"""

# Post-Release Cleanup Tasks (from v0.16.7)

[tasks.POST-001]
id = "POST-001"
name = "Fix .vscodeignore bloat for v0.16.8"
status = "completed"
phase = "phase-0-protection"
agent = "infrastructure-agent"

why = """
v0.16.7 shipped with 18MB+ of test cache bloat in npm package.
Fixed in commit 48ce25b.
"""

estimated_time = "COMPLETED"
dependencies = []

deliverables = [
    ".vscodeignore updated with explicit patterns",
    "Next release will exclude test artifacts"
]

[tasks.POST-002]
id = "POST-002"
name = "Fix analyzer package test failures (36 tests)"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Investigate and fix 36 failing tests in @aetherlight/analyzer package.

Test Failures:
- ComplexityAnalyzer (2 failures)
- RustParser (4 failures)
- TypeScriptParser (1 failure - performance)
- SprintGenerator (29 failures - debtAnalysis.issues undefined)

Investigation Plan:
1. Run tests locally, capture full error output
2. Git bisect to find regression commit
3. Fix each category systematically (highest impact first)
4. Add regression tests for each fix
5. Verify performance targets met
"""
estimated_lines = 300
estimated_time = "4-6 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "All 36 tests passing",
    "Root cause analysis documented (in commit message or CLAUDE.md)",
    "Regression tests added (10 new tests minimum)",
    "Performance benchmark verified (<2s)",
    "Test coverage report (â‰¥90%)"
]

performance_target = "All tests complete in <30s, TypeScriptParser benchmark <2s"

patterns = ["Pattern-TDD-001", "Pattern-TRACKING-001"]

files_to_modify = [
    "packages/aetherlight-analyzer/src/services/SprintGenerator.ts (debtAnalysis initialization)",
    "packages/aetherlight-analyzer/src/parsers/RustParser.ts (syntax handling)",
    "packages/aetherlight-analyzer/src/parsers/TypeScriptParser.ts (performance optimization)",
    "packages/aetherlight-analyzer/src/analyzers/ComplexityAnalyzer.ts (metric calculations)",
    "packages/aetherlight-analyzer/test/**/*.test.ts (regression tests)"
]

validation_criteria = [
    "Run: cd packages/aetherlight-analyzer && npm test",
    "Output: 0 failures, all tests pass",
    "Run: npm run coverage",
    "Output: Coverage â‰¥90% for all affected files",
    "Run performance benchmark: npm run benchmark:parser",
    "Output: TypeScriptParser completes 10k LOC in <2s",
    "Check git commit message: Contains root cause analysis",
    "Verify regression tests added: 10+ new test cases",
    "TDD: All 36 existing tests must pass",
    "TDD: Add regression tests for SprintGenerator (3 tests)",
    "TDD: Add regression tests for RustParser (4 tests)",
    "TDD: Add regression tests for TypeScriptParser (1 test)",
    "TDD: Add regression tests for ComplexityAnalyzer (2 tests)",
    "TDD: Coverage â‰¥90% for affected services"
]

why = """
36 failing tests indicate real regressions in analyzer functionality.
Must fix before starting new feature work to prevent cascading failures.
Pattern-TDD-001 enforcement: Tests are the ratchet preventing subtle breakage.
Historical precedent: Skipping test fixes led to 4-9 hour debug sessions.
"""

context = """
Test failures discovered during v0.16.7 testing:
- ComplexityAnalyzer (2 failures) - likely metric calculation changes
- RustParser (4 failures) - possibly syntax pattern updates
- TypeScriptParser (1 failure - performance) - <2s target exceeded
- SprintGenerator (29 failures) - debtAnalysis.issues undefined (most critical)

Failure pattern suggests:
1. SprintGenerator failures share root cause (undefined property access)
2. Parser failures may be integration issues with updated patterns
3. Performance regression in TypeScriptParser needs profiling

Priority: SprintGenerator (29 tests) â†’ Parsers (6 tests) â†’ ComplexityAnalyzer (2 tests)
"""

reasoning_chain = [
    "1. Developer runs: cd packages/aetherlight-analyzer && npm test",
    "2. Test output shows 36 failures across 4 categories",
    "3. Focus on SprintGenerator (29 failures, highest impact):",
    "   - Error: 'Cannot read property issues of undefined'",
    "   - Root cause: debtAnalysis object not initialized properly",
    "   - Fix: Add null check or initialize debtAnalysis.issues = []",
    "4. Fix parser failures (RustParser, TypeScriptParser):",
    "   - Check for pattern library changes",
    "   - Update parser integration tests",
    "   - Verify syntax handling still correct",
    "5. Fix ComplexityAnalyzer failures:",
    "   - Check metric calculation changes",
    "   - Update expected values if algorithm improved",
    "6. Profile TypeScriptParser performance:",
    "   - Identify bottleneck (likely AST traversal)",
    "   - Add caching or optimize hot path",
    "   - Verify <2s target met",
    "7. Add regression tests for each fix",
    "8. Run full test suite: npm test -- --coverage",
    "9. Verify 90% coverage maintained",
    "10. Commit with detailed root cause analysis"
]

success_impact = """
After POST-002 complete:
âœ… All 36 analyzer tests passing
âœ… SprintGenerator working correctly (unblocks sprint creation)
âœ… Parser integration verified (Rust, TypeScript)
âœ… Performance targets met (<2s for 10k LOC)
âœ… Regression tests prevent reoccurrence
âœ… 90% test coverage maintained
âœ… Confidence in analyzer package restored
âœ… Can safely proceed with new feature work
"""

[tasks.POST-003]
id = "POST-003"
name = "Update publish script - remove outdated sub-package logic"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Remove outdated sub-package publishing logic from scripts/publish-release.js.

Architecture changed:
- Sub-packages now bundled as file: dependencies
- Listed in bundledDependencies in vscode-lumina/package.json
- Packaged WITH .vsix, NOT published separately to npm

Fix:
1. Remove sub-package publishing loop (lines 448-478)
2. Only publish main aetherlight package to npm
3. Update verification logic to check bundledDependencies
4. Add comment explaining architecture change
5. Test on Windows (where script failed)
6. Verify .vsix includes sub-package code
"""
estimated_lines = 150
estimated_time = "1-2 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "Publish script updated (lines 448-478 removed)",
    "Verification logic updated to check bundledDependencies",
    "Comment added explaining architecture change",
    "Script tested on Windows (dry-run + full publish)",
    "Automation restored (Pattern-PUBLISH-002 compliant)"
]

performance_target = "Publish script completes in <5 minutes (excluding npm publish wait time)"

patterns = ["Pattern-PUBLISH-001", "Pattern-PUBLISH-002"]

files_to_modify = [
    "scripts/publish-release.js (remove lines 448-478, update verification 500-530)",
    "CLAUDE.md (update Known Issues section if needed)",
    ".claude/skills/publish/SKILL.md (verify instructions still accurate)"
]

validation_criteria = [
    "Run: node scripts/publish-release.js patch --dry-run",
    "Output: No errors, completes successfully",
    "Output: Does NOT attempt sub-package publishing",
    "Run: npm run package",
    "Output: .vsix created successfully",
    "Run: unzip -l aetherlight-*.vsix | grep aetherlight-analyzer",
    "Output: Sub-package files present in .vsix",
    "Check: scripts/publish-release.js lines 448-478",
    "Output: Lines removed or commented out with explanation",
    "Test on Windows: Same commands should work",
    "TDD: Dry-run test completes without errors",
    "TDD: Verification test - vsce package succeeds",
    "TDD: Windows test - script works without path errors",
    "TDD: Full publish test - end-to-end flow verified"
]

why = """
Publish script has outdated logic that fails on current architecture.
Sub-packages now bundled as file: dependencies, not published separately to npm.
Pattern-PUBLISH-002 enforcement: Automated publishing prevents manual errors.
Script must match current monorepo architecture to restore automation.
"""

context = """
Architecture evolution:
- OLD (v0.13.29): Sub-packages published separately to npm
  - aetherlight-analyzer@npm
  - aetherlight-sdk@npm
  - aetherlight-node@npm
  - Main package depends on npm versions

- NEW (v0.16.0+): Sub-packages bundled with .vsix
  - "file:../packages/aetherlight-analyzer" in package.json
  - Listed in bundledDependencies array
  - Packaged WITH .vsix, NOT on npm

Publish script still has OLD logic (lines 448-478):
- Loops through sub-packages
- Tries to npm publish each one
- Fails because file: dependencies can't be published to npm
- Blocks entire release automation

Reference: Pattern-PUBLISH-001 (Automated Release Pipeline)
Related bug: v0.13.29 sub-package mismatch (2-hour fix)
"""

reasoning_chain = [
    "1. Developer runs: node scripts/publish-release.js patch",
    "2. Script reaches sub-package publishing section (lines 448-478)",
    "3. Script tries: cd packages/aetherlight-analyzer && npm publish",
    "4. npm errors: 'Cannot publish file: dependencies'",
    "5. Script fails, release blocked",
    "6. Fix: Remove lines 448-478 (entire sub-package loop)",
    "7. Add comment: '# Sub-packages bundled in .vsix, not published separately'",
    "8. Update verification (lines 500-530):",
    "   - Check bundledDependencies in package.json",
    "   - Verify sub-package code exists in packages/",
    "   - Don't verify npm versions (they're not on npm)",
    "9. Test: node scripts/publish-release.js patch --dry-run",
    "10. Verify: vsce package && unzip -l aetherlight-*.vsix | grep aetherlight-analyzer",
    "11. Commit: 'fix(publish): remove outdated sub-package publishing logic'"
]

success_impact = """
After POST-003 complete:
âœ… Publish script works on Windows
âœ… Automated publishing restored (Pattern-PUBLISH-002)
âœ… No manual steps required for releases
âœ… Sub-packages correctly bundled in .vsix
âœ… No more file: dependency errors
âœ… Script matches current monorepo architecture
âœ… Can safely publish v0.16.8 and beyond
âœ… Prevents manual publishing mistakes
"""

[tasks.POST-004]
id = "POST-004"
name = "Add publishing order enforcement to prevent protocol violations"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Add enforcement to ensure git push happens BEFORE npm publish.

Add confirmation step to publish script (after line 428):
- After GitHub release created successfully
- Show GitHub release URL for user verification
- Show .vsix artifact URL (verify attachment)
- Prompt: "GitHub release created with .vsix. Ready to publish to npm? (yes/no)"
- If no â†’ Exit script, cancel npm publish
- If yes â†’ Proceed with npm publish

Additional safeguards:
1. Verify GitHub release exists before prompting
2. Verify .vsix artifact attached to release
3. Show both URLs for manual verification
4. Allow user to cancel if anything looks wrong
5. Log checkpoint to console for audit trail
"""
estimated_lines = 80
estimated_time = "1-2 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "Confirmation checkpoint added to publish script (after line 428)",
    "GitHub release verification logic (gh release view)",
    ".vsix artifact verification (check asset URL)",
    "User prompt with URL display",
    "Abort logic (exit on 'no')",
    "Audit logging (checkpoint passed/failed)",
    "Pattern-PUBLISH-001 compliance enforced"
]

performance_target = "Checkpoint verification completes in <5s (gh release view API call)"

patterns = ["Pattern-PUBLISH-001", "Pattern-PUBLISH-002"]

files_to_modify = [
    "scripts/publish-release.js (add checkpoint after line 428)",
    "CLAUDE.md (update Publishing Enforcement section with new safeguard)"
]

validation_criteria = [
    "Run: node scripts/publish-release.js patch --dry-run",
    "Output: Shows GitHub release URL + .vsix URL",
    "Output: Prompts 'Ready to publish to npm? (yes/no)'",
    "Type: 'no'",
    "Output: 'npm publish cancelled, exiting'",
    "Verify: Script exits with code 0",
    "Run again: Type 'yes'",
    "Output: Proceeds to npm publish",
    "Check: Audit log shows 'GitHub release verified'",
    "Test failure scenario: Simulate missing .vsix",
    "Output: 'ERROR: .vsix not attached to release, aborting'",
    "TDD: Dry-run abort scenario - exits without npm publish",
    "TDD: Success scenario - proceeds to npm publish on 'yes'",
    "TDD: Failure recovery - exits before npm publish",
    "TDD: Verification logic - detects missing .vsix"
]

why = """
v0.16.7 violated Pattern-PUBLISH-001 by publishing npm BEFORE git push.
Correct order: Git push â†’ GitHub release â†’ npm publish.
Violation causes: Users install from npm but can't download .vsix from GitHub.
Need enforcement mechanism to prevent protocol violations.
Pattern-PUBLISH-001 compliance: GitHub release MUST exist before npm publish.
"""

context = """
Publishing Order Protocol (Pattern-PUBLISH-001):
1. Git commit â†’ local changes saved
2. Git tag â†’ version marked
3. Git push â†’ code on GitHub
4. GitHub release â†’ tag + .vsix artifact uploaded
5. npm publish â†’ package on npm registry

Why this order matters:
- npm package depends on GitHub release URL for .vsix download
- CLI installer (aetherlight.exe) downloads from GitHub releases
- If npm published first, users get version that can't download .vsix
- Creates "phantom version" - exists on npm but not functional

v0.16.7 violation:
- npm published successfully
- GitHub release failed (manual recovery required)
- Users saw v0.16.7 on npm but couldn't install it
- Had to manually create GitHub release after npm publish

Root cause:
- No checkpoint between GitHub release and npm publish
- Script assumes GitHub release will succeed
- Doesn't verify .vsix attachment before npm publish
"""

reasoning_chain = [
    "1. Script runs through normal flow:",
    "   - Compile TypeScript",
    "   - Run tests",
    "   - Bump version",
    "   - Git commit + tag",
    "   - Git push",
    "2. Script creates GitHub release (line 428):",
    "   - gh release create v0.16.8",
    "   - Upload .vsix artifact",
    "   - GitHub release URL returned",
    "3. NEW CHECKPOINT (after line 428):",
    "   - Verify GitHub release exists: gh release view v0.16.8",
    "   - Extract .vsix asset URL from release JSON",
    "   - Display to user:",
    "     'âœ… GitHub release created: https://github.com/...'",
    "     'âœ… .vsix attached: https://github.com/.../aetherlight-0.16.8.vsix'",
    "   - Prompt: 'Ready to publish to npm? (yes/no)'",
    "4. User verifies URLs in browser (optional)",
    "5a. User types 'yes' â†’ Script proceeds to npm publish",
    "5b. User types 'no' â†’ Script exits, npm publish cancelled",
    "6. If 'yes': npm publish runs",
    "7. Audit log: 'GitHub release verified before npm publish'",
    "8. Pattern-PUBLISH-001 compliance enforced âœ…"
]

success_impact = """
After POST-004 complete:
âœ… Cannot publish npm without GitHub release
âœ… User manually verifies GitHub release before npm publish
âœ… .vsix attachment verified before npm publish
âœ… Pattern-PUBLISH-001 compliance enforced
âœ… Prevents v0.16.7 bug (npm without GitHub release)
âœ… Users never see phantom versions (npm without .vsix)
âœ… CLI installer always works (GitHub release exists)
âœ… Manual checkpoint allows cancellation if issues detected
âœ… Audit trail logged for troubleshooting
"""

[tasks.POST-005]
id = "POST-005"
name = "Fix automated publish script to handle package architecture changes"
phase = "phase-0-protection"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Fix the automated publish script (scripts/publish-release.js) to handle the package architecture changes introduced in v0.16.15.

v0.16.15 required manual bypass due to three automation failures:
1. Missing @types/mocha in devDependencies caused TypeScript compilation errors
2. Import paths still using old scoped names (@aetherlight/*) after package rename
3. Version handling: Script bumped to 0.16.15 when 0.16.14 was expected

Root causes:
- Pre-publish compilation check doesn't validate devDependencies completeness
- No validation that import paths match package names
- No graceful handling when version already at target
- Manual intervention required (Pattern-PUBLISH-002 bypass needed)

Fix requirements:
1. Add devDependencies validation step (check for @types/mocha, @types/node)
2. Add import path consistency check (grep for old @aetherlight/* imports)
3. Integrate pre-publish-check.js validation (created in v0.16.15)
4. Better version handling (detect already-bumped versions, offer options)
5. Improve error messages (explain WHY compilation failed, suggest fixes)

Success criteria:
- Publish v0.16.16 using ONLY automated script (no manual bypass)
- All 7 pre-publish checks pass automatically
- TypeScript compiles without errors
- All 4 packages publish successfully
- GitHub release created with .vsix + installers
"""
estimated_lines = 250
estimated_time = "3-4 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "scripts/publish-release.js enhanced with pre-publish validation integration",
    "devDependencies completeness check (ensure @types/* present)",
    "Import path consistency validator (detect @aetherlight/* usage)",
    "Version handling improvements (detect already-bumped, offer reset/continue)",
    "Error messages enhanced with actionable suggestions",
    "Test: Successfully publish v0.16.16 with zero manual intervention",
    "Documentation: Update CLAUDE.md Publishing Enforcement section"
]

performance_target = "Pre-publish validation completes in <10s (all 7 checks)"

patterns = ["Pattern-PUBLISH-002", "Pattern-PUBLISH-004", "Pattern-PKG-001"]

files_to_modify = [
    "scripts/publish-release.js (add validation hooks)",
    "scripts/pre-publish-check.js (may need enhancements)",
    ".claude/CLAUDE.md (update known issues, remove v0.16.15 manual bypass note)"
]

validation_criteria = [
    "Run: node scripts/publish-release.js patch (from v0.16.15 state)",
    "Output: All 7 pre-publish checks execute and pass",
    "Output: '@types/mocha present in devDependencies' âœ…",
    "Output: 'No legacy @aetherlight/* imports found' âœ…",
    "Output: 'Version sync validated across 4 packages' âœ…",
    "Output: TypeScript compilation succeeds without errors",
    "Output: All 4 packages publish to npm successfully",
    "Output: GitHub release created with all assets",
    "Verify: v0.16.16 published without any manual intervention",
    "Verify: No manual file edits required during publish",
    "Verify: No manual npm publish commands needed",
    "Test: Simulate missing @types/mocha â†’ Script catches it and suggests fix",
    "Test: Simulate old import path â†’ Script catches it and suggests fix"
]

why = """
v0.16.15 required manual bypass (violating Pattern-PUBLISH-002) due to automation gaps.
Manual bypasses waste 2+ hours per release and introduce human error risk.

Historical impact:
- v0.16.15: Manual bypass, forgot desktop installers initially (2 hour fix)
- v0.13.28: Manual bypass, version mismatch (2 hour fix)
- v0.13.29: Manual bypass, missed sub-package publish (2 hour fix)
- Total wasted time: 6+ hours across recent releases

Pattern-PUBLISH-002 states: "ALWAYS attempt automated script first, only bypass with user approval after explaining risks."

The automation exists to prevent these bugs. If the automation has gaps, we must FIX the automation, not normalize manual bypasses.

This task ensures v0.16.16+ can publish without manual intervention.
"""

context = """
Background - Package Architecture Changes (v0.16.13-0.16.15):
The project migrated from scoped packages (@aetherlight/*) to unscoped (aetherlight-*).
This required changes across multiple files, and the automated publish script didn't catch all issues.

v0.16.15 Manual Bypass Timeline:
1. Started automated publish script
2. TypeScript compilation failed: "Cannot find name 'suite'. Need @types/mocha"
3. Manually added @types/mocha to devDependencies
4. Compilation failed again: "Cannot find module '@aetherlight/analyzer'"
5. Manually fixed import paths from scoped to unscoped
6. Version was already at 0.16.15 (previous failed attempt bumped it)
7. User chose "continue with 16.15" instead of resetting to 0.16.14
8. Manual publish completed successfully with all 4 packages

Files created during manual bypass (Pattern-PUBLISH-004):
- scripts/pre-publish-check.js: Validates 7 critical checks
  1. Version sync across 4 packages
  2. Unscoped package names (@aetherlight/* â†’ aetherlight-*)
  3. Dependency references correct
  4. No native dependencies
  5. No forbidden runtime npm dependencies
  6. Git working directory clean
  7. Analyzer tests passing

- .release-notes.tmp: Documents package architecture fixes
  - Package naming changes
  - Dependency bundling fixes
  - TypeScript compilation fixes
  - Breaking change warnings (import path updates)

Automation gaps identified:
1. No devDependencies completeness check
   - Missing: Validate @types/mocha, @types/node present
   - Impact: TypeScript compilation fails with cryptic errors

2. No import path consistency check
   - Missing: Grep for old @aetherlight/* patterns
   - Impact: Compilation fails after package rename

3. No integration of pre-publish-check.js
   - Exists: scripts/pre-publish-check.js (7 checks)
   - Missing: Integration into publish-release.js workflow
   - Impact: Validations must be run manually

4. Poor version handling
   - Issue: If version already bumped, script doesn't offer options
   - Missing: Detect state, offer reset or continue
   - Impact: User forced to manual intervention

5. Weak error messages
   - Issue: "Compilation failed" without explaining WHY
   - Missing: Parse error output, suggest fixes
   - Impact: User must debug manually
"""

reasoning_chain = [
    "1. v0.16.15 required manual bypass due to automation gaps",
    "2. Manual bypasses waste time (2+ hours) and introduce errors",
    "3. Pattern-PUBLISH-002 requires automation-first approach",
    "4. Identify root causes:",
    "   - No devDependencies validation",
    "   - No import path consistency check",
    "   - pre-publish-check.js not integrated",
    "   - Poor version handling",
    "   - Weak error messages",
    "5. Solution approach:",
    "   a. Integrate pre-publish-check.js into publish-release.js",
    "   b. Add devDependencies completeness check",
    "   c. Add import path consistency validator",
    "   d. Enhance version handling (detect state, offer options)",
    "   e. Improve error messages (parse errors, suggest fixes)",
    "6. Validation strategy:",
    "   - Test with v0.16.16 publish (real-world validation)",
    "   - Simulate failure scenarios (missing deps, bad imports)",
    "   - Ensure zero manual intervention required",
    "7. Success criteria:",
    "   - v0.16.16 publishes with ONLY automated script",
    "   - All validations pass automatically",
    "   - Clear error messages if issues detected",
    "   - No manual file edits needed",
    "8. Documentation updates:",
    "   - Remove v0.16.15 manual bypass note from CLAUDE.md",
    "   - Document new validation hooks",
    "   - Update Pattern-PUBLISH-002 examples"
]

success_impact = """
After POST-005 complete:
âœ… Automated publish script handles package architecture changes
âœ… devDependencies validated before compilation
âœ… Import paths checked for consistency
âœ… pre-publish-check.js integrated (7 automated checks)
âœ… Better version handling (detect state, offer options)
âœ… Clear error messages with actionable suggestions
âœ… v0.16.16+ publishes without manual intervention
âœ… Pattern-PUBLISH-002 compliance restored (automation-first)
âœ… Saves 2+ hours per release (no manual bypass)
âœ… Reduces human error risk
âœ… Builds confidence in automation
âœ… Future package changes handled gracefully

Historical bugs prevented (with this automation):
- v0.16.15 type: Missing @types/mocha â†’ Auto-detected, suggested npm install
- v0.16.15 type: Old imports â†’ Auto-detected, listed files to fix
- v0.13.28 type: Version mismatch â†’ Auto-detected by pre-publish-check.js
- v0.13.29 type: Missing sub-packages â†’ Auto-verified by dependency checks

Total time savings: 2 hours per release Ã— 10 releases/year = 20 hours/year
"""

# =============================================================================
# PHASE 0.5: VOICE PANEL UX POLISH (from v0.16.7 manual testing)
# =============================================================================

[tasks.UI-POLISH-001]
id = "UI-POLISH-001"
name = "Remove confusing 'Record Voice' button with text"
status = "pending"
phase = "phase-0-ux-polish"
agent = "ui-agent"

why = """
Manual testing revealed confusing UI element.
Button says 'Record Voice' with text 'press backtick key'.
Creates cognitive dissonance - button vs keyboard shortcut.
"""

description = """
Remove the 'Record Voice' button/icon that displays 'press backtick key' text.

**Current State:** Button with text label is confusing
**Desired State:** Rely on placeholder text in text area for instructions

**Files to Modify:**
- vscode-lumina/webview-ui/voice-panel.html (remove button element)
- vscode-lumina/webview-ui/voice-panel.js (remove button event handlers if any)
"""

estimated_time = "30 minutes"
dependencies = []

deliverables = [
    "Confusing button removed",
    "Voice panel cleaner UI"
]

[tasks.UI-POLISH-002]
id = "UI-POLISH-002"
name = "Remove text from toolbar icons (icon-only mode)"
status = "pending"
phase = "phase-0-ux-polish"
agent = "ui-agent"

why = """
Text labels on icons create visual clutter.
Modern UI pattern: icon-only buttons with tooltips.
Makes toolbar more compact and professional.
"""

description = """
Remove text labels from toolbar icon buttons:
1. 'Send' button â†’ icon only
2. 'Send to Terminal' button â†’ icon only

Keep tooltips for accessibility (hover to see description).

**Files to Modify:**
- vscode-lumina/webview-ui/voice-panel.html
- vscode-lumina/webview-ui/voice-panel.css (adjust button styling)
"""

estimated_time = "30 minutes"
dependencies = []

deliverables = [
    "Icon-only toolbar buttons",
    "Tooltips added for accessibility",
    "Cleaner, more compact toolbar"
]

[tasks.UI-POLISH-003]
id = "UI-POLISH-003"
name = "Remove 'Command / Transcription' label above text area"
status = "pending"
phase = "phase-0-ux-polish"
agent = "ui-agent"

why = """
Label is redundant - users know what the text area is for.
Removes visual clutter, makes UI cleaner.
"""

description = """
Remove the 'Command / Transcription' label that appears above the text area.

**Files to Modify:**
- vscode-lumina/webview-ui/voice-panel.html (remove label element)
- vscode-lumina/webview-ui/voice-panel.css (adjust spacing if needed)
"""

estimated_time = "15 minutes"
dependencies = []

deliverables = [
    "Label removed",
    "Cleaner visual hierarchy"
]

[tasks.UI-POLISH-004]
id = "UI-POLISH-004"
name = "Enhance text area placeholder with comprehensive instructions"
status = "pending"
phase = "phase-0-ux-polish"
agent = "ui-agent"

why = """
Current placeholder is minimal - only mentions backtick key.
Users need clear guidance on all interaction methods.
Reduces support burden by providing inline help.
"""

description = """
Enhance text area placeholder with comprehensive, concise instructions:

**Current Placeholder:**
"Press the backtick symbol (`) to record (usually to the left of the 1 key)"

**New Placeholder (simplified from user input):**
"Press ` (backtick, left of 1 key) to record voice
Or type directly
Hit 'Enhance' to enrich your prompt
Use Ctrl+Enter to send to terminal
Multiple terminals highlighted? They share a name - right-click to rename for unique targeting"

**Requirements:**
- Placeholder disappears when user types or records
- Placeholder reappears when text area is empty
- Line breaks for readability
- Concise but comprehensive

**Files to Modify:**
- vscode-lumina/webview-ui/voice-panel.html (update placeholder attribute)
- vscode-lumina/webview-ui/voice-panel.js (ensure placeholder logic works)
"""

estimated_time = "45 minutes"
dependencies = ["UI-POLISH-003"]

deliverables = [
    "Enhanced placeholder text",
    "Clear, comprehensive user instructions",
    "Placeholder hide/show behavior verified"
]

[tasks.BACKLOG-001]
id = "BACKLOG-001"
name = "Add visual transcription progress indicator"
status = "pending"
phase = "backlog"
agent = "ui-agent"

why = """
Users can't tell when transcription is processing vs broken.
Creates uncertainty and support burden.
Feature request from manual testing.
"""

description = """
Add visual indicator when voice transcription is in progress.

**Options:**
1. Spinner icon in text area
2. Status bar message: "Transcribing..."
3. Progress bar (if API provides progress)
4. Pulse animation on record button

**User Feedback:**
"Sometimes it looks like it breaks, and you don't know that it broke."

**This is a BACKLOG item** - not blocking v0.16.8 release.
Can be prioritized in future sprint based on user feedback volume.

**Files to Modify:**
- vscode-lumina/src/commands/voicePanel.ts (add transcription state)
- vscode-lumina/webview-ui/voice-panel.html (add indicator element)
- vscode-lumina/webview-ui/voice-panel.js (show/hide indicator)
"""

estimated_time = "1-2 hours"
dependencies = []

deliverables = [
    "Transcription progress indicator implemented",
    "User clarity improved",
    "Reduced support burden"
]

[tasks.DEPRECATE-001]
id = "DEPRECATE-001"
name = "Remove F13 quick voice capture functionality"
status = "pending"
phase = "phase-0-ux-polish"
agent = "infrastructure-agent"

why = """
F13 key doesn't work on Windows or Mac keyboards.
Only works on Windows/Linux systems with specific keyboard configurations.
User feedback: Deprecated function, needs to be deleted.
"""

description = """
Remove F13 quick voice capture functionality from codebase.

**Reason for Removal:**
- F13 key is not standard on Windows/Mac keyboards
- Creates confusion and support burden
- User explicitly requested removal

**Files to Remove/Modify:**
1. Search for F13 keybinding registrations in package.json
2. Remove F13 command handler (likely in src/commands/)
3. Remove any F13 references in documentation
4. Update keybindings documentation

**Commands to Run:**
- grep -r "F13" vscode-lumina/
- Check package.json for keybinding registration
"""

estimated_time = "45 minutes"
dependencies = []

deliverables = [
    "F13 functionality removed",
    "Keybindings updated",
    "Documentation updated",
    "No F13 references remaining"
]

[tasks.DEPRECATE-002]
id = "DEPRECATE-002"
name = "Remove Shift+Backtick global voice typing functionality"
status = "pending"
phase = "phase-0-ux-polish"
agent = "infrastructure-agent"

why = """
Shift+Backtick conflicts with backtick-only approach.
User decision: Not forcing panel open, backtick key alone is sufficient.
Deprecating Shift+Backtick to simplify keybinding model.
"""

description = """
Remove Shift+Backtick global voice typing functionality from codebase.

**Reason for Removal:**
- Conflicting with backtick-only approach
- User explicitly requested removal: "deprecate shift back tick"
- Simplifies keybinding model (backtick only)

**Files to Remove/Modify:**
1. Search for Shift+` keybinding registrations in package.json
2. Remove Shift+Backtick command handler (likely captureVoiceGlobal.ts)
3. Remove any Shift+` references in documentation
4. Update Test 2.3 status in MANUAL_TEST_v0.16.7.md (already marked DEPRECATED)

**Commands to Run:**
- grep -r "shift.*backtick" vscode-lumina/ (case insensitive)
- grep -r "captureVoiceGlobal" vscode-lumina/
- Check package.json for "shift+`" keybinding
"""

estimated_time = "45 minutes"
dependencies = []

deliverables = [
    "Shift+Backtick functionality removed",
    "Keybindings updated",
    "Documentation updated",
    "No Shift+` references remaining"
]

# =============================================================================
# PHASE 1: BUILD SPRINT TEMPLATE SYSTEM (Infrastructure) - HIGH PRIORITY
# =============================================================================

# This phase builds the normalization infrastructure that Phase 7 will use.
# These tasks were originally Phase 6, now moved to Phase 1 to enable testing.

[tasks.TEMPLATE-001]
id = "TEMPLATE-001"
name = "Create SPRINT_TEMPLATE.toml"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Create internal/sprints/SPRINT_TEMPLATE.toml with normalized tasks:

REQUIRED Tasks (cannot skip):
- Documentation: CHANGELOG, README, patterns, CLAUDE.md (4 tasks)
- Code Quality: ripple, test coverage, dependency audit, type safety (4 tasks)
- Agent Sync: context updates, pitfalls (2 tasks)
- Infrastructure: git hooks, validation scripts (2 tasks)
- Configuration: settings schema check (1 task)

SUGGESTED Tasks (can skip with justification):
- Performance: regression testing (1 task)
- Security: vulnerability scan (1 task)
- Compatibility: cross-platform, backwards compat (2 tasks)

CONDITIONAL Tasks (only if condition met):
- Publishing: pre-publish checklist, artifacts, verification (5 tasks if publishing)
- User Experience: upgrade docs, release notes (3 tasks if user-facing changes)

RETROSPECTIVE Tasks (every sprint):
- Sprint retrospective (1 task)
- Pattern extraction (1 task)

Total: 27 normalized tasks per sprint

Template format:
- Each task has full rich format (why, context, reasoning_chain, etc.)
- Task IDs prefixed by category (DOC-001, QA-001, etc.)
- Clear conditions for CONDITIONAL tasks
- Justification placeholder for SUGGESTED tasks
"""
estimated_lines = 1500
estimated_time = "3-4 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "SPRINT_TEMPLATE.toml created (internal/sprints/)",
    "All 27 task categories defined with rich format",
    "Template structure documented (comments)",
    "Usage instructions included",
    "Validated with SprintLoader parser"
]

performance_target = "Template loading completes in <100ms (for sprint generation)"

patterns = ["Pattern-TRACKING-001", "Pattern-TDD-001"]

files_to_modify = [
    "internal/sprints/SPRINT_TEMPLATE.toml (create new file, 1500+ lines)",
    "CLAUDE.md (add Pattern-SPRINT-TEMPLATE-001 reference)"
]

validation_criteria = [
    "Run: node -e \"const toml = require('@iarna/toml'); toml.parse(require('fs').readFileSync('internal/sprints/SPRINT_TEMPLATE.toml', 'utf-8'));\"",
    "Output: Parses successfully, no errors",
    "Check: Template has [template.metadata] section",
    "Check: Template has [template.required] with 13 tasks",
    "Check: Template has [template.suggested] with 4 tasks",
    "Check: Template has [template.conditional.publishing] with 5 tasks",
    "Check: Template has [template.conditional.ux] with 3 tasks",
    "Check: Template has [template.retrospective] with 2 tasks",
    "Verify: Each task has all rich fields (why, context, reasoning_chain, success_impact, etc.)",
    "Test: Import template into SprintLoader, verify parsing works",
    "TDD: Parse template with SprintLoader.parseTomlTasks()",
    "TDD: Verify all 27 tasks load correctly",
    "TDD: Check rich field parsing",
    "TDD: Validate category completeness"
]

why = """
Normalized sprint tasks prevent forgotten steps (Pattern-TRACKING-001).
Historical bugs: 15+ hours wasted on v0.13.23, v0.13.29, v0.15.31 due to missing quality checks.
Template ensures every sprint meets quality bar consistently.
Prevents "forgot to test", "forgot to update CHANGELOG", "forgot pre-commit hook" bugs.
"""

context = """
Historical bug patterns from previous releases:
- v0.13.23: Native dependency bug (9 hours) - NO pre-publish dependency check
- v0.13.29: Sub-package version mismatch (2 hours) - NO version sync verification
- v0.15.31: Runtime npm dependency (2 hours) - NO dependency audit
- v0.16.7: Publishing order violation - NO protocol enforcement

Common pattern: Skipped quality tasks â†’ bugs in production â†’ hours debugging

Solution: Normalize quality tasks into reusable template
- Every sprint gets baseline quality checks
- Required tasks cannot be skipped
- Suggested tasks need justification to skip
- Conditional tasks auto-added based on sprint metadata

Reference: Pattern-TRACKING-001 (Task Tracking & Pre-Commit Protocol)
Goal: Create "ratchet" preventing quality regression across sprints
"""

reasoning_chain = [
    "1. Analyze historical bugs: v0.13.23, v0.13.29, v0.15.31, v0.16.7",
    "2. Identify common pattern: Skipped quality checks â†’ production bugs",
    "3. Extract quality tasks that prevent each bug category:",
    "   - Dependency audit â†’ prevents native dep bugs (v0.13.23)",
    "   - Version sync check â†’ prevents mismatch bugs (v0.13.29)",
    "   - Pre-publish checklist â†’ prevents protocol violations (v0.16.7)",
    "4. Categorize tasks by requirement level:",
    "   - REQUIRED: Must have in every sprint (13 tasks)",
    "   - SUGGESTED: Best practice but can skip (4 tasks)",
    "   - CONDITIONAL: Only if sprint type matches (8 tasks)",
    "   - RETROSPECTIVE: Every sprint learns from itself (2 tasks)",
    "5. Design template structure in TOML:",
    "   - [template.metadata] section (version, last_updated)",
    "   - [template.required] section (13 tasks)",
    "   - [template.suggested] section (4 tasks)",
    "   - [template.conditional.publishing] section (5 tasks)",
    "   - [template.conditional.ux] section (3 tasks)",
    "   - [template.retrospective] section (2 tasks)",
    "6. Write each task with full rich format:",
    "   - why, context, description, reasoning_chain, success_impact",
    "   - test_requirements, deliverables, patterns, files_to_modify",
    "   - validation_criteria, performance_target",
    "7. Add usage instructions in comments",
    "8. Validate template structure with SprintLoader parser",
    "9. Test by manually applying to this sprint (Phase 7)",
    "10. Commit: 'feat(template): create SPRINT_TEMPLATE.toml v1.0'"
]

success_impact = """
After TEMPLATE-001 complete:
âœ… Reusable sprint template exists (27 normalized tasks)
âœ… Every sprint gets baseline quality checks
âœ… Historical bugs preventable with template
âœ… Required tasks cannot be skipped (enforced by validator)
âœ… Suggested tasks require justification to skip
âœ… Conditional tasks auto-added based on sprint type
âœ… Retrospective tasks ensure continuous improvement
âœ… Consistent quality bar across all sprints
âœ… Reduces bug rate by 60%+ (prevents 15+ hour bugs)
âœ… Sprint-plan skill can inject template automatically
"""

[tasks.TEMPLATE-002]
id = "TEMPLATE-002"
name = "Enhance sprint-plan skill with template injection"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Update .claude/skills/sprint-plan/SKILL.md to inject template:

1. ENHANCE mode logic (after feature task generation):
   - Load SPRINT_TEMPLATE.toml using fs.readFileSync()
   - Parse template with @iarna/toml
   - Extract task categories (required, suggested, conditional, retrospective)
   - Merge template tasks with generated feature tasks
   - Return unified sprint TOML

2. Template task categorization:
   - Mark REQUIRED tasks (always include, cannot skip)
   - Mark SUGGESTED tasks (include with skip justification prompt)
   - Mark CONDITIONAL tasks (detect conditions, include if match):
     * Publishing tasks â†’ if sprint has "release" or "publish" in name
     * UX tasks â†’ if sprint has user-facing changes
   - Mark RETROSPECTIVE tasks (always include, every sprint)

3. Condition detection logic:
   - Check sprint name/description for keywords
   - Check if CHANGELOG updates present
   - Check if version bump planned
   - Auto-determine which conditional tasks to inject

4. Task ID collision avoidance:
   - Feature tasks: Use feature-specific prefixes (FEAT-001, BUG-001, etc.)
   - Template tasks: Use category prefixes (DOC-001, QA-001, INFRA-001, etc.)
   - No overlap possible

5. Test template injection:
   - Generate test sprint: "build API endpoint"
   - Verify all required tasks present
   - Verify suggested tasks included with justification
   - Verify conditional tasks NOT included (no publishing)
   - Generate test sprint: "release v1.0"
   - Verify conditional publishing tasks NOW included
"""
estimated_lines = 200
estimated_time = "3-4 hours"
dependencies = ["TEMPLATE-001"]
agent = "infrastructure-agent"

deliverables = [
    "sprint-plan skill enhanced (.claude/skills/sprint-plan/SKILL.md)",
    "Template injection logic implemented",
    "Condition detection working (publishing, UX)",
    "Task ID collision avoidance working",
    "Test sprint generated with template tasks",
    "Template merging verified with SprintLoader"
]

performance_target = "Template injection adds <100ms to sprint generation time"

patterns = ["Pattern-SPRINT-PLAN-001", "Pattern-TRACKING-001"]

files_to_modify = [
    ".claude/skills/sprint-plan/SKILL.md (add template injection section, 200+ lines)",
    "CLAUDE.md (update Sprint Planning Protocol with template injection)"
]

validation_criteria = [
    "Run: /sprint-plan 'build API endpoints'",
    "Output: Sprint TOML with feature + template tasks",
    "Check: Required tasks present (DOC-001, QA-001, etc.)",
    "Check: Suggested tasks present (PERF-001, SEC-001, etc.)",
    "Check: Publishing tasks ABSENT (no release keyword)",
    "Run: /sprint-plan 'release v2.0'",
    "Output: Sprint TOML with publishing tasks NOW present",
    "Check: PUB-001 through PUB-005 present",
    "Verify: All generated sprints validate with SprintSchemaValidator",
    "Test: Parse generated sprint with SprintLoader, loads successfully",
    "TDD: Feature-only sprint - 23 tasks generated",
    "TDD: Release sprint - 26 tasks with publishing",
    "TDD: User-facing sprint - UX tasks included",
    "TDD: Template parsing - all 27 tasks extracted"
]

why = """
Sprint-plan skill must automatically inject template tasks.
Enables consistent quality across all sprints without manual checklist.
Prevents forgotten quality tasks that cause production bugs.
Makes quality checks non-negotiable (baked into every sprint).
"""

context = """
Current sprint-plan skill workflow:
1. User runs: /sprint-plan "build new feature"
2. Skill analyzes requirements, generates feature tasks only
3. User manually adds quality tasks (often forgotten)
4. Result: Inconsistent sprint quality

Desired workflow with template injection:
1. User runs: /sprint-plan "build new feature"
2. Skill loads SPRINT_TEMPLATE.toml
3. Skill generates feature tasks from requirements
4. Skill merges template + feature tasks:
   - REQUIRED tasks: Always included
   - SUGGESTED tasks: Included with skip justification prompt
   - CONDITIONAL tasks: Included if sprint metadata matches
   - RETROSPECTIVE tasks: Always included (every sprint)
5. Result: Every sprint has baseline quality checks

Integration points:
- .claude/skills/sprint-plan/SKILL.md (skill logic)
- internal/sprints/SPRINT_TEMPLATE.toml (template source)
- SprintLoader.ts (TOML parsing)

Reference: Pattern-SPRINT-PLAN-001 (Sprint Planning Protocol)
"""

reasoning_chain = [
    "1. User runs: /sprint-plan 'build user authentication system'",
    "2. Skill enters ENHANCE mode",
    "3. Skill generates feature tasks:",
    "   - AUTH-001: Design authentication architecture",
    "   - AUTH-002: Implement JWT token system",
    "   - AUTH-003: Add password hashing",
    "   - AUTH-004: Create login/signup endpoints",
    "   (4 feature tasks generated)",
    "4. Skill loads SPRINT_TEMPLATE.toml:",
    "   - fs.readFileSync('internal/sprints/SPRINT_TEMPLATE.toml')",
    "   - toml.parse() extracts template structure",
    "5. Skill extracts template categories:",
    "   - template.required â†’ 13 tasks (always include)",
    "   - template.suggested â†’ 4 tasks (include with skip option)",
    "   - template.conditional.publishing â†’ Skip (no 'release' keyword)",
    "   - template.conditional.ux â†’ Include (auth is user-facing)",
    "   - template.retrospective â†’ 2 tasks (always include)",
    "   Total: 13 + 4 + 3 + 2 = 22 template tasks",
    "6. Skill merges: 4 feature tasks + 22 template tasks = 26 total",
    "7. Skill returns unified sprint TOML",
    "8. SprintLoader parses unified sprint successfully",
    "9. Sprint panel displays all 26 tasks with rich format",
    "10. User gets quality checks automatically (no manual checklist)"
]

success_impact = """
After TEMPLATE-002 complete:
âœ… Sprint-plan skill automatically injects template tasks
âœ… Every generated sprint has baseline quality checks
âœ… REQUIRED tasks always included (13 tasks minimum)
âœ… CONDITIONAL tasks smart-detected based on sprint type
âœ… SUGGESTED tasks included with skip justification prompt
âœ… RETROSPECTIVE tasks ensure continuous learning
âœ… No manual checklist needed (automation handles it)
âœ… Consistent quality across all sprints
âœ… Reduces forgotten quality tasks by 100%
âœ… Template becomes default, not opt-in
"""

[tasks.TEMPLATE-003]
id = "TEMPLATE-003"
name = "Update SprintSchemaValidator with template validation"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Update vscode-lumina/src/services/SprintSchemaValidator.ts:

1. Add template validation rules:
   - REQUIRED tasks MUST be present (13 tasks)
   - CONDITIONAL tasks only if condition met (publishing, UX)
   - SUGGESTED tasks skipped need justification in [sprint.metadata]
   - RETROSPECTIVE tasks MUST be present (2 tasks)

2. Validate task categories by prefix:
   - Check for documentation tasks (DOC-001, DOC-002, etc.)
   - Check for code quality tasks (QA-001 through QA-004)
   - Check for agent sync tasks (AGENT-001, AGENT-002)
   - Check for infrastructure tasks (INFRA-001, INFRA-002)
   - Check for configuration tasks (CONFIG-001)
   - Check for retrospective tasks (RETRO-001, RETRO-002)

3. Condition detection logic:
   - Publishing sprint: Check sprint name for "release", "publish", "v\\d+\\.\\d+"
   - UX sprint: Check if user-facing tasks present (UI-, UX- prefixes)
   - If conditions met, require conditional tasks

4. Error messages with actionable guidance:
   - Error: "Missing required task DOC-001 (Update CHANGELOG)"
   - Error: "Missing retrospective task RETRO-001"
   - Warning: "SUGGESTED task SEC-001 skipped without justification"
   - Info: "To skip SUGGESTED tasks, add [sprint.metadata.skipped_tasks] section"

5. Integration with existing validation:
   - Add new validateTemplateCompliance() method
   - Call from existing validate() method (after basic validation)
   - Return errors + warnings arrays
"""
estimated_lines = 150
estimated_time = "2-3 hours"
dependencies = ["TEMPLATE-001"]
agent = "infrastructure-agent"

deliverables = [
    "SprintSchemaValidator.ts updated with validateTemplateCompliance() method",
    "Template validation working (required, suggested, conditional, retrospective)",
    "Condition detection logic (publishing, UX)",
    "Actionable error messages",
    "6 new test cases passing (sprintSchemaValidator.test.ts)",
    "90% test coverage maintained"
]

performance_target = "Template validation adds <50ms to total validation time (acceptable for file save)"

patterns = ["Pattern-VALIDATION-001", "Pattern-TDD-001", "Pattern-TRACKING-001"]

files_to_modify = [
    "vscode-lumina/src/services/SprintSchemaValidator.ts (add validateTemplateCompliance method, ~150 lines)",
    "vscode-lumina/test/services/sprintSchemaValidator.test.ts (add 6 test cases, ~200 lines)"
]

validation_criteria = [
    "Run: npm test -- sprintSchemaValidator.test.ts",
    "Output: 6 new tests passing (template validation)",
    "Test: Create sprint missing DOC-001",
    "Output: Validation error shown in VS Code notification",
    "Test: Create publishing sprint missing PUB-001",
    "Output: Validation error 'Publishing sprint missing PUB-001'",
    "Test: Create sprint with all required tasks",
    "Output: Validation passes, sprint loads in panel",
    "Check: Test coverage for SprintSchemaValidator",
    "Output: â‰¥90% coverage (lines, branches, functions)",
    "TDD RED: Sprint with all required tasks â†’ PASS",
    "TDD RED: Sprint missing required task â†’ FAIL",
    "TDD RED: Publishing sprint with conditional tasks â†’ PASS",
    "TDD RED: Publishing sprint missing conditional task â†’ FAIL",
    "TDD RED: Sprint skipping suggested task WITH justification â†’ PASS",
    "TDD RED: Sprint skipping suggested task WITHOUT justification â†’ WARN"
]

why = """
Schema validator must enforce template task presence at validation time.
Prevents skipping required tasks that prevent production bugs.
Provides fail-fast feedback before sprint starts.
Complements runtime enforcement (WorkflowCheck) with static validation.
"""

context = """
Current SprintSchemaValidator (VAL-001):
- Validates TOML syntax and structure
- Checks required fields (id, name, status, etc.)
- Validates task dependencies (no circular deps)
- Validates status values (pending, in_progress, completed)
- BUT: Doesn't check if required quality tasks present

With template system:
- REQUIRED tasks must be present (DOC-001, QA-001, etc.)
- SUGGESTED tasks can be skipped with justification in sprint metadata
- CONDITIONAL tasks only required if sprint type matches
- RETROSPECTIVE tasks always required (every sprint)

Validation timing:
1. Static validation: SprintSchemaValidator (this task)
   - Runs on file save (FileSystemWatcher)
   - Runs pre-commit (git hook)
   - Blocks sprint loading if invalid
2. Runtime validation: WorkflowCheck (TEMPLATE-004)
   - Runs when completing tasks
   - Blocks sprint promotion if template tasks incomplete

Two-layer defense: Catch issues early (static) + ongoing enforcement (runtime)

Reference: Pattern-VALIDATION-001 (4-layer defense)
Related: SprintSchemaValidator.ts lines 1-350 (existing validation)
"""

reasoning_chain = [
    "1. User saves ACTIVE_SPRINT.toml with new sprint",
    "2. FileSystemWatcher triggers SprintSchemaValidator",
    "3. Validator runs basic validation (syntax, structure, deps)",
    "4. NEW: Validator runs validateTemplateCompliance():",
    "   - Load SPRINT_TEMPLATE.toml to get required task list",
    "   - Extract required task IDs (DOC-001, QA-001, etc.)",
    "   - Check if sprint has each required task ID",
    "5. If required task missing:",
    "   - Error: 'Missing required task DOC-001 (Update CHANGELOG)'",
    "   - Validator returns validation failed",
    "   - VS Code shows error notification",
    "   - Sprint panel won't load (validation failed)",
    "6. If conditional tasks missing but condition met:",
    "   - Detect 'release' in sprint name",
    "   - Error: 'Publishing sprint missing PUB-001 task'",
    "7. If suggested task skipped without justification:",
    "   - Warning: 'SEC-001 skipped without justification'",
    "   - Show how to add justification in [sprint.metadata]",
    "8. If all validation passes:",
    "   - Success: Sprint loads in panel",
    "   - Template compliance verified âœ…",
    "9. Developer fixes missing tasks, saves again",
    "10. Validation passes, sprint loads successfully"
]

success_impact = """
After TEMPLATE-003 complete:
âœ… Static validation enforces template compliance
âœ… Required tasks cannot be skipped (validation fails)
âœ… Clear error messages guide developer to fix
âœ… Fail-fast feedback (before sprint starts)
âœ… Complements runtime enforcement (WorkflowCheck)
âœ… Two-layer defense: Static + Runtime validation
âœ… FileSystemWatcher shows errors on save
âœ… Pre-commit hook blocks commits with missing tasks
âœ… Sprint panel won't load invalid sprints
âœ… Reduces risk of forgotten quality tasks to near zero
"""

[tasks.TEMPLATE-004]
id = "TEMPLATE-004"
name = "Update WorkflowCheck with template enforcement"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Update vscode-lumina/src/services/WorkflowCheck.ts:

1. Add template task completion checks:
   - Load SPRINT_TEMPLATE.toml to get required task list
   - Check current sprint's task statuses
   - Verify all REQUIRED tasks status === 'completed'
   - Warn if SUGGESTED tasks incomplete (non-blocking)
   - Validate CONDITIONAL tasks based on sprint metadata (if applicable)
   - Verify RETROSPECTIVE tasks completed (RETRO-001, RETRO-002)

2. Integrate with existing workflow checks:
   - Add checkTemplateCompliance() method
   - Call from checkWorkflow('sprint') after existing checks
   - Return gaps array + confidence adjustment
   - Block sprint promotion if required template tasks incomplete

3. Clear error messages with actionable guidance:
   - "Cannot start new sprint: Required task DOC-001 (Update CHANGELOG) incomplete"
   - "Cannot start new sprint: Retrospective task RETRO-001 incomplete"
   - "Warning: Suggested task SEC-001 (Security audit) incomplete"
   - "Tip: Complete required template tasks before starting next sprint"

4. Confidence score adjustment:
   - Required tasks incomplete: confidence = 0.0 (CRITICAL)
   - Suggested tasks incomplete: confidence -= 0.10 (WARNING)
   - All template tasks complete: confidence += 0.10 (BONUS)

5. Integration with MiddlewareLogger:
   - Log template compliance checks
   - Log blocked sprint attempts (audit trail)
   - Log compliance scores for analytics
"""
estimated_lines = 150
estimated_time = "2-3 hours"
dependencies = ["TEMPLATE-001"]
agent = "infrastructure-agent"

deliverables = [
    "WorkflowCheck.ts updated with checkTemplateCompliance() method",
    "Runtime template enforcement working",
    "Sprint promotion blocked if required tasks incomplete",
    "Clear error messages with actionable guidance",
    "Confidence score adjustments based on template compliance",
    "5 new test cases passing (workflowCheck.test.ts)",
    "90% test coverage maintained"
]

performance_target = "Template compliance check completes in <100ms (acceptable for sprint creation)"

patterns = ["Pattern-CODE-001", "Pattern-TRACKING-001", "Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/WorkflowCheck.ts (add checkTemplateCompliance method, ~150 lines)",
    "vscode-lumina/test/services/workflowCheck.test.ts (add 5 test cases, ~180 lines)"
]

validation_criteria = [
    "Run: npm test -- workflowCheck.test.ts",
    "Output: 5 new tests passing (template compliance)",
    "Test: Create sprint with DOC-001 incomplete",
    "Output: Workflow check blocks, shows error 'Required task DOC-001 incomplete'",
    "Test: Complete DOC-001, run workflow check again",
    "Output: Check passes, sprint creation allowed",
    "Test: Create sprint with SEC-001 incomplete (suggested)",
    "Output: Warning shown, but sprint creation allowed",
    "Check: Test coverage for WorkflowCheck",
    "Output: â‰¥90% coverage",
    "TDD RED: Sprint with all required tasks complete â†’ PASS",
    "TDD RED: Sprint missing required task â†’ FAIL",
    "TDD RED: Sprint missing retrospective task â†’ FAIL",
    "TDD RED: Sprint with incomplete suggested task â†’ WARN",
    "TDD RED: Publishing sprint missing publishing task â†’ FAIL"
]

why = """
Runtime enforcement of template task completion prevents shipping sprints without quality checks.
Blocks next sprint creation if current sprint's template tasks incomplete.
Complements static validation (TEMPLATE-003) with runtime enforcement.
Pattern-TRACKING-001: Pre-commit checklist enforced at runtime.
"""

context = """
WorkflowCheck service (Pattern-CODE-001):
- Runs before major operations (code, sprint creation, publishing)
- Checks prerequisites (tests passing, git clean, agent assigned, confidence score)
- Returns gaps + confidence score + critical junction flag

With template system:
- NEW CHECK: Verify template tasks completed before sprint promotion
- Required tasks MUST be completed (DOC-001, QA-001, etc.)
- Suggested tasks can be incomplete (but warn user)
- Conditional tasks checked based on sprint type
- Retrospective tasks MUST be completed (every sprint learns)

Two-layer defense:
1. Static (TEMPLATE-003): Catch missing tasks when sprint created
2. Runtime (this task): Catch incomplete tasks when sprint completes

Validation timing:
- WorkflowCheck('sprint') runs when user clicks "Create New Sprint"
- Checks current sprint's template task completion status
- Blocks new sprint creation if required template tasks incomplete
- Shows clear error: "Cannot start new sprint: Required task DOC-001 incomplete"

Reference: Pattern-CODE-001 (Code Development Protocol)
Related: WorkflowCheck.ts lines 1-500 (existing workflow checks)
"""

reasoning_chain = [
    "1. User completes feature tasks in current sprint",
    "2. User clicks 'Create New Sprint' button in Sprint Panel",
    "3. WorkflowCheck.checkWorkflow('sprint') triggered",
    "4. Existing checks run:",
    "   - Git status check (clean working directory)",
    "   - Agent availability check (AgentRegistry loaded)",
    "   - Confidence score check (feature tasks complete)",
    "5. NEW: checkTemplateCompliance() runs:",
    "   - Load SPRINT_TEMPLATE.toml",
    "   - Extract required task IDs (DOC-001, QA-001, etc.)",
    "   - Check current sprint task statuses",
    "6. If DOC-001 status !== 'completed':",
    "   - Gap: 'Required task DOC-001 (Update CHANGELOG) incomplete'",
    "   - Confidence: 0.0 (CRITICAL)",
    "   - Critical junction: YES (blocks sprint creation)",
    "7. Show workflow check to user:",
    "   - âŒ Template compliance: FAILED",
    "   - Gap: Required task DOC-001 incomplete",
    "   - Action: Complete DOC-001 before creating new sprint",
    "8. User completes DOC-001, runs workflow check again",
    "9. All required tasks complete:",
    "   - âœ… Template compliance: PASSED",
    "   - Confidence: 0.90 (HIGH)",
    "   - Ready to create new sprint âœ…",
    "10. Sprint creation proceeds (template enforcement satisfied)"
]

success_impact = """
After TEMPLATE-004 complete:
âœ… Runtime enforcement of template task completion
âœ… Cannot create new sprint without completing required tasks
âœ… Clear error messages guide user to fix gaps
âœ… Suggested tasks generate warnings (non-blocking)
âœ… Two-layer defense: Static + Runtime validation
âœ… WorkflowCheck blocks incomplete sprints
âœ… Audit trail of compliance checks (MiddlewareLogger)
âœ… Confidence score reflects template compliance
âœ… Prevents shipping sprints without quality checks
âœ… Pattern-TRACKING-001 enforced at runtime
"""

[tasks.TEMPLATE-005]
id = "TEMPLATE-005"
name = "Document sprint template system"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Create internal/SPRINT_TEMPLATE_GUIDE.md with comprehensive documentation:

1. Template System Overview:
   - What is the template system?
   - Why does it exist? (historical bugs, quality enforcement)
   - How does it work? (4-layer enforcement: injection, static, runtime, retrospective)

2. Task Categories Explained:
   - REQUIRED tasks (13): Cannot skip, blocks sprint if missing
   - SUGGESTED tasks (4): Can skip with justification in [sprint.metadata]
   - CONDITIONAL tasks (8): Auto-included based on sprint type
     * Publishing tasks (5): If sprint name contains "release", "publish", "v\\d+\\.\\d+"
     * UX tasks (3): If user-facing changes detected
   - RETROSPECTIVE tasks (2): Every sprint learns from itself

3. Using the Template:
   - Automatic injection via sprint-plan skill
   - Manual sprint creation (copy template tasks)
   - Skipping suggested tasks (add justification)
   - Customizing conditional detection logic

4. Task Category Reference Table:
   | Category | Task IDs | Count | Requirement | Purpose |
   |----------|----------|-------|-------------|---------|
   | Documentation | DOC-001 to DOC-004 | 4 | REQUIRED | CHANGELOG, README, patterns, CLAUDE.md |
   | Code Quality | QA-001 to QA-004 | 4 | REQUIRED | Ripple, tests, deps, types |
   | Agent Sync | AGENT-001 to AGENT-002 | 2 | REQUIRED | Context updates, pitfalls |
   | Infrastructure | INFRA-001 to INFRA-002 | 2 | REQUIRED | Git hooks, validation |
   | Configuration | CONFIG-001 | 1 | REQUIRED | Settings schema |
   | Performance | PERF-001 | 1 | SUGGESTED | Regression testing |
   | Security | SEC-001 | 1 | SUGGESTED | Vulnerability scan |
   | Compatibility | COMPAT-001 to COMPAT-002 | 2 | SUGGESTED | Cross-platform, backwards |
   | Publishing | PUB-001 to PUB-005 | 5 | CONDITIONAL | Pre-publish checklist |
   | UX | UX-001 to UX-003 | 3 | CONDITIONAL | Upgrade docs, release notes |
   | Retrospective | RETRO-001 to RETRO-002 | 2 | REQUIRED | Sprint retro, pattern extraction |

5. Customizing the Template:
   - Adding project-specific tasks
   - Modifying requirement levels (REQUIRED â†’ SUGGESTED)
   - Updating conditional detection logic
   - Versioning template changes

6. Template Version History:
   - v1.0.0 (2025-11-05): Initial template (27 tasks)
   - Future versions will be documented here

7. Enforcement Mechanisms:
   - Layer 1: Auto-injection (sprint-plan skill) - TEMPLATE-002
   - Layer 2: Static validation (SprintSchemaValidator) - TEMPLATE-003
   - Layer 3: Runtime enforcement (WorkflowCheck) - TEMPLATE-004
   - Layer 4: Retrospective learning (RETRO tasks) - Every sprint

8. Troubleshooting:
   - "Missing required task DOC-001" â†’ Complete task or add to sprint
   - "SUGGESTED task SEC-001 skipped without justification" â†’ Add to [sprint.metadata.skipped_tasks]
   - "Publishing sprint missing PUB-001" â†’ Add publishing tasks or remove "release" from sprint name

9. Update CLAUDE.md:
   - Add Pattern-SPRINT-TEMPLATE-001 to Design Patterns section
   - Update Sprint Planning Protocol (Pattern-SPRINT-PLAN-001) with template injection
   - Document template task categories in Pre-Task Analysis Protocol
"""
estimated_lines = 600
estimated_time = "2-3 hours"
dependencies = ["TEMPLATE-001", "TEMPLATE-002", "TEMPLATE-003", "TEMPLATE-004"]
agent = "documentation-agent"

deliverables = [
    "internal/SPRINT_TEMPLATE_GUIDE.md created (comprehensive guide, ~600 lines)",
    "Task category reference table (all 27 tasks)",
    "Usage examples (skipping tasks, customizing, troubleshooting)",
    "CLAUDE.md updated with Pattern-SPRINT-TEMPLATE-001",
    "Sprint Planning Protocol updated with template injection",
    "Version history section (v1.0.0 documented)"
]

performance_target = "Documentation loading <10ms (markdown files are fast)"

patterns = ["Pattern-DOCS-001", "Pattern-SPRINT-TEMPLATE-001 (NEW)", "Pattern-TRACKING-001"]

files_to_modify = [
    "internal/SPRINT_TEMPLATE_GUIDE.md (create new file, ~600 lines)",
    "CLAUDE.md (add Pattern-SPRINT-TEMPLATE-001, update Sprint Planning Protocol, ~100 lines)"
]

validation_criteria = [
    "Check: internal/SPRINT_TEMPLATE_GUIDE.md exists",
    "Check: Task category table lists all 27 tasks",
    "Verify: Task counts accurate (13+4+8+2 = 27)",
    "Verify: All task IDs match SPRINT_TEMPLATE.toml",
    "Check: CLAUDE.md has Pattern-SPRINT-TEMPLATE-001",
    "Check: Sprint Planning Protocol references template injection",
    "Verify: All cross-references valid (file paths, patterns)",
    "Test: Ask Claude to plan sprint, verify it references guide",
    "Documentation: Accuracy check - task counts and IDs match",
    "Documentation: Completeness check - all 27 tasks documented",
    "Documentation: Usability check - examples and troubleshooting present",
    "Documentation: Integration check - CLAUDE.md updated"
]

why = """
Users and developers need comprehensive documentation to understand and use template system.
Documentation enables proper usage, customization, and maintenance.
Pattern-DOCS-001: High reusability (referenced in sprint-plan skill, CLAUDE.md, agent contexts).
"""

context = """
Template system components (after TEMPLATE-001 through TEMPLATE-004):
- SPRINT_TEMPLATE.toml: 27 normalized tasks (required, suggested, conditional, retrospective)
- Sprint-plan skill: Auto-injects template tasks when generating sprints
- SprintSchemaValidator: Static validation (file save, pre-commit)
- WorkflowCheck: Runtime enforcement (sprint promotion)

Documentation needs:
1. User guide: How to use template system
2. Developer guide: How to customize/extend template
3. Pattern reference: Pattern-SPRINT-TEMPLATE-001
4. CLAUDE.md update: Add to Sprint Planning Protocol

Documentation will be referenced by:
- Claude when planning sprints
- Developers customizing template for their projects
- Future Ã†therLight contributors
- Agent contexts (infrastructure-agent, documentation-agent)

High reusability â†’ Create comprehensive documentation (Pattern-DOCS-001)
"""

reasoning_chain = [
    "1. Gather all template system components:",
    "   - SPRINT_TEMPLATE.toml structure (TEMPLATE-001)",
    "   - Sprint-plan injection logic (TEMPLATE-002)",
    "   - SprintSchemaValidator validation (TEMPLATE-003)",
    "   - WorkflowCheck enforcement (TEMPLATE-004)",
    "2. Identify documentation audiences:",
    "   - Users: How to use template when planning sprints",
    "   - Developers: How to customize template for their projects",
    "   - Future contributors: How system works internally",
    "   - Claude: Reference when planning sprints",
    "3. Structure documentation by audience needs:",
    "   - Overview (all audiences)",
    "   - Task categories (users + Claude)",
    "   - Using template (users)",
    "   - Customizing template (developers)",
    "   - Version history (all audiences)",
    "   - Enforcement mechanisms (developers + contributors)",
    "   - Troubleshooting (users)",
    "4. Create comprehensive task category table:",
    "   - All 27 tasks listed with IDs, counts, requirements, purposes",
    "   - Easy reference for sprint planning",
    "5. Add examples for each section:",
    "   - Example: Skipping suggested task with justification",
    "   - Example: Customizing conditional detection logic",
    "   - Example: Adding project-specific template task",
    "6. Update CLAUDE.md with new pattern:",
    "   - Pattern-SPRINT-TEMPLATE-001 (Sprint Template System)",
    "   - Reference in Sprint Planning Protocol",
    "   - Add to Design Patterns section",
    "7. Cross-link related patterns:",
    "   - Pattern-TRACKING-001 (Pre-Commit Protocol)",
    "   - Pattern-TDD-001 (Test-Driven Development)",
    "   - Pattern-VALIDATION-001 (4-Layer Defense)",
    "8. Validate documentation:",
    "   - All task IDs match SPRINT_TEMPLATE.toml",
    "   - All counts accurate (13 required, 4 suggested, etc.)",
    "   - All references valid (file paths, pattern names)",
    "9. Commit: 'docs(template): create comprehensive template system guide'",
    "10. Pattern now reusable across all future sprints"
]

success_impact = """
After TEMPLATE-005 complete:
âœ… Comprehensive template system documentation exists
âœ… Users understand how to use template for sprint planning
âœ… Developers can customize template for their projects
âœ… Claude references guide when planning sprints
âœ… Future contributors understand system architecture
âœ… All 27 tasks documented with purpose and requirements
âœ… Troubleshooting section prevents support burden
âœ… Pattern-SPRINT-TEMPLATE-001 created (high reusability)
âœ… CLAUDE.md updated with template protocol
âœ… Version history enables template evolution tracking
"""

[tasks.TEMPLATE-006]
id = "TEMPLATE-006"
name = "Test template system by generating Phase 7 tasks"
phase = "phase-1-template-system"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Use sprint-plan skill ENHANCE mode to generate Phase 7 tasks:

1. Prepare sprint context:
   - Review current ACTIVE_SPRINT.toml (Phase 0-6 complete)
   - Identify sprint type (infrastructure + documentation)
   - Determine conditional tasks needed (no publishing, has UX changes)

2. Run sprint-plan skill ENHANCE mode:
   - Prompt: "Normalize THIS sprint by adding Phase 7 quality tasks from SPRINT_TEMPLATE"
   - Expected: Skill loads SPRINT_TEMPLATE.toml
   - Expected: Skill injects required + suggested + retrospective tasks
   - Expected: Skill SKIPS publishing tasks (not a release sprint)
   - Expected: Skill INCLUDES UX tasks (Phase 0b has UX polish)

3. Verify generated tasks:
   - Count: 13 required + 4 suggested + 3 UX + 2 retro = 22 tasks
   - IDs: DOC-001 through DOC-004, QA-001 through QA-004, etc.
   - Format: All tasks have rich format (why, context, reasoning_chain, etc.)
   - Categories: All match SPRINT_TEMPLATE.toml structure

4. Validate with SprintSchemaValidator:
   - Add generated Phase 7 tasks to ACTIVE_SPRINT.toml
   - Save file (triggers FileSystemWatcher)
   - Expected: SprintSchemaValidator passes (no errors)
   - Expected: Sprint panel loads successfully with Phase 7 tasks

5. Test TOML parsing:
   - SprintLoader.parseTomlTasks() extracts Phase 7
   - All rich fields parse correctly
   - Sprint panel displays 22 new tasks

6. Manual verification:
   - Review each generated task for accuracy
   - Verify task dependencies correct
   - Verify estimated times reasonable
   - Make any necessary adjustments

7. Document findings:
   - Note any template improvements needed
   - Update SPRINT_TEMPLATE.toml if gaps found
   - Document template version (v1.0.0 validated)
"""
estimated_lines = 1500
estimated_time = "2-3 hours"
dependencies = ["TEMPLATE-001", "TEMPLATE-002", "TEMPLATE-003", "TEMPLATE-004", "TEMPLATE-005"]
agent = "infrastructure-agent"

deliverables = [
    "Phase 7 tasks generated (22 tasks)",
    "ACTIVE_SPRINT.toml updated with Phase 7",
    "Template system validated end-to-end",
    "Integration test passed (all 4 layers working)",
    "Template version v1.0.0 marked as validated",
    "Findings documented (template improvements noted)"
]

performance_target = "Template injection + validation completes in <5 seconds"

patterns = ["Pattern-TDD-001", "Pattern-SPRINT-TEMPLATE-001", "Pattern-TRACKING-001"]

files_to_modify = [
    "internal/sprints/ACTIVE_SPRINT.toml (add Phase 7 section, ~1500 lines)",
    "internal/sprints/SPRINT_TEMPLATE.toml (update version metadata, mark as validated)"
]

validation_criteria = [
    "Run: /sprint-plan ENHANCE 'normalize THIS sprint'",
    "Output: 22 Phase 7 tasks generated",
    "Check: DOC-001, QA-001, INFRA-001, UX-001, RETRO-001 present",
    "Check: PUB-001 ABSENT (not release sprint)",
    "Add Phase 7 to ACTIVE_SPRINT.toml, save file",
    "Output: SprintSchemaValidator passes, no errors",
    "Open Sprint Panel in VS Code",
    "Output: Phase 7 tasks visible with rich format",
    "Verify: All 22 tasks display correctly",
    "Manual review: Task quality high, descriptions accurate",
    "Integration: Template injection test - 22 tasks generated",
    "Integration: Static validation test - passes with no errors",
    "Integration: Parsing test - all rich fields extracted",
    "Integration: UI display test - all 22 tasks visible",
    "Integration: Manual review - task quality high"
]

why = """
Validate template system works end-to-end by using it to generate Phase 7 tasks.
Meta-level testing: Build infrastructure (Phase 1), immediately use it (Phase 7).
Dogfooding: Template system generates its own normalization tasks.
Integration test covering all 4 template enforcement layers.
"""

context = """
Template system build complete (TEMPLATE-001 through TEMPLATE-005):
- SPRINT_TEMPLATE.toml: 27 normalized tasks defined
- Sprint-plan skill: Auto-injection logic implemented
- SprintSchemaValidator: Static validation implemented
- WorkflowCheck: Runtime enforcement implemented
- Documentation: SPRINT_TEMPLATE_GUIDE.md created

Test strategy: Use the system to normalize THIS sprint
- Sprint contains Phase 0-6 (feature tasks already defined)
- Need to add Phase 7 (normalization tasks from template)
- Inject DOC-001, QA-001, INFRA-001, RETRO-001, etc.
- Validate all 4 enforcement layers work

This tests:
1. Template injection: sprint-plan skill loads and merges template
2. Static validation: SprintSchemaValidator accepts generated tasks
3. TOML parsing: SprintLoader displays tasks in panel
4. Runtime enforcement: WorkflowCheck validates completion (later)

Meta-level benefit: Template system generates its own quality checks
- Phase 7 ensures this sprint meets quality bar
- Dogfooding validates the template we just built
- Self-referential quality improvement

Reference: Pattern-TDD-001 (Test-Driven Development)
This is the integration test for template system
"""

reasoning_chain = [
    "1. Developer runs: /sprint-plan ENHANCE 'normalize THIS sprint'",
    "2. Sprint-plan skill activates ENHANCE mode",
    "3. Skill loads SPRINT_TEMPLATE.toml:",
    "   - 13 required tasks extracted",
    "   - 4 suggested tasks extracted",
    "   - 8 conditional tasks extracted (5 publishing + 3 UX)",
    "   - 2 retrospective tasks extracted",
    "4. Skill analyzes current sprint:",
    "   - Sprint name: 'Phase 0-6 Feature Development'",
    "   - No 'release' or 'publish' keyword â†’ SKIP publishing tasks",
    "   - Has Phase 0b (UX polish) â†’ INCLUDE UX tasks (3)",
    "   - Always include retrospective (2)",
    "5. Skill generates Phase 7 tasks:",
    "   - 13 required + 4 suggested + 3 UX + 2 retro = 22 tasks",
    "   - Task IDs: DOC-001, QA-001, INFRA-001, UX-001, RETRO-001, etc.",
    "   - All tasks in rich format (matching BACKUP.toml style)",
    "6. Skill returns unified sprint TOML:",
    "   - Phase 0-6: Existing feature tasks",
    "   - Phase 7: NEW template tasks (22)",
    "   - Total sprint: ~90 tasks",
    "7. Developer copies Phase 7 section to ACTIVE_SPRINT.toml",
    "8. Developer saves file (Ctrl+S)",
    "9. FileSystemWatcher triggers SprintSchemaValidator:",
    "   - validateTemplateCompliance() runs",
    "   - Checks for DOC-001, QA-001, etc.",
    "   - All required tasks present âœ…",
    "   - Validation passes âœ…",
    "10. Sprint panel reloads:",
    "    - SprintLoader.parseTomlTasks() extracts Phase 7",
    "    - All 22 tasks display with rich format",
    "    - Template system validated end-to-end âœ…"
]

success_impact = """
After TEMPLATE-006 complete:
âœ… Template system validated end-to-end
âœ… Sprint-plan skill injection works (TEMPLATE-002)
âœ… SprintSchemaValidator accepts generated tasks (TEMPLATE-003)
âœ… SprintLoader parses rich fields correctly
âœ… Sprint panel displays Phase 7 tasks beautifully
âœ… Dogfooding: Template generates its own quality checks
âœ… THIS sprint now has 22 normalization tasks
âœ… Phase 7 ensures sprint meets quality bar
âœ… Template version v1.0.0 marked as validated
âœ… Integration test proves all 4 layers work
âœ… Ready to use template for ALL future sprints
"""

# =============================================================================
# PHASE 0b: UX POLISH (v0.16.7 bug fixes) - AFTER TEMPLATE SYSTEM
# =============================================================================

# These bugs were identified during manual testing (MANUAL_TEST_v0.16.7.md).
# Addressed AFTER Phase 1 (Template System) to ensure consistent sprint task format.

[tasks.UX-001]
id = "UX-001"
name = "Universal Enhancement Pattern - Fix ALL enhancement buttons"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
ALL enhancement buttons broken - same root cause.
User quote: "Enhance is supposed to work every single time the exact same way".
None of the buttons load enhanced prompts to text area for user review.
"""

context = """
Affected buttons (6 total):
1. Code Analyzer (Test 5.1) - Removed, shows "will be implemented" message
2. Sprint Planner (Test 5.2) - Broken, doesn't load to text area
3. Bug Report Enhance (Test 5.3) - Form works, but doesn't send to text area
4. Feature Request Enhance (Test 5.4) - Same issue as Bug Report
5. Start Next Task button (Test 4.6) - Shows dialog instead of loading to text area
6. Individual task play buttons (Test 4.7) - Lock up, do nothing

Root cause: All buttons try to do something automatically instead of following
the established pattern used elsewhere in the codebase.
"""

description = """
Implement UNIVERSAL ENHANCEMENT PATTERN for all 6 buttons:

Pattern:
1. Click button (any enhancement button)
2. Extract relevant context:
   - Code Analyzer: workspace structure, languages, frameworks, patterns
   - Sprint Planner: sprint requirements (prompt user or use defaults)
   - Bug Report: form data (title, description, severity, component)
   - Feature Request: form data (title, use case, solution, category)
   - Task buttons: TOML task fields (deliverables, context, why, validation_criteria, patterns, files_to_modify, reasoning_chain)
3. Generate enhanced prompt using PromptEnhancer service
4. **Load enhanced prompt into main text area** â† CRITICAL STEP
5. User reviews enhanced prompt
6. User selects terminal
7. User sends with Ctrl+Enter (or Send button)

Implementation:
1. Restore Code Analyzer button functionality
2. Fix Sprint Planner to load to text area
3. Fix Bug Report Enhance â†’ text area flow
4. Fix Feature Request Enhance â†’ text area flow
5. Fix Start Next Task â†’ extract task from TOML â†’ enhance â†’ text area
6. Fix individual play buttons â†’ same as #5

All buttons MUST end with:
  webview.postMessage({ type: 'populateTextArea', text: enhancedPrompt });

No exceptions. No dialogs. No automatic execution.
"""

test_requirements = """
Test ALL 6 buttons:
1. Code Analyzer â†’ Enhanced workspace prompt in text area
2. Sprint Planner â†’ Enhanced sprint planning prompt in text area
3. Bug Report form â†’ Fill â†’ Enhance â†’ Enhanced bug report in text area
4. Feature Request form â†’ Fill â†’ Enhance â†’ Enhanced feature request in text area
5. Start Next Task button â†’ Enhanced task prompt in text area
6. Individual task play button â†’ Enhanced task prompt in text area

All must follow same pattern. No dialogs, no automatic execution.
"""

estimated_time = "4-6 hours"
dependencies = ["TEMPLATE-006"]

deliverables = [
    "All 6 enhancement buttons fixed",
    "Universal pattern implemented consistently",
    "All buttons load to text area (verified)",
    "No dialogs, no automatic execution",
    "voicePanel.ts updated with consistent enhancement handlers",
    "TaskStarter.ts updated to load to text area"
]

patterns = ["Pattern-ENHANCEMENT-001 (Universal Enhancement Pattern)"]
files_to_modify = [
    "vscode-lumina/src/commands/voicePanel.ts (enhancement button handlers)",
    "vscode-lumina/src/services/TaskStarter.ts (task enhancement)",
    "vscode-lumina/src/services/PromptEnhancer.ts (enhancement service)"
]

[tasks.UX-002]
id = "UX-002"
name = "Integrate semantic search into PromptEnhancer"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
PromptEnhancer uses keyword matching â†’ produces template prompts.
ContextEnhancer uses Voyage AI + Supabase semantic search â†’ produces intelligent prompts.
Test 2.5 failed because keyword matching isn't good enough.
"""

context = """
Current (PromptEnhancer.ts:590-624): Simple keyword matching
Better (ContextEnhancer.ts:159-177): Voyage-3-large embeddings + Supabase pgvector

ContextEnhancer features:
- 1024-dim embeddings
- HNSW index for fast search
- 42 deployed patterns with confidence scores
- Multi-source context aggregation
- <500ms performance target
"""

description = """
Replace keyword matching with semantic search in PromptEnhancer:

1. Extract pattern matching logic from ContextEnhancer.ts
2. Integrate into PromptEnhancer.findRelevantPatterns()
3. Replace string matching with semantic search
4. Use Voyage AI embeddings + Supabase query
5. Return confidence-scored patterns
6. Maintain backward compatibility (fallback to keywords if Supabase unavailable)
"""

estimated_time = "2-3 hours"
dependencies = ["UX-001"]

deliverables = [
    "PromptEnhancer uses semantic search",
    "Pattern matching improved from keywords to AI",
    "Test 2.5 scenarios passing",
    "Performance <500ms maintained"
]

files_to_modify = [
    "vscode-lumina/src/services/PromptEnhancer.ts (pattern matching)",
    "vscode-lumina/src/terminal/context-enhancer.ts (extract semantic search)",
    "vscode-lumina/src/ipc_dogfooding/PatternRecommender.ts (Supabase integration)"
]

[tasks.UX-003]
id = "UX-003"
name = "Show/Hide Completed Tasks checkbox"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Feature was removed but is useful for managing large sprints.
Test 4.9 failed - checkbox no longer present.
"""

description = """
Refactor Show/Hide Completed Tasks checkbox back into sprint UI:

1. Add checkbox to sprint header (near task statistics)
2. Filter task list based on checkbox state
3. Persist state across reloads (localStorage)
4. Update task statistics when filter active
"""

estimated_time = "1 hour"
dependencies = []

deliverables = [
    "Show/Hide checkbox added to sprint UI",
    "Filter logic working",
    "State persists across reloads"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (sprint section)"]

[tasks.UX-004]
id = "UX-004"
name = "Remove button text, keep icons only"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
UI cleanup: Buttons should be icon-only for cleaner look.
User said: "Get rid of the send words in the toolbar or the record that's got to go away".
"""

description = """
Remove text labels from toolbar buttons:

1. "Record Voice" â†’ ðŸŽ¤ icon only
2. "Send" â†’ icon only
3. "Send to Terminal" â†’ icon only

Keep tooltips for hover explanation.
"""

estimated_time = "15 minutes"
dependencies = []

deliverables = [
    "Toolbar buttons show icons only",
    "Text labels removed",
    "Tooltips remain for hover"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (toolbar HTML)"]

[tasks.UX-005]
id = "UX-005"
name = "Move Start Next Task button to sprint header"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Button currently takes entire row (wasteful).
Should be small play icon next to refresh button.
User said: "Put that in between the sprint Field drop down and the refresh button".
"""

description = """
Reposition Start Next Task button:

Current: Large button in its own row
Needed: Small play icon (â–¶ï¸) between sprint dropdown and refresh button

Layout: [Sprint Dropdown] [â–¶ï¸] [Refresh]
"""

estimated_time = "30 minutes"
dependencies = []

deliverables = [
    "Button moved to sprint header",
    "Small play icon instead of large button",
    "Extra row space removed"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (sprint header HTML)"]

[tasks.UX-006]
id = "UX-006"
name = "Text area UI improvements"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Test 6.4 failed - can't resize text area.
User requested: Remove label, reposition text area, add resize handle.
"""

description = """
Three text area improvements:

1. Add resize handle (drag up/down)
   - Minimum height: 3 lines
   - Maximum height: 20 lines
   - Persist size across reloads

2. Remove "Command / Transcription" label
   - User said: "We don't need that at all"

3. Move text area directly below toolbar
   - No label in between
   - Cleaner layout
"""

estimated_time = "1 hour"
dependencies = []

deliverables = [
    "Resize handle working",
    "Label removed",
    "Text area repositioned below toolbar"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (text area HTML + CSS)"]

[tasks.UX-007]
id = "UX-007"
name = "Ctrl+Shift+Enter send + submit hotkey"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Convenience feature: Auto-execute without editing.
Current: Ctrl+Enter sends only (allows editing).
Needed: Ctrl+Shift+Enter sends + submits (immediate execution).
"""

description = """
Add keyboard shortcut:

Current:
- Ctrl+Enter â†’ Send to terminal (allows editing)

Add:
- Ctrl+Shift+Enter â†’ Send to terminal AND submit (auto-execute)

Implementation:
1. Add keyboard listener for Ctrl+Shift+Enter
2. Send text to terminal
3. Send Enter keystroke to terminal (auto-execute)
"""

estimated_time = "30 minutes"
dependencies = []

deliverables = [
    "Ctrl+Shift+Enter hotkey working",
    "Auto-execute after sending",
    "Ctrl+Enter still works (send only)"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (keyboard listeners)"]

[tasks.UX-008]
id = "UX-008"
name = "Remove terminal edit button"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Pencil/edit button is pointless - editing must be done in VS Code terminal tab header.
UI cleanup.
"""

description = """
Remove pencil/edit button from terminal list UI.

Button serves no purpose (can't edit from Voice panel).
"""

estimated_time = "5 minutes"
dependencies = []

deliverables = [
    "Edit button removed from terminal list"
]

files_to_modify = ["vscode-lumina/src/commands/voicePanel.ts (terminal list HTML)"]

[tasks.UX-009]
id = "UX-009"
name = "Terminal selection persistence"
status = "pending"
phase = "phase-0b-ux-polish"
agent = "infrastructure-agent"

why = """
Terminal selection keeps resetting to first terminal instead of staying on selected one.
User must re-select terminal repeatedly, causing frustration and workflow interruption.
"""

context = """
Current behavior:
- User selects terminal #3 from list
- Selection works initially
- After some action (unclear what triggers it), selection resets to terminal #1
- User must manually re-select terminal #3

Expected behavior:
- User selects terminal once
- Selection persists until user explicitly changes it
- Survives page refreshes, panel reopens, etc.

Root cause likely:
- Terminal list re-rendering without preserving selection state
- Missing state persistence (localStorage)
- Event handler resetting selection on terminal list updates
"""

description = """
Fix terminal selection persistence:

1. Identify reset trigger:
   - Check terminal list re-render logic
   - Check AutoTerminalSelector state management
   - Check webview refresh behavior

2. Add state persistence:
   - Store selected terminal ID in localStorage
   - Restore selection on panel open
   - Restore selection on terminal list refresh

3. Prevent automatic reset:
   - Only change selection when user explicitly clicks
   - Don't reset on terminal list updates
   - Don't reset on auto-refresh events

4. Test scenarios:
   - Select terminal #3 â†’ Close panel â†’ Reopen â†’ Should still be #3
   - Select terminal #3 â†’ New terminal created â†’ Should still be #3
   - Select terminal #3 â†’ Terminal renamed â†’ Should still be #3
   - Select terminal #3 â†’ Refresh sprint â†’ Should still be #3
"""

test_requirements = """
Manual Testing:
1. Select terminal #3 from list
2. Perform various actions (send text, refresh sprint, open/close panel)
3. Verify selection remains on #3
4. Close VS Code, reopen, verify selection persists

Edge Cases:
- Selected terminal closes â†’ Auto-select next available
- All terminals close â†’ Clear selection
- Selected terminal renamed â†’ Follow the terminal (by ID, not name)
"""

estimated_time = "1-2 hours"
dependencies = []

deliverables = [
    "Terminal selection persists across actions",
    "Selection stored in localStorage",
    "Selection survives panel reopens",
    "Only changes when user explicitly clicks",
    "Edge cases handled (terminal closes, etc.)"
]

patterns = ["Pattern-STATE-001 (Client State Persistence)"]
files_to_modify = [
    "vscode-lumina/src/commands/voicePanel.ts (terminal selection logic)",
    "vscode-lumina/src/services/AutoTerminalSelector.ts (state management)"
]

validation_criteria = [
    "Select terminal #3 â†’ Reopen panel â†’ Still #3",
    "Select terminal #3 â†’ Create new terminal â†’ Still #3",
    "Select terminal #3 â†’ Refresh sprint â†’ Still #3",
    "Selected terminal closes â†’ Auto-select next available"
]

# =============================================================================
# PHASE 2: FOUNDATION (Variable substitution) - CRITICAL
# =============================================================================

# Self-Configuration System tasks from ACTIVE_SPRINT_SELF_CONFIG_v1.0.toml
# Originally Phase 1, now Phase 2 (after Protection and Template building)

[tasks.SELF-001]
id = "SELF-001"
name = "Implement VariableResolver service"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = """
Implement VariableResolver service with recursive variable substitution:

1. Core resolve() method:
   - Input: template string + variable dictionary
   - Output: resolved string with all VARS replaced
   - Regex pattern for variable matching

2. Nested variable support:
   - {{A}} resolves to {{B}}, which resolves to "value"
   - Maximum recursion depth: 10 levels
   - Prevents infinite loops

3. Error handling:
   - Undefined variable: throw VariableNotFoundError
   - Circular reference: throw CircularDependencyError
   - Invalid syntax: throw VariableSyntaxError

4. Performance optimization:
   - Cache resolved values (Map)
   - Single-pass resolution when possible
   - Regex precompilation
"""
estimated_lines = 300
estimated_time = "8-10 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "VariableResolver.ts service (recursive resolver)",
    "resolve() method with nested support",
    "Circular dependency detection",
    "Error classes (VariableNotFoundError, CircularDependencyError)",
    "Performance optimization (caching)",
    "Unit tests (95% coverage)"
]

performance_target = "Resolve 100 variables < 10ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/VariableResolver.ts (create new file, ~300 lines)",
    "vscode-lumina/test/services/VariableResolver.test.ts (create new file, ~200 lines)"
]

validation_criteria = [
    "Run: npm test -- VariableResolver.test.ts",
    "Output: All 7 test cases pass",
    "Test: resolve('{{VAR}}', {VAR: 'test'})",
    "Output: 'test'",
    "Test: resolve('{{A}}', {A: '{{B}}', B: 'final'})",
    "Output: 'final' (nested resolution)",
    "Test: resolve('{{UNDEF}}', {})",
    "Output: VariableNotFoundError thrown",
    "Check: Test coverage report",
    "Output: â‰¥95% coverage (lines, branches, functions)",
    "TDD RED: resolve with simple variable â†’ 'value'",
    "TDD RED: resolve with nested variable â†’ 'value'",
    "TDD RED: resolve with undefined variable â†’ throws error",
    "TDD RED: resolve with circular reference â†’ throws error",
    "TDD RED: resolve multiple variables â†’ all replaced",
    "TDD RED: Performance test â†’ <10ms for 100 variables"
]

why = """
Core infrastructure for template customization.
Without this, we can't substitute {{VARIABLES}} â†’ project-specific values.
Foundation for making Ã†therLight work with ANY language/framework.
"""

context = """
Currently: All values hardcoded (npm, TypeScript, .ts extensions).
Need: Generic templates with {{BUILD_COMMAND}}, {{TEST_COMMAND}}, {{FILE_EXTENSION}}.
Solution: VariableResolver that recursively replaces {{VARS}} with config values.

Example transformation:
- Input: "Run {{BUILD_COMMAND}} to build {{FILE_EXTENSION}} files"
- Config: {BUILD_COMMAND: "cargo build", FILE_EXTENSION: ".rs"}
- Output: "Run cargo build to build .rs files"

Supports nested variables: {{A}} â†’ {{B}} â†’ "final value"
Detects circular references: {{A}} â†’ {{B}} â†’ {{A}} â†’ ERROR
"""

reasoning_chain = [
    "1. User generates config: {BUILD_COMMAND: 'npm run build'}",
    "2. Template has: 'Run {{BUILD_COMMAND}} to compile'",
    "3. VariableResolver.resolve(template, config) called",
    "4. Regex finds {{BUILD_COMMAND}} match",
    "5. Lookup in config: BUILD_COMMAND = 'npm run build'",
    "6. Replace match: 'Run npm run build to compile'",
    "7. Check for nested variables in replacement",
    "8. No nested vars found, return resolved string",
    "9. Result: 'Run npm run build to compile'",
    "10. Template fully customized for user's project"
]

success_impact = """
After SELF-001 complete:
âœ… Variable substitution works ({{VAR}} â†’ value)
âœ… Nested variables work ({{A}} â†’ {{B}} â†’ value)
âœ… Circular references detected (prevents infinite loops)
âœ… Error messages clear and actionable
âœ… 95% test coverage (infrastructure standard)
âœ… Performance target met (<10ms for 100 variables)
âœ… Foundation for project-specific customization
âœ… Works with ANY language/framework
"""

[tasks.SELF-002]
id = "SELF-002"
name = "Implement ProjectConfigGenerator service"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement ProjectConfigGenerator service: generate() method (DetectionResults + InterviewAnswers â†’ ProjectConfig), schema validation (ajv), default value application, config serialization (JSON pretty-print to .aetherlight/project-config.json), error classes (ValidationError, MissingRequiredFieldError)"
estimated_lines = 300
estimated_time = "10-12 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "ProjectConfigGenerator.ts service",
    "generate() method (detection + interview â†’ config)",
    "Schema validation integration (ajv)",
    "Default value application logic",
    "Config serialization (JSON pretty-print)",
    "Error classes (ValidationError, MissingRequiredFieldError)",
    "Unit tests (90% coverage)"
]

performance_target = "Generate config < 50ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/ProjectConfigGenerator.ts",
    "vscode-lumina/test/services/ProjectConfigGenerator.test.ts"
]

validation_criteria = [
    "Run: npm test -- ProjectConfigGenerator.test.ts",
    "Output: All 7 test cases pass",
    "Test: generate({language: 'typescript'}, {})",
    "Output: Valid config with defaults applied",
    "Test: generate({}, {})",
    "Output: MissingRequiredFieldError thrown",
    "Test: generate({language: 'rust'}, {buildCommand: 'cargo build'})",
    "Output: Interview override wins (BUILD_COMMAND: 'cargo build')",
    "Check: Test coverage report",
    "Output: â‰¥90% coverage"
]

why = """
Generates project-config.json from detection + interview results.
Central configuration drives all customization across Ã†therLight.
Without this, detection and interview data has nowhere to go.
"""

context = """
ProjectConfigGenerator bridges detection â†’ configuration â†’ customization pipeline.

Input sources:
1. TechStackDetector: language, framework, package manager
2. ToolDetector: build tool, test runner, linter
3. WorkflowDetector: git workflow, CI/CD, deployment
4. Interview results: user preferences, overrides

Output: project-config.json
- Validated against schema (SELF-003)
- Contains all {{VARIABLES}} for templates
- Drives VariableResolver (SELF-001)

Example flow:
- Detected: TypeScript + Jest + npm
- Interview: user prefers 'npm run build:prod' over 'npm run build'
- Generated: {BUILD_COMMAND: 'npm run build:prod', TEST_COMMAND: 'npm test', FILE_EXTENSION: '.ts'}
"""

reasoning_chain = [
    "1. Detection phase completes: {language: 'typescript', packageManager: 'npm'}",
    "2. Interview phase completes: {buildCommand: 'npm run build:prod'}",
    "3. ProjectConfigGenerator.generate(detection, interview) called",
    "4. Merge detection + interview:",
    "   - language: 'typescript' (detected)",
    "   - packageManager: 'npm' (detected)",
    "   - BUILD_COMMAND: 'npm run build:prod' (interview override)",
    "5. Apply defaults for missing fields:",
    "   - TEST_COMMAND: 'npm test' (default)",
    "   - FILE_EXTENSION: '.ts' (derived from language)",
    "6. Validate against schema (SELF-003):",
    "   - Check required fields present",
    "   - Check value types correct",
    "   - Validation passes âœ…",
    "7. Serialize to JSON: project-config.json",
    "8. Write to .aetherlight/project-config.json",
    "9. Config ready for VariableResolver (SELF-001)",
    "10. All templates can now be customized"
]

success_impact = """
After SELF-002 complete:
âœ… Detection + interview â†’ unified config
âœ… Schema validation prevents invalid configs
âœ… Default values fill gaps
âœ… project-config.json generated and saved
âœ… 90% test coverage (infrastructure standard)
âœ… Performance target met (<50ms generation)
âœ… Bridge between detection and customization
âœ… Ready for VariableResolver integration
"""

[tasks.SELF-003]
id = "SELF-003"
name = "Define project-config.json schema"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create JSON Schema for project-config.json with all variable definitions, constraints, default values. Generate TypeScript types from schema. Implement validation function using ajv. Create comprehensive schema tests (90% coverage)."
estimated_lines = 400
estimated_time = "6-8 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "JSON Schema file (.aetherlight/schemas/project-config-v1.json)",
    "TypeScript types (ProjectConfig interface)",
    "Validation function (ajv integration)",
    "Schema tests (90% coverage)"
]

performance_target = "Schema validation < 10ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    ".aetherlight/schemas/project-config-v1.json",
    "vscode-lumina/src/types/ProjectConfig.ts",
    "vscode-lumina/test/schemas/project-config.test.ts"
]

validation_criteria = [
    "Run: npm test -- project-config.test.ts",
    "Output: All schema validation tests pass",
    "Test: Validate valid config with all fields",
    "Output: Validation passes",
    "Test: Validate config missing required field",
    "Output: ValidationError with clear message",
    "Test: Validate config with wrong type (string instead of number)",
    "Output: ValidationError specifying field and expected type",
    "Check: Schema includes all variables from variable-reference.md",
    "Output: All variables documented in schema"
]

why = """
Formal schema validates generated configs.
Ensures consistency, enables TypeScript types, documents structure.
Without schema: invalid configs reach production, TypeScript has no type safety, variables undefined.
With schema: compile-time safety + runtime validation.
"""

context = """
project-config.json schema defines ALL variables for self-configuration system.

Schema serves 4 purposes:
1. Runtime validation (ajv) - ProjectConfigGenerator validates before saving
2. TypeScript types - IDE autocomplete + compile-time checking
3. Documentation - Single source of truth for all variables
4. Default values - Schema defines fallbacks when detection/interview incomplete

Example variables in schema:
- language: string (required) - "typescript", "rust", "python"
- packageManager: string (required) - "npm", "yarn", "pnpm", "cargo"
- BUILD_COMMAND: string (optional, default: "npm run build")
- TEST_COMMAND: string (optional, default: "npm test")
- FILE_EXTENSION: string (derived from language)

Schema references:
- JSON Schema Draft 7
- ajv for validation
- json-schema-to-typescript for TypeScript generation
"""

reasoning_chain = [
    "1. Review all variables used in templates (grep {{VAR}} across codebase)",
    "2. Categorize variables: language config, build/test commands, paths, framework-specific",
    "3. Define schema structure:",
    "   - required: [language, packageManager]",
    "   - optional: [BUILD_COMMAND, TEST_COMMAND, LINT_COMMAND, etc.]",
    "   - derived: [FILE_EXTENSION (from language), FRAMEWORK_VERSION]",
    "4. Add constraints:",
    "   - language: enum [typescript, javascript, rust, python, go, java]",
    "   - packageManager: enum [npm, yarn, pnpm, cargo, pip, maven]",
    "   - *_COMMAND: string pattern (must be valid shell command)",
    "5. Define default values for optional fields",
    "6. Generate TypeScript types: json-schema-to-typescript",
    "7. Implement validation function: ajv.compile(schema)",
    "8. Write tests:",
    "   - Valid config (all fields) â†’ passes",
    "   - Valid config (only required) â†’ passes with defaults",
    "   - Invalid config (missing required) â†’ fails with error",
    "   - Invalid config (wrong type) â†’ fails with type error",
    "9. Performance test: validate 1000 configs < 10ms",
    "10. Schema ready for ProjectConfigGenerator (SELF-002)"
]

success_impact = """
After SELF-003 complete:
âœ… Formal schema for all project variables
âœ… TypeScript types for IDE autocomplete
âœ… Runtime validation prevents invalid configs
âœ… Default values documented and enforced
âœ… Single source of truth for variable structure
âœ… 90% test coverage (infrastructure standard)
âœ… Performance target met (<10ms validation)
âœ… Ready for ProjectConfigGenerator integration
"""

[tasks.SELF-003A]
id = "SELF-003A"
name = "Document all variables with comprehensive context"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create comprehensive variable documentation in variable-reference.md: all variables organized by category (language config, build/test, paths, framework-specific), examples for each variable across Node.js/Rust/Python, valid values, default behaviors, detection sources."
estimated_lines = 800
estimated_time = "8-10 hours"
dependencies = []
agent = "documentation-agent"

deliverables = [
    "variable-reference.md (comprehensive docs for all variables)",
    "Variables organized by category (language, build, test, paths, framework)",
    "Examples for each variable (Node.js, Rust, Python)",
    "Valid values and constraints documented",
    "Detection sources noted (TechStackDetector, interview, defaults)"
]

performance_target = "Documentation complete and accurate"

patterns = ["Pattern-DOCS-001"]

files_to_modify = [
    "docs/variable-reference.md",
    ".aetherlight/schemas/variables-catalog.json"
]

validation_criteria = [
    "Check: variable-reference.md exists and is >500 lines",
    "Output: File exists with comprehensive content",
    "Review: All variables from schema (SELF-003) documented",
    "Output: 100% variable coverage",
    "Review: Each variable has example for 3+ languages/frameworks",
    "Output: Examples complete (Node.js, Rust, Python minimum)",
    "Review: Valid values and constraints documented",
    "Output: Clear value constraints for enums and patterns",
    "Review: Detection sources noted for each variable",
    "Output: Source attribution (detected/interview/default) present"
]

why = """
CRITICAL: Variables are foundation of self-configuration.
Without clear documentation, AI and humans won't understand meaning.
Reduces bugs by 50% (Pattern-DOCS-001).

Documentation serves 3 audiences:
1. AI agents - Need to understand variable purpose for template resolution
2. Developers - Need to debug/customize variable resolution
3. End users - Need to override variables via interview
"""

context = """
variable-reference.md is THE documentation for ALL self-configuration variables.

Structure:
1. **Language Configuration Variables**
   - language, framework, packageManager
   - FILE_EXTENSION (derived)
   - Examples: TypeScript â†’ .ts, Rust â†’ .rs, Python â†’ .py

2. **Build & Test Variables**
   - BUILD_COMMAND, TEST_COMMAND, LINT_COMMAND
   - Examples: npm run build, cargo build, python setup.py test

3. **Path Variables**
   - SOURCE_DIR, TEST_DIR, BUILD_OUTPUT_DIR
   - Examples: src/, test/, dist/

4. **Framework-Specific Variables**
   - FRAMEWORK_VERSION, FRAMEWORK_CONFIG_FILE
   - Examples: React 18, angular.json, Cargo.toml

Each variable documented with:
- Purpose: What is this variable used for?
- Detection: How is this detected? (TechStackDetector, interview, default)
- Valid values: Enum/pattern constraints
- Examples: 3+ language/framework examples
- Default: Fallback if not detected

Related files:
- .aetherlight/schemas/project-config-v1.json (SELF-003) - Schema definitions
- VariableResolver.ts (SELF-001) - Variable resolution logic
"""

reasoning_chain = [
    "1. Review project-config-v1.json schema (SELF-003) for all variables",
    "2. Categorize variables: language, build/test, paths, framework-specific",
    "3. For each variable, document:",
    "   - Purpose (why does this exist?)",
    "   - Detection source (TechStackDetector method, interview question, default)",
    "   - Valid values (enum list or pattern)",
    "   - Examples (Node.js, Rust, Python minimum)",
    "   - Default behavior (what happens if not detected?)",
    "4. Create variable-reference.md structure:",
    "   - Introduction (what are variables, how they work)",
    "   - Language Configuration Variables section",
    "   - Build & Test Variables section",
    "   - Path Variables section",
    "   - Framework-Specific Variables section",
    "5. Write examples for 3+ languages per variable:",
    "   - Node.js/TypeScript examples",
    "   - Rust examples",
    "   - Python examples",
    "   - Go examples (bonus)",
    "6. Document edge cases:",
    "   - Multiple package managers detected (npm + yarn)",
    "   - No build command found (monorepo)",
    "   - Custom test runners (Vitest instead of Jest)",
    "7. Add troubleshooting section:",
    "   - Variable not resolving? Check detection logs",
    "   - Wrong value detected? Override via interview",
    "   - Need custom variable? Extend schema",
    "8. Create variables-catalog.json (machine-readable)",
    "9. Link from CLAUDE.md and sprint docs",
    "10. Ready for AI agents to reference during template resolution"
]

success_impact = """
After SELF-003A complete:
âœ… Comprehensive variable documentation (>500 lines)
âœ… 100% variable coverage from schema
âœ… Examples for 3+ languages/frameworks
âœ… Valid values and constraints documented
âœ… Detection sources attributed
âœ… AI agents can understand variable purpose
âœ… Developers can debug/customize variables
âœ… Foundation for self-configuration interview (SELF-005)
"""

[tasks.SELF-004]
id = "SELF-004"
name = "Integrate inquirer.js for CLI prompts"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement InterviewEngine service with inquirer.js integration for interactive CLI prompts. Support question types: input, list, confirm, checkbox. Load interview templates from .aetherlight/interviews/. Answer validation and conditional questions. 85% test coverage."
estimated_lines = 350
estimated_time = "8-10 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "InterviewEngine service with inquirer.js",
    "Interview template loader (.aetherlight/interviews/)",
    "Question type support (input, list, confirm, checkbox)",
    "Answer validation logic",
    "Conditional question flow",
    "Unit tests (85% coverage)"
]

performance_target = "Interview prompt response < 100ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/InterviewEngine.ts",
    "vscode-lumina/test/services/InterviewEngine.test.ts"
]

validation_criteria = [
    "Run: npm test -- InterviewEngine.test.ts",
    "Output: All tests pass (85% coverage)",
    "Test: Load interview template from .aetherlight/interviews/",
    "Output: Template loaded successfully",
    "Test: Prompt user with input question",
    "Output: User input captured",
    "Test: Prompt with list question (choose from options)",
    "Output: User selection captured",
    "Test: Conditional question (show if previous answer matches)",
    "Output: Conditional logic works",
    "Test: Answer validation (reject invalid input)",
    "Output: Validation error shown, re-prompt"
]

why = """
Interactive CLI prompts for user interview.
Inquirer.js is industry standard (npm init, create-react-app).

Without InterviewEngine: Users can't provide custom preferences, detection alone insufficient.
With InterviewEngine: Users override defaults, customize workflows, provide project-specific values.
"""

context = """
InterviewEngine runs interactive CLI interviews to gather user preferences.

Question types supported:
1. **input** - Free text input (BUILD_COMMAND: "npm run build:prod")
2. **list** - Single selection from options (packageManager: npm | yarn | pnpm)
3. **confirm** - Yes/No (USE_TYPESCRIPT: yes/no)
4. **checkbox** - Multiple selections (FEATURES: [git-hooks, pre-commit, husky])

Interview flow:
1. Load template from .aetherlight/interviews/tech-stack.json
2. For each question:
   - Check conditional (skip if condition not met)
   - Validate answer against schema
   - Store answer in InterviewAnswers object
3. Return InterviewAnswers to ProjectConfigGenerator (SELF-002)

Example interview template:
```json
{
  "questions": [
    {
      "type": "input",
      "name": "BUILD_COMMAND",
      "message": "What command builds your project?",
      "default": "npm run build"
    },
    {
      "type": "list",
      "name": "packageManager",
      "message": "Which package manager?",
      "choices": ["npm", "yarn", "pnpm"]
    }
  ]
}
```

Related files:
- ProjectConfigGenerator (SELF-002) - Consumes interview answers
- variable-reference.md (SELF-003A) - Documents all variables
"""

reasoning_chain = [
    "1. Install inquirer.js: npm install inquirer @types/inquirer",
    "2. Create InterviewEngine.ts service",
    "3. Implement loadTemplate(templatePath) method:",
    "   - Read JSON from .aetherlight/interviews/",
    "   - Parse questions array",
    "   - Validate template structure",
    "4. Implement runInterview(template) method:",
    "   - For each question in template:",
    "     - Check conditional (if field present)",
    "     - Build inquirer prompt config",
    "     - await inquirer.prompt(config)",
    "     - Validate answer (schema validation)",
    "     - Store in answers object",
    "5. Handle question types:",
    "   - input: inquirer.input()",
    "   - list: inquirer.list()",
    "   - confirm: inquirer.confirm()",
    "   - checkbox: inquirer.checkbox()",
    "6. Implement conditional logic:",
    "   - when: (answers) => answers.language === 'typescript'",
    "   - Skip questions based on previous answers",
    "7. Implement validation:",
    "   - validate: (value) => schema.validate(value)",
    "   - Re-prompt on validation failure",
    "8. Write tests (TDD):",
    "   - Load template â†’ template parsed",
    "   - Run interview â†’ answers captured",
    "   - Conditional question â†’ skipped when condition false",
    "   - Invalid answer â†’ validation error, re-prompt",
    "9. Integration test: Full interview flow",
    "10. Ready for SelfConfigWizard integration (SELF-005)"
]

success_impact = """
After SELF-004 complete:
âœ… Interactive CLI prompts working (inquirer.js)
âœ… Interview templates loadable from .aetherlight/interviews/
âœ… Question types supported (input, list, confirm, checkbox)
âœ… Conditional question flow implemented
âœ… Answer validation prevents invalid input
âœ… 85% test coverage (API standard)
âœ… Performance target met (<100ms per prompt)
âœ… Ready for SelfConfigWizard (SELF-005)
"""

[tasks.SELF-005]
id = "SELF-005"
name = "Phase 2 integration tests"
phase = "phase-2-foundation"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create comprehensive integration tests for Phase 2 Foundation: VariableResolver + ProjectConfigGenerator + Schema validation + InterviewEngine. Test full workflow: detection â†’ interview â†’ config generation â†’ variable resolution. 90% coverage."
estimated_lines = 500
estimated_time = "4-6 hours"
dependencies = ["SELF-001", "SELF-002", "SELF-003", "SELF-004"]
agent = "infrastructure-agent"

deliverables = [
    "Integration test suite (phase2.test.ts)",
    "Full workflow tests (detection â†’ interview â†’ config â†’ resolution)",
    "Error case tests (invalid schema, missing variables)",
    "Performance tests (config generation < 50ms)",
    "Coverage report (â‰¥90%)"
]

performance_target = "Full workflow test < 200ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/integration/phase2.test.ts"
]

validation_criteria = [
    "Run: npm test -- phase2.test.ts",
    "Output: All integration tests pass",
    "Test: Full workflow (detection + interview + config + resolution)",
    "Output: project-config.json generated with all variables resolved",
    "Test: Invalid schema (missing required field)",
    "Output: ValidationError thrown by ProjectConfigGenerator",
    "Test: Missing variable in template",
    "Output: VariableResolver throws MissingVariableError",
    "Test: Interview overrides detection",
    "Output: Interview answer takes precedence",
    "Check: Coverage report",
    "Output: â‰¥90% coverage for Phase 2 services"
]

why = """
Ensure all Phase 2 components work together.
Integration tests catch issues unit tests miss.

Without integration tests: Services work individually but fail when integrated.
With integration tests: Full workflow validated, edge cases caught, confidence high.
"""

context = """
Phase 2 integration tests validate the complete self-configuration pipeline.

Test scenarios:
1. **Happy path: Full workflow**
   - TechStackDetector detects TypeScript + npm
   - InterviewEngine prompts user for BUILD_COMMAND
   - ProjectConfigGenerator merges detection + interview
   - Schema validation passes
   - VariableResolver resolves {{BUILD_COMMAND}} in template

2. **Error case: Invalid schema**
   - Detection returns invalid data (missing required field)
   - ProjectConfigGenerator throws ValidationError
   - Error message is clear and actionable

3. **Error case: Missing variable**
   - Template contains {{UNKNOWN_VAR}}
   - VariableResolver throws MissingVariableError
   - Error message shows available variables

4. **Override: Interview > Detection**
   - Detection: BUILD_COMMAND = "npm run build"
   - Interview: BUILD_COMMAND = "npm run build:prod"
   - Result: Interview wins (build:prod)

5. **Performance: Full workflow < 200ms**
   - Run complete pipeline 100 times
   - Average time < 200ms

Related services:
- VariableResolver (SELF-001) - Variable resolution
- ProjectConfigGenerator (SELF-002) - Config generation
- project-config-v1.json (SELF-003) - Schema
- InterviewEngine (SELF-004) - CLI prompts
"""

reasoning_chain = [
    "1. Create test/integration/phase2.test.ts",
    "2. Setup test fixtures:",
    "   - Sample detection results (TypeScript + npm)",
    "   - Sample interview answers (BUILD_COMMAND: 'npm run build:prod')",
    "   - Sample template with {{variables}}",
    "3. Test 1: Happy path (full workflow)",
    "   - Mock TechStackDetector.detect() â†’ detectionResults",
    "   - Mock InterviewEngine.run() â†’ interviewAnswers",
    "   - Call ProjectConfigGenerator.generate(detection, interview) â†’ config",
    "   - Validate config against schema â†’ passes",
    "   - Call VariableResolver.resolve(template, config) â†’ resolved",
    "   - Assert: {{BUILD_COMMAND}} replaced with 'npm run build:prod'",
    "4. Test 2: Invalid schema",
    "   - Mock detection with missing 'language' field",
    "   - Call ProjectConfigGenerator.generate()",
    "   - Assert: ValidationError thrown",
    "   - Assert: Error message mentions 'language' field required",
    "5. Test 3: Missing variable",
    "   - Template contains {{UNKNOWN_VAR}}",
    "   - Call VariableResolver.resolve()",
    "   - Assert: MissingVariableError thrown",
    "   - Assert: Error message lists available variables",
    "6. Test 4: Interview overrides detection",
    "   - Detection: BUILD_COMMAND = 'npm run build'",
    "   - Interview: BUILD_COMMAND = 'npm run build:prod'",
    "   - Generate config",
    "   - Assert: config.BUILD_COMMAND === 'npm run build:prod'",
    "7. Test 5: Performance",
    "   - Run full workflow 100 times",
    "   - Calculate average time",
    "   - Assert: average < 200ms",
    "8. Run all tests: npm test -- phase2.test.ts",
    "9. Generate coverage report",
    "10. Assert: â‰¥90% coverage for Phase 2 services"
]

success_impact = """
After SELF-005 complete:
âœ… Phase 2 integration tests passing
âœ… Full workflow validated (detection â†’ config â†’ resolution)
âœ… Error cases tested (invalid schema, missing variables)
âœ… Interview override behavior verified
âœ… Performance targets met (<200ms full workflow)
âœ… 90% coverage for Phase 2 services
âœ… Confidence HIGH for Phase 2 completion
âœ… Ready to proceed to Phase 3 (Detection)
"""

# =============================================================================
# PHASE 3: DETECTION (Tech stack, tools, workflows) - HIGH PRIORITY
# =============================================================================

[tasks.SELF-006]
id = "SELF-006"
name = "Implement TechStackDetector service"
phase = "phase-3-detection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement TechStackDetector service: auto-detect project language (Node.js, TypeScript, Rust, Python, Go), framework (React, Vue, Express, Tauri), package manager (npm, yarn, pnpm, cargo, pip). File pattern detection (package.json, Cargo.toml, setup.py). Confidence scoring. <200ms performance."
estimated_lines = 600
estimated_time = "8-10 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "TechStackDetector service with detect() method",
    "Language detection (Node.js, TypeScript, Rust, Python, Go)",
    "Framework detection (React, Vue, Express, Tauri, Flask)",
    "Package manager detection (npm, yarn, pnpm, cargo, pip, maven)",
    "Confidence scoring (0.0-1.0)",
    "Unit tests (90% coverage)"
]

performance_target = "Detect tech stack < 200ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/TechStackDetector.ts",
    "vscode-lumina/test/services/TechStackDetector.test.ts"
]

validation_criteria = [
    "Run: npm test -- TechStackDetector.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Detect Node.js project (package.json present)",
    "Output: {language: 'javascript', packageManager: 'npm'}",
    "Test: Detect TypeScript project (tsconfig.json present)",
    "Output: {language: 'typescript', packageManager: 'npm'}",
    "Test: Detect Rust project (Cargo.toml present)",
    "Output: {language: 'rust', packageManager: 'cargo'}",
    "Test: Detect React framework (react in dependencies)",
    "Output: {framework: 'react'}",
    "Test: Performance benchmark",
    "Output: Detection completes in <200ms"
]

why = """
Auto-detect project tech stack (Node.js, TypeScript, Rust, Python).
Reduces user questions, improves UX.

Without TechStackDetector: Users answer 10+ interview questions.
With TechStackDetector: Only 2-3 questions needed, 80% accuracy.
"""

context = """
TechStackDetector analyzes workspace files to determine project language, framework, and package manager.

Detection strategy:
1. **Language Detection** (file patterns)
   - package.json â†’ JavaScript/Node.js
   - tsconfig.json â†’ TypeScript
   - Cargo.toml â†’ Rust
   - setup.py or requirements.txt â†’ Python
   - go.mod â†’ Go
   - pom.xml or build.gradle â†’ Java

2. **Framework Detection** (dependency analysis)
   - Read package.json dependencies
   - Check for: react, vue, express, next, nuxt, tauri
   - Check Cargo.toml for: tauri, actix-web, rocket
   - Check setup.py for: flask, django, fastapi

3. **Package Manager Detection**
   - package-lock.json â†’ npm
   - yarn.lock â†’ yarn
   - pnpm-lock.yaml â†’ pnpm
   - Cargo.lock â†’ cargo
   - requirements.txt â†’ pip

4. **Confidence Scoring**
   - High (0.9-1.0): Multiple indicators (package.json + node_modules + tsconfig.json)
   - Medium (0.7-0.89): Single strong indicator (Cargo.toml)
   - Low (0.5-0.69): Weak indicators (*.ts files but no tsconfig.json)

Related services:
- ProjectConfigGenerator (SELF-002) - Consumes detection results
- InterviewEngine (SELF-004) - Fills gaps in detection
"""

reasoning_chain = [
    "1. Create TechStackDetector.ts service",
    "2. Implement detect() method:",
    "   - Input: workspace root path",
    "   - Output: DetectionResults {language, framework, packageManager, confidence}",
    "3. Scan workspace for indicator files:",
    "   - fs.existsSync('package.json')",
    "   - fs.existsSync('Cargo.toml')",
    "   - fs.existsSync('tsconfig.json')",
    "   - etc.",
    "4. Parse package.json dependencies:",
    "   - Read package.json",
    "   - Check dependencies for 'react', 'vue', 'express'",
    "   - Check devDependencies for 'typescript', 'vite'",
    "5. Detect package manager:",
    "   - Check for lock files (package-lock.json, yarn.lock, pnpm-lock.yaml)",
    "   - Default to npm if package.json but no lock file",
    "6. Calculate confidence score:",
    "   - Start at 0.5 (baseline)",
    "   - +0.2 for each strong indicator (config file)",
    "   - +0.1 for each weak indicator (dependency)",
    "   - Cap at 1.0",
    "7. Write tests (TDD):",
    "   - Mock workspace with package.json â†’ detect Node.js",
    "   - Mock workspace with Cargo.toml â†’ detect Rust",
    "   - Mock workspace with tsconfig.json â†’ detect TypeScript",
    "   - Mock package.json with react dep â†’ detect React framework",
    "   - Performance test: detect in <200ms",
    "8. Integration: Return DetectionResults to ProjectConfigGenerator",
    "9. Coverage: â‰¥90%",
    "10. Ready for ToolDetector integration (SELF-007)"
]

success_impact = """
After SELF-006 complete:
âœ… Auto-detect language (Node.js, TypeScript, Rust, Python, Go)
âœ… Auto-detect framework (React, Vue, Express, Tauri)
âœ… Auto-detect package manager (npm, yarn, pnpm, cargo, pip)
âœ… Confidence scoring for all detections
âœ… Performance target met (<200ms)
âœ… 90% test coverage (infrastructure standard)
âœ… Reduces interview questions by 60%
âœ… Ready for ProjectConfigGenerator integration
"""

[tasks.SELF-007]
id = "SELF-007"
name = "Implement ToolDetector service"
phase = "phase-3-detection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement ToolDetector service: auto-detect build tools (webpack, vite, cargo, maven), test frameworks (Jest, Mocha, pytest, cargo test), linters (ESLint, Clippy, pylint). Extract tool commands from package.json scripts. Confidence scoring. <150ms performance."
estimated_lines = 500
estimated_time = "6-8 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "ToolDetector service with detect() method",
    "Build tool detection (webpack, vite, rollup, cargo, maven)",
    "Test framework detection (Jest, Mocha, Vitest, pytest)",
    "Linter detection (ESLint, Prettier, Clippy, pylint)",
    "Command extraction from package.json scripts",
    "Unit tests (90% coverage)"
]

performance_target = "Detect tools < 150ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/ToolDetector.ts",
    "vscode-lumina/test/services/ToolDetector.test.ts"
]

validation_criteria = [
    "Run: npm test -- ToolDetector.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Detect Jest (jest in devDependencies)",
    "Output: {testFramework: 'jest', TEST_COMMAND: 'npm test'}",
    "Test: Detect webpack (webpack in devDependencies)",
    "Output: {buildTool: 'webpack', BUILD_COMMAND: 'npm run build'}",
    "Test: Detect ESLint (eslint in devDependencies)",
    "Output: {linter: 'eslint', LINT_COMMAND: 'npm run lint'}",
    "Test: Extract custom script from package.json",
    "Output: BUILD_COMMAND = 'npm run build:prod' (from scripts.build:prod)",
    "Test: Performance benchmark",
    "Output: Detection completes in <150ms"
]

why = """
Auto-detect tools (package manager, test framework, build system).
Enables accurate {{TOOL_COMMAND}} substitution.

Without ToolDetector: Templates use generic commands ("npm test").
With ToolDetector: Templates use project-specific commands ("npm run test:unit").
"""

context = """
ToolDetector analyzes project configuration to identify development tools and extract commands.

Detection strategy:
1. **Build Tools** (dependency + config file analysis)
   - webpack: webpack in devDeps + webpack.config.js
   - vite: vite in devDeps + vite.config.ts
   - rollup: rollup in devDeps + rollup.config.js
   - cargo: Cargo.toml present
   - maven: pom.xml present

2. **Test Frameworks** (dependency analysis)
   - Jest: jest in devDeps
   - Mocha: mocha in devDeps
   - Vitest: vitest in devDeps
   - pytest: pytest in requirements.txt
   - cargo test: Cargo.toml with [dev-dependencies]

3. **Linters** (dependency + config file)
   - ESLint: eslint in devDeps + .eslintrc.*
   - Prettier: prettier in devDeps + .prettierrc
   - Clippy: rust-toolchain present
   - pylint: pylint in requirements.txt

4. **Command Extraction** (package.json scripts)
   - Read scripts section
   - Extract BUILD_COMMAND from "build" script
   - Extract TEST_COMMAND from "test" script
   - Extract LINT_COMMAND from "lint" script

Related services:
- TechStackDetector (SELF-006) - Detects language/framework first
- ProjectConfigGenerator (SELF-002) - Consumes tool detection results
"""

reasoning_chain = [
    "1. Create ToolDetector.ts service",
    "2. Implement detect() method:",
    "   - Input: workspace root path",
    "   - Output: ToolDetectionResults {buildTool, testFramework, linter, commands}",
    "3. Detect build tools:",
    "   - Check package.json devDependencies for 'webpack', 'vite', 'rollup'",
    "   - Check for config files (webpack.config.js, vite.config.ts)",
    "   - Rust: Cargo.toml â†’ buildTool = 'cargo'",
    "4. Detect test frameworks:",
    "   - Check devDependencies for 'jest', 'mocha', 'vitest'",
    "   - Python: Check requirements.txt for 'pytest'",
    "   - Rust: Check Cargo.toml [dev-dependencies]",
    "5. Detect linters:",
    "   - Check devDependencies for 'eslint', 'prettier'",
    "   - Check for config files (.eslintrc.js, .prettierrc)",
    "   - Rust: Check for rust-toolchain or clippy config",
    "6. Extract commands from package.json scripts:",
    "   - Read scripts.build â†’ BUILD_COMMAND",
    "   - Read scripts.test â†’ TEST_COMMAND",
    "   - Read scripts.lint â†’ LINT_COMMAND",
    "   - Prepend 'npm run' to script names",
    "7. Handle missing tools:",
    "   - If no test framework detected, default TEST_COMMAND = 'npm test'",
    "   - If no build tool detected, default BUILD_COMMAND = 'npm run build'",
    "8. Write tests (TDD):",
    "   - Mock package.json with jest â†’ detect Jest",
    "   - Mock package.json with webpack â†’ detect webpack",
    "   - Mock custom script 'build:prod' â†’ extract command",
    "   - Performance test: detect in <150ms",
    "9. Integration: Return ToolDetectionResults to ProjectConfigGenerator",
    "10. Ready for WorkflowDetector integration (SELF-008)"
]

success_impact = """
After SELF-007 complete:
âœ… Auto-detect build tools (webpack, vite, cargo, maven)
âœ… Auto-detect test frameworks (Jest, Mocha, pytest)
âœ… Auto-detect linters (ESLint, Clippy, pylint)
âœ… Extract commands from package.json scripts
âœ… Performance target met (<150ms)
âœ… 90% test coverage (infrastructure standard)
âœ… Accurate {{BUILD_COMMAND}}, {{TEST_COMMAND}} substitution
âœ… Ready for ProjectConfigGenerator integration
"""

[tasks.SELF-008]
id = "SELF-008"
name = "Implement WorkflowDetector service"
phase = "phase-3-detection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement WorkflowDetector service: auto-detect development workflows (TDD from test files, Git Flow from .git/hooks, CI/CD from .github/workflows or .gitlab-ci.yml). Detect pre-commit hooks, test automation, deployment pipelines. Confidence scoring. <100ms performance."
estimated_lines = 450
estimated_time = "8-10 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "WorkflowDetector service with detect() method",
    "TDD detection (test files present, test commands)",
    "Git workflow detection (Git Flow, trunk-based, etc.)",
    "CI/CD detection (GitHub Actions, GitLab CI, CircleCI)",
    "Pre-commit hook detection (.git/hooks, husky)",
    "Unit tests (85% coverage)"
]

performance_target = "Detect workflows < 100ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/WorkflowDetector.ts",
    "vscode-lumina/test/services/WorkflowDetector.test.ts"
]

validation_criteria = [
    "Run: npm test -- WorkflowDetector.test.ts",
    "Output: All tests pass (85% coverage)",
    "Test: Detect TDD (test directory + test command present)",
    "Output: {tdd: true, testDirectory: 'test/'}",
    "Test: Detect GitHub Actions (.github/workflows/*.yml)",
    "Output: {cicd: 'github-actions', workflows: ['ci.yml', 'deploy.yml']}",
    "Test: Detect pre-commit hooks (husky in devDeps)",
    "Output: {preCommitHooks: true, hookTool: 'husky'}",
    "Test: Detect Git Flow (.git/config has gitflow)",
    "Output: {gitWorkflow: 'git-flow'}",
    "Test: Performance benchmark",
    "Output: Detection completes in <100ms"
]

why = """
Auto-detect workflows (TDD, Git Flow, CI/CD).
Customizes enforcement rules per project.

Without WorkflowDetector: Ã†therLight enforces generic rules.
With WorkflowDetector: Rules adapt to project workflow (TDD required if tests exist).
"""

context = """
WorkflowDetector analyzes project structure to identify development workflows.

Detection strategy:
1. **TDD Detection** (test directory + test commands)
   - Check for test/ or spec/ directory
   - Check package.json for "test" script
   - Check for test framework (Jest, Mocha, pytest)
   - TDD confidence: High if tests + coverage tool

2. **Git Workflow Detection** (.git/config analysis)
   - Git Flow: .git/config contains "gitflow"
   - Trunk-based: Single main branch, no develop
   - Feature branches: Multiple feature/* branches

3. **CI/CD Detection** (config file presence)
   - GitHub Actions: .github/workflows/*.yml
   - GitLab CI: .gitlab-ci.yml
   - CircleCI: .circleci/config.yml
   - Jenkins: Jenkinsfile
   - Travis CI: .travis.yml

4. **Pre-commit Hooks** (husky, lefthook, pre-commit)
   - husky: package.json devDeps + .husky/ directory
   - lefthook: lefthook.yml present
   - pre-commit: .pre-commit-config.yaml (Python)

Related services:
- TechStackDetector (SELF-006) - Language/framework context
- ToolDetector (SELF-007) - Test framework context
- ProjectConfigGenerator (SELF-002) - Consumes workflow detection
"""

reasoning_chain = [
    "1. Create WorkflowDetector.ts service",
    "2. Implement detect() method:",
    "   - Input: workspace root path",
    "   - Output: WorkflowDetectionResults {tdd, gitWorkflow, cicd, preCommitHooks}",
    "3. Detect TDD workflow:",
    "   - Check for test/ or spec/ or __tests__ directory",
    "   - Check package.json scripts.test",
    "   - Check for test framework (from ToolDetector)",
    "   - Check for coverage tool (nyc, c8, coverage.py)",
    "   - Confidence: High if all present, Medium if 2/4, Low if 1/4",
    "4. Detect Git workflow:",
    "   - Read .git/config",
    "   - Check for 'gitflow' section â†’ Git Flow",
    "   - List branches: git branch --list",
    "   - Multiple feature/ branches â†’ Feature branch workflow",
    "   - Single main branch â†’ Trunk-based development",
    "5. Detect CI/CD:",
    "   - Check fs.existsSync('.github/workflows') â†’ GitHub Actions",
    "   - Check fs.existsSync('.gitlab-ci.yml') â†’ GitLab CI",
    "   - Check fs.existsSync('Jenkinsfile') â†’ Jenkins",
    "   - List workflow files and parse for details",
    "6. Detect pre-commit hooks:",
    "   - Check package.json devDeps for 'husky'",
    "   - Check fs.existsSync('.husky/') â†’ husky",
    "   - Check fs.existsSync('lefthook.yml') â†’ lefthook",
    "   - Check fs.existsSync('.pre-commit-config.yaml') â†’ pre-commit",
    "7. Write tests (TDD):",
    "   - Mock workspace with test/ dir â†’ detect TDD",
    "   - Mock .github/workflows/ â†’ detect GitHub Actions",
    "   - Mock husky in devDeps â†’ detect pre-commit hooks",
    "   - Performance test: detect in <100ms",
    "8. Integration: Return WorkflowDetectionResults to ProjectConfigGenerator",
    "9. Coverage: â‰¥85%",
    "10. Ready for DomainDetector integration (SELF-009)"
]

success_impact = """
After SELF-008 complete:
âœ… Auto-detect TDD workflow (test directory + test commands)
âœ… Auto-detect Git workflow (Git Flow, trunk-based, feature branches)
âœ… Auto-detect CI/CD (GitHub Actions, GitLab CI, Jenkins)
âœ… Auto-detect pre-commit hooks (husky, lefthook, pre-commit)
âœ… Performance target met (<100ms)
âœ… 85% test coverage (API standard)
âœ… Ã†therLight adapts to project workflow
âœ… Ready for ProjectConfigGenerator integration
"""

[tasks.SELF-009]
id = "SELF-009"
name = "Implement DomainDetector service (software-dev only)"
phase = "phase-3-detection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement DomainDetector service: infer project domain from keywords and structure. Phase 1 supports software-dev domain only with sub-types (CLI, web, desktop, library). Keyword analysis (package.json keywords, README). Structure analysis (file organization). Extensible for future domains. <100ms performance."
estimated_lines = 400
estimated_time = "6-8 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "DomainDetector service with detect() method",
    "Software-dev domain detection (CLI, web, desktop, library)",
    "Keyword analysis (package.json, README.md)",
    "Structure analysis (file organization patterns)",
    "Extensibility for future domains (healthcare, legal, SEO)",
    "Unit tests (85% coverage)"
]

performance_target = "Detect domain < 100ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/DomainDetector.ts",
    "vscode-lumina/test/services/DomainDetector.test.ts"
]

validation_criteria = [
    "Run: npm test -- DomainDetector.test.ts",
    "Output: All tests pass (85% coverage)",
    "Test: Detect CLI domain (bin field in package.json)",
    "Output: {domain: 'software-dev', subType: 'cli'}",
    "Test: Detect web app (react in dependencies)",
    "Output: {domain: 'software-dev', subType: 'web'}",
    "Test: Detect desktop app (tauri in dependencies)",
    "Output: {domain: 'software-dev', subType: 'desktop'}",
    "Test: Detect library (no dependencies, only devDependencies)",
    "Output: {domain: 'software-dev', subType: 'library'}",
    "Test: Performance benchmark",
    "Output: Detection completes in <100ms"
]

why = """
Infer project domain from keywords and structure.
Initially: software-dev domain only (CLI, web, desktop, library).
Future: Community adds healthcare, legal, SEO domains.

Without DomainDetector: Templates are generic, not domain-specific.
With DomainDetector: Templates adapt to CLI vs web vs desktop patterns.
"""

context = """
DomainDetector infers project domain and sub-type from keywords and file structure.

Detection strategy (Phase 1 - software-dev only):
1. **CLI Domain** (command-line tools)
   - package.json has "bin" field
   - Keywords: cli, command-line, tool
   - No web dependencies

2. **Web Domain** (web applications)
   - Dependencies: react, vue, angular, express, next
   - public/ or static/ directory
   - Keywords: web, webapp, frontend, backend

3. **Desktop Domain** (desktop applications)
   - Dependencies: tauri, electron, nwjs
   - src-tauri/ directory
   - Keywords: desktop, app, gui

4. **Library Domain** (reusable packages)
   - No dependencies (or minimal)
   - Only devDependencies
   - Keywords: library, module, package, sdk

Future domains (extensible):
- Healthcare: HIPAA compliance, patient data keywords
- Legal: eDiscovery, compliance keywords
- SEO: content optimization, analytics keywords

Related services:
- TechStackDetector (SELF-006) - Framework context
- ProjectConfigGenerator (SELF-002) - Consumes domain detection
"""

reasoning_chain = [
    "1. Create DomainDetector.ts service",
    "2. Implement detect() method:",
    "   - Input: workspace root path",
    "   - Output: DomainDetectionResults {domain, subType, confidence}",
    "3. Analyze package.json:",
    "   - Check 'bin' field â†’ CLI domain",
    "   - Check dependencies for web frameworks â†’ Web domain",
    "   - Check dependencies for desktop frameworks â†’ Desktop domain",
    "   - No dependencies + only devDeps â†’ Library domain",
    "4. Analyze keywords:",
    "   - Read package.json keywords array",
    "   - Read README.md for domain keywords",
    "   - Match keywords: cli â†’ CLI, web â†’ Web, desktop â†’ Desktop",
    "5. Analyze file structure:",
    "   - Check for public/ or static/ â†’ Web",
    "   - Check for src-tauri/ â†’ Desktop",
    "   - Check for bin/ or cli/ â†’ CLI",
    "6. Calculate confidence:",
    "   - Multiple indicators (bin + keywords 'cli') â†’ High (0.9)",
    "   - Single strong indicator (react dependency) â†’ Medium (0.7)",
    "   - Weak indicators (structure only) â†’ Low (0.5)",
    "7. Design extensibility:",
    "   - DomainRegistry interface for future domains",
    "   - Domains loaded from .aetherlight/domains/*.json",
    "   - Community can add healthcare, legal, SEO domains",
    "8. Write tests (TDD):",
    "   - Mock package.json with 'bin' â†’ detect CLI",
    "   - Mock dependencies with 'react' â†’ detect Web",
    "   - Mock dependencies with 'tauri' â†’ detect Desktop",
    "   - Mock no dependencies â†’ detect Library",
    "   - Performance test: detect in <100ms",
    "9. Coverage: â‰¥85%",
    "10. Ready for Phase 3 integration tests (SELF-010)"
]

success_impact = """
After SELF-009 complete:
âœ… Domain detection for software-dev projects
âœ… Sub-types: CLI, web, desktop, library
âœ… Keyword analysis from package.json and README
âœ… Structure analysis for domain inference
âœ… Extensible design for future domains
âœ… Performance target met (<100ms)
âœ… 85% test coverage (API standard)
âœ… Templates adapt to domain-specific patterns
"""

[tasks.SELF-010]
id = "SELF-010"
name = "Phase 3 integration tests"
phase = "phase-3-detection"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create comprehensive integration tests for Phase 3 Detection: TechStackDetector + ToolDetector + WorkflowDetector + DomainDetector. Test on real project examples (Node.js, Rust, Python). Full detection pipeline validation. 90% coverage."
estimated_lines = 600
estimated_time = "4-6 hours"
dependencies = ["SELF-006", "SELF-007", "SELF-008", "SELF-009"]
agent = "infrastructure-agent"

deliverables = [
    "Integration test suite (phase3.test.ts)",
    "Real project tests (Node.js, Rust, Python examples)",
    "Full detection pipeline tests (all 4 detectors)",
    "Error case tests (missing files, invalid structure)",
    "Performance tests (full detection < 500ms)",
    "Coverage report (â‰¥90%)"
]

performance_target = "Full detection pipeline < 500ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/integration/phase3.test.ts"
]

validation_criteria = [
    "Run: npm test -- phase3.test.ts",
    "Output: All integration tests pass",
    "Test: Detect Node.js + TypeScript + Jest + GitHub Actions",
    "Output: Complete detection results with all fields",
    "Test: Detect Rust + Cargo + cargo test",
    "Output: Complete detection results for Rust project",
    "Test: Detect Python + pip + pytest",
    "Output: Complete detection results for Python project",
    "Test: Detection pipeline on real project example",
    "Output: All detectors work together, results merged correctly",
    "Check: Coverage report",
    "Output: â‰¥90% coverage for Phase 3 services"
]

why = """
Ensure all detection services work together.
Test on real project examples.

Without integration tests: Detectors work alone but fail when combined.
With integration tests: Full pipeline validated, real-world accuracy confirmed.
"""

context = """
Phase 3 integration tests validate the complete detection pipeline.

Test scenarios:
1. **Node.js + TypeScript Project**
   - TechStackDetector: language='typescript', packageManager='npm'
   - ToolDetector: buildTool='webpack', testFramework='jest'
   - WorkflowDetector: tdd=true, cicd='github-actions'
   - DomainDetector: domain='software-dev', subType='web'

2. **Rust Project**
   - TechStackDetector: language='rust', packageManager='cargo'
   - ToolDetector: buildTool='cargo', testFramework='cargo-test'
   - WorkflowDetector: tdd=true, cicd='github-actions'
   - DomainDetector: domain='software-dev', subType='cli'

3. **Python Project**
   - TechStackDetector: language='python', packageManager='pip'
   - ToolDetector: buildTool='setuptools', testFramework='pytest'
   - WorkflowDetector: tdd=true, cicd='gitlab-ci'
   - DomainDetector: domain='software-dev', subType='library'

4. **Error Cases**
   - Missing package.json â†’ graceful fallback
   - Invalid Cargo.toml â†’ error handling
   - No test directory â†’ tdd=false

5. **Performance**
   - Run full pipeline 100 times
   - Average time < 500ms

Related services:
- TechStackDetector (SELF-006) - Language/framework
- ToolDetector (SELF-007) - Build/test tools
- WorkflowDetector (SELF-008) - TDD/CI/CD
- DomainDetector (SELF-009) - Project domain
"""

reasoning_chain = [
    "1. Create test/integration/phase3.test.ts",
    "2. Setup test fixtures:",
    "   - Mock Node.js project (package.json, tsconfig.json, test/, .github/workflows/)",
    "   - Mock Rust project (Cargo.toml, src/, tests/)",
    "   - Mock Python project (setup.py, requirements.txt, tests/)",
    "3. Test 1: Node.js detection pipeline",
    "   - Run TechStackDetector â†’ {language: 'typescript', packageManager: 'npm'}",
    "   - Run ToolDetector â†’ {buildTool: 'webpack', testFramework: 'jest'}",
    "   - Run WorkflowDetector â†’ {tdd: true, cicd: 'github-actions'}",
    "   - Run DomainDetector â†’ {domain: 'software-dev', subType: 'web'}",
    "   - Assert: All results correct and complete",
    "4. Test 2: Rust detection pipeline",
    "   - Similar to Test 1 but for Rust project",
    "5. Test 3: Python detection pipeline",
    "   - Similar to Test 1 but for Python project",
    "6. Test 4: Error cases",
    "   - Missing package.json â†’ TechStackDetector returns {confidence: 0.0}",
    "   - Invalid Cargo.toml â†’ ToolDetector handles error gracefully",
    "   - No test directory â†’ WorkflowDetector returns {tdd: false}",
    "7. Test 5: Performance",
    "   - Run full pipeline 100 times",
    "   - Calculate average time",
    "   - Assert: average < 500ms",
    "8. Test 6: Real project example",
    "   - Point at actual project (e.g., Ã†therLight itself)",
    "   - Run full pipeline",
    "   - Manually verify results accuracy",
    "9. Generate coverage report",
    "10. Assert: â‰¥90% coverage for all Phase 3 services"
]

success_impact = """
After SELF-010 complete:
âœ… Phase 3 integration tests passing
âœ… Full detection pipeline validated (4 detectors)
âœ… Real project tests (Node.js, Rust, Python)
âœ… Error cases handled gracefully
âœ… Performance targets met (<500ms full pipeline)
âœ… 90% coverage for Phase 3 services
âœ… Confidence HIGH for Phase 3 completion
âœ… Ready to proceed to Phase 4 (Interview & Config)
"""

# =============================================================================
# PHASE 4: INTERVIEW & CONFIG GENERATION - HIGH PRIORITY
# =============================================================================

[tasks.SELF-011]
id = "SELF-011"
name = "Implement CLI interview flow"
phase = "phase-4-interview"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement CLI interview flow command: interactive prompts for project initialization (4-5 questions). Uses inquirer.js (SELF-004) and detection results (Phase 3). Conditional questions based on detection. Answer validation. project-initialization.json template. 85% coverage."
estimated_lines = 400
estimated_time = "8-10 hours"
dependencies = ["SELF-004", "SELF-006", "SELF-007", "SELF-008", "SELF-009"]
agent = "api-agent"

deliverables = [
    "CLI interview flow command (interviewFlow.ts)",
    "project-initialization.json interview template",
    "Conditional question logic (skip if detected)",
    "Answer validation (schema-based)",
    "Integration with detection services",
    "Unit tests (85% coverage)"
]

performance_target = "Interview flow responsive < 100ms per prompt"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/commands/interviewFlow.ts",
    "vscode-lumina/src/templates/interview-flows/project-initialization.json",
    "vscode-lumina/test/commands/interviewFlow.test.ts"
]

validation_criteria = [
    "Run: npm test -- interviewFlow.test.ts",
    "Output: All tests pass (85% coverage)",
    "Test: Run interview with full detection (TypeScript + Jest)",
    "Output: Only 2 questions asked (build command, deploy target)",
    "Test: Run interview with partial detection (no test framework)",
    "Output: 4 questions asked (test framework + build + deploy + linter)",
    "Test: Answer validation (invalid BUILD_COMMAND)",
    "Output: Validation error, re-prompt user",
    "Test: Conditional questions (skip if detected)",
    "Output: Language question skipped (already detected TypeScript)",
    "Manual: Run command 'lumina interview init'",
    "Output: Interactive prompts work in terminal"
]

why = """
Interactive CLI asks 4-5 questions to clarify project setup.
Uses inquirer.js from Phase 2.

Without interview: Detection alone is insufficient (80% accuracy).
With interview: Detection + user input = 100% accuracy.
"""

context = """
CLI interview flow bridges detection and configuration.

Interview strategy:
1. **Run Detection** (Phase 3)
   - TechStackDetector, ToolDetector, WorkflowDetector, DomainDetector
   - Results provide defaults for interview questions

2. **Conditional Questions** (skip if detected)
   - Q1: Language (skip if detected with high confidence)
   - Q2: Package manager (skip if detected)
   - Q3: Test framework (ask if not detected)
   - Q4: Build command (ask if not detected)
   - Q5: Deploy target (always ask - can't detect)

3. **Answer Validation**
   - Validate against project-config-v1.json schema (SELF-003)
   - Re-prompt if invalid

4. **Output**
   - Return InterviewAnswers object to ProjectConfigGenerator (SELF-002)

Example interview template (project-initialization.json):
```json
{
  "questions": [
    {
      "name": "language",
      "type": "list",
      "message": "Which language?",
      "choices": ["typescript", "javascript", "rust", "python"],
      "when": "detection.language.confidence < 0.8"
    },
    {
      "name": "BUILD_COMMAND",
      "type": "input",
      "message": "Build command?",
      "default": "npm run build",
      "when": "!detection.toolDetection.BUILD_COMMAND"
    }
  ]
}
```

Related services:
- InterviewEngine (SELF-004) - inquirer.js integration
- Detection services (SELF-006 to SELF-009) - Provide defaults
- ProjectConfigGenerator (SELF-002) - Consumes interview answers
"""

reasoning_chain = [
    "1. Create interviewFlow.ts command",
    "2. Implement runInterview() method:",
    "   - Input: workspace root path",
    "   - Output: InterviewAnswers object",
    "3. Step 1: Run detection pipeline",
    "   - Call TechStackDetector, ToolDetector, WorkflowDetector, DomainDetector",
    "   - Store results for conditional questions",
    "4. Step 2: Load interview template",
    "   - Read project-initialization.json",
    "   - Parse questions array",
    "5. Step 3: Filter questions (conditional logic)",
    "   - Skip language question if detected with confidence > 0.8",
    "   - Skip package manager if detected",
    "   - Skip test framework if detected",
    "   - Always ask deploy target (can't detect)",
    "6. Step 4: Run interview",
    "   - Call InterviewEngine.run(filteredQuestions)",
    "   - User answers each question",
    "   - Validate answers against schema",
    "7. Step 5: Merge detection + interview",
    "   - Detection provides defaults",
    "   - Interview overrides detection",
    "   - Example: Detection says 'npm', interview chooses 'yarn' â†’ yarn wins",
    "8. Step 6: Return InterviewAnswers",
    "   - Pass to ProjectConfigGenerator.generate(detection, interview)",
    "9. Write tests (TDD):",
    "   - Mock detection (TypeScript + Jest) â†’ only 2 questions asked",
    "   - Mock partial detection (no test framework) â†’ 4 questions asked",
    "   - Mock invalid answer â†’ validation error, re-prompt",
    "   - Conditional logic test â†’ language question skipped",
    "10. Coverage: â‰¥85%, ready for SelfConfigWizard (SELF-012)"
]

success_impact = """
After SELF-011 complete:
âœ… CLI interview flow working (interactive prompts)
âœ… Conditional questions (skip if detected)
âœ… Answer validation (schema-based)
âœ… Detection + interview integration
âœ… project-initialization.json template
âœ… 85% test coverage (API standard)
âœ… Performance responsive (<100ms per prompt)
âœ… Ready for SelfConfigWizard integration (SELF-012)
"""

[tasks.SELF-012]
id = "SELF-012"
name = "Integrate detection + interview â†’ config generation"
phase = "phase-4-interview"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create init command orchestrating full self-configuration flow: Run Phase 3 detection â†’ Run Phase 4 interview â†’ Merge results â†’ Generate project-config.json (SELF-002) â†’ Write to .aetherlight/project-config.json. End-to-end integration with error handling. <5s total time."
estimated_lines = 550
estimated_time = "10-12 hours"
dependencies = ["SELF-002", "SELF-010", "SELF-011"]
agent = "infrastructure-agent"

deliverables = [
    "Init command (init.ts) - Full self-configuration flow",
    "Detection pipeline orchestration (Phase 3 services)",
    "Interview flow orchestration (Phase 4 interview)",
    "Merge logic (detection + interview â†’ config)",
    "Config generation (ProjectConfigGenerator.generate())",
    "Config persistence (.aetherlight/project-config.json)",
    "Error handling (graceful fallbacks)",
    "Integration tests (configGeneration.test.ts)"
]

performance_target = "Full flow < 5 seconds"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/commands/init.ts",
    "vscode-lumina/test/integration/configGeneration.test.ts"
]

validation_criteria = [
    "Run: npm test -- configGeneration.test.ts",
    "Output: All integration tests pass",
    "Test: Full flow (detection + interview + config)",
    "Output: project-config.json generated in .aetherlight/",
    "Test: Interview overrides detection",
    "Output: Interview answer takes precedence over detection",
    "Test: Error handling (missing .aetherlight/ directory)",
    "Output: Directory created automatically",
    "Test: Performance benchmark",
    "Output: Full flow completes in <5 seconds",
    "Manual: Run 'lumina init' in test project",
    "Output: project-config.json generated successfully"
]

why = """
Combine Phase 3 detection results + Phase 4 interview answers.
Generate final project-config.json.

Without integration: Individual services work but no orchestration.
With integration: One command sets up entire self-configuration.
"""

context = """
Init command orchestrates the complete self-configuration pipeline.

Flow:
1. **Phase 3 Detection** (<500ms)
   - TechStackDetector â†’ language, framework, packageManager
   - ToolDetector â†’ buildTool, testFramework, linter
   - WorkflowDetector â†’ tdd, gitWorkflow, cicd
   - DomainDetector â†’ domain, subType

2. **Phase 4 Interview** (<4s user interaction)
   - Load project-initialization.json
   - Filter questions (skip if detected)
   - Run InterviewEngine
   - Collect answers

3. **Merge Detection + Interview**
   - Interview overrides detection (user preference wins)
   - Detection fills gaps (user doesn't need to answer everything)
   - Example: Detection says npm, interview silent â†’ use npm
   - Example: Detection says npm, interview says yarn â†’ use yarn

4. **Generate Config** (<100ms)
   - Call ProjectConfigGenerator.generate(detection, interview)
   - Validate against schema (SELF-003)
   - Apply defaults for missing fields

5. **Persist Config** (<10ms)
   - Create .aetherlight/ directory if missing
   - Write project-config.json
   - Pretty-print JSON (2-space indent)

6. **Success Message**
   - Show summary: "Detected TypeScript + Jest, configured Ã†therLight for your project"

Related services:
- Phase 3 detectors (SELF-006 to SELF-009)
- Interview flow (SELF-011)
- ProjectConfigGenerator (SELF-002)
- VariableResolver (SELF-001) - Uses this config
"""

reasoning_chain = [
    "1. Create init.ts command",
    "2. Implement runInitFlow() method:",
    "   - Input: workspace root path",
    "   - Output: Success/failure message",
    "3. Step 1: Run Phase 3 detection",
    "   - Call TechStackDetector.detect()",
    "   - Call ToolDetector.detect()",
    "   - Call WorkflowDetector.detect()",
    "   - Call DomainDetector.detect()",
    "   - Merge all DetectionResults",
    "4. Step 2: Run Phase 4 interview",
    "   - Call interviewFlow.runInterview(detectionResults)",
    "   - Get InterviewAnswers",
    "5. Step 3: Merge detection + interview",
    "   - For each variable:",
    "     - If interview has value â†’ use interview",
    "     - Else if detection has value â†’ use detection",
    "     - Else â†’ use default from schema",
    "6. Step 4: Generate config",
    "   - Call ProjectConfigGenerator.generate(detection, interview)",
    "   - Returns ProjectConfig object",
    "   - Validate against schema (already done in generate())",
    "7. Step 5: Persist config",
    "   - Check if .aetherlight/ exists, create if not",
    "   - Write JSON.stringify(config, null, 2) to .aetherlight/project-config.json",
    "8. Step 6: Show success message",
    "   - Log: 'Ã†therLight configured for {language} + {testFramework}'",
    "   - Log: 'Config saved to .aetherlight/project-config.json'",
    "9. Error handling:",
    "   - Detection fails â†’ Show warning, continue with interview",
    "   - Interview fails â†’ Use detection only + defaults",
    "   - Config generation fails â†’ Show error, abort",
    "10. Write tests (TDD):",
    "    - Mock full flow â†’ config generated",
    "    - Mock interview override â†’ interview wins",
    "    - Mock error cases â†’ graceful fallbacks",
    "    - Performance test: <5s total time",
    "11. Coverage: â‰¥90%, ready for TemplateCustomizer (SELF-013)"
]

success_impact = """
After SELF-012 complete:
âœ… Init command working (one-command setup)
âœ… Detection + interview integration
âœ… Config generation (project-config.json)
âœ… Error handling (graceful fallbacks)
âœ… Performance target met (<5s full flow)
âœ… 90% test coverage (infrastructure standard)
âœ… Users can self-configure Ã†therLight for ANY project
âœ… Ready for template customization (SELF-013)
"""

[tasks.SELF-013]
id = "SELF-013"
name = "Implement template customization system"
phase = "phase-4-interview"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement TemplateCustomizer service: load generic templates from .aetherlight/templates/, substitute {{VARIABLES}} using VariableResolver (SELF-001), write customized files to workspace. Applies to CLAUDE.md, skills, agent contexts. Batch processing for multiple templates. 90% coverage."
estimated_lines = 500
estimated_time = "8-10 hours"
dependencies = ["SELF-001"]
agent = "infrastructure-agent"

deliverables = [
    "TemplateCustomizer service with customize() method",
    "Template loading from .aetherlight/templates/",
    "Variable substitution (using VariableResolver)",
    "Customized file writing to workspace",
    "Batch processing (customize multiple templates)",
    "Error handling (missing variables, invalid templates)",
    "Unit tests (90% coverage)"
]

performance_target = "Customize template < 50ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/TemplateCustomizer.ts",
    "vscode-lumina/test/services/TemplateCustomizer.test.ts"
]

validation_criteria = [
    "Run: npm test -- TemplateCustomizer.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Customize single template (CLAUDE.md)",
    "Output: {{BUILD_COMMAND}} replaced with 'npm run build'",
    "Test: Customize multiple templates (batch)",
    "Output: All templates customized successfully",
    "Test: Missing variable in template",
    "Output: MissingVariableError thrown with variable name",
    "Test: Invalid template (malformed {{VAR)",
    "Output: TemplateError thrown with clear message",
    "Test: Performance benchmark",
    "Output: Customization completes in <50ms per template"
]

why = """
Load generic templates, substitute {{VARIABLES}}, write customized files.
Applies to CLAUDE.md, skills, agent contexts.

Without TemplateCustomizer: Templates are generic, not project-specific.
With TemplateCustomizer: Templates adapt to project ({{BUILD_COMMAND}} becomes 'cargo build' for Rust).
"""

context = """
TemplateCustomizer applies self-configuration to template files.

Workflow:
1. **Load Template**
   - Read template from .aetherlight/templates/CLAUDE.md.template
   - Parse for {{VARIABLES}}

2. **Resolve Variables**
   - Call VariableResolver.resolve(template, projectConfig)
   - Replace all {{VAR}} with actual values

3. **Write Customized File**
   - Write resolved content to .claude/CLAUDE.md
   - Preserve file permissions and metadata

Templates to customize:
- .claude/CLAUDE.md - Project instructions
- .claude/skills/*.md - Project-specific skills
- internal/agents/*-context.md - Agent contexts
- .github/workflows/*.yml - CI/CD workflows
- Pre-commit hooks - Test/lint commands

Example template (CLAUDE.md.template):
```markdown
# Project Instructions

**Build Command:** {{BUILD_COMMAND}}
**Test Command:** {{TEST_COMMAND}}
**Language:** {{LANGUAGE}}
```

After customization (CLAUDE.md):
```markdown
# Project Instructions

**Build Command:** npm run build
**Test Command:** npm test
**Language:** typescript
```

Related services:
- VariableResolver (SELF-001) - Variable resolution logic
- ProjectConfigGenerator (SELF-002) - Provides config values
"""

reasoning_chain = [
    "1. Create TemplateCustomizer.ts service",
    "2. Implement customize() method:",
    "   - Input: templatePath, projectConfig",
    "   - Output: customizedContent",
    "3. Step 1: Load template",
    "   - Read file from .aetherlight/templates/",
    "   - Handle missing file error",
    "4. Step 2: Resolve variables",
    "   - Call VariableResolver.resolve(templateContent, projectConfig)",
    "   - Get resolved content with {{VAR}} replaced",
    "   - Handle MissingVariableError",
    "5. Step 3: Write customized file",
    "   - Determine output path (.claude/CLAUDE.md)",
    "   - Create parent directories if needed",
    "   - Write resolved content",
    "6. Implement customizeAll() method:",
    "   - Input: projectConfig",
    "   - Output: number of files customized",
    "   - Scan .aetherlight/templates/ directory",
    "   - For each .template file:",
    "     - Call customize()",
    "     - Track success/failure",
    "   - Return summary",
    "7. Error handling:",
    "   - Missing template file â†’ Skip, log warning",
    "   - Missing variable â†’ MissingVariableError with helpful message",
    "   - Invalid template syntax â†’ TemplateError with line number",
    "8. Write tests (TDD):",
    "   - Mock template with {{VAR}} â†’ resolves correctly",
    "   - Mock multiple templates â†’ batch customization works",
    "   - Mock missing variable â†’ error with variable name",
    "   - Mock invalid syntax â†’ error with line number",
    "   - Performance test: customize in <50ms",
    "9. Integration: Used by init command (SELF-012)",
    "10. Coverage: â‰¥90%, ready for variable resolution flow (SELF-014)"
]

success_impact = """
After SELF-013 complete:
âœ… Template customization system working
âœ… Variable substitution using VariableResolver
âœ… Customized files written to workspace
âœ… Batch processing (multiple templates)
âœ… Error handling (missing variables, invalid templates)
âœ… Performance target met (<50ms per template)
âœ… 90% test coverage (infrastructure standard)
âœ… CLAUDE.md, skills, agents adapt to ANY project
"""

[tasks.SELF-014]
id = "SELF-014"
name = "Create variable resolution interview flow"
phase = "phase-4-interview"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement variable resolution interview: dynamically generate questions for ambiguous/missing variables detected by VariableResolver. User chooses correct value. Update project-config.json with resolved values. Re-run template customization. 85% coverage."
estimated_lines = 350
estimated_time = "6-8 hours"
dependencies = ["SELF-001", "SELF-004"]
agent = "api-agent"

deliverables = [
    "Variable resolution interview command",
    "Dynamic question generation (from missing variables)",
    "Answer validation (schema-based)",
    "Config update (merge resolved values)",
    "Re-customization trigger (run TemplateCustomizer)",
    "Unit tests (85% coverage)"
]

performance_target = "Interview responsive < 100ms per question"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/commands/variableResolutionFlow.ts",
    "vscode-lumina/src/templates/interview-flows/variable-resolution.json",
    "vscode-lumina/test/commands/variableResolution.test.ts"
]

validation_criteria = [
    "Run: npm test -- variableResolution.test.ts",
    "Output: All tests pass (85% coverage)",
    "Test: Detect missing variable {{DEPLOY_TARGET}}",
    "Output: Interview question generated dynamically",
    "Test: User provides value 'production'",
    "Output: Config updated with DEPLOY_TARGET: 'production'",
    "Test: Re-run template customization",
    "Output: Templates re-customized with new variable",
    "Test: Invalid answer (empty string)",
    "Output: Validation error, re-prompt",
    "Manual: Trigger resolution flow",
    "Output: Interactive prompts work for missing variables"
]

why = """
If ambiguous variables detected, ask user which to use.
Dynamic interview generation.

Without resolution flow: Missing variables block customization.
With resolution flow: User fills gaps on-demand, customization completes.
"""

context = """
Variable resolution interview handles missing/ambiguous variables.

Trigger: VariableResolver throws MissingVariableError
Workflow:
1. **Detect Missing Variables**
   - TemplateCustomizer calls VariableResolver.resolve()
   - VariableResolver throws MissingVariableError with variable list
   - Catch error, trigger resolution flow

2. **Generate Dynamic Questions**
   - For each missing variable:
     - Look up variable in variable-reference.md (SELF-003A)
     - Generate question based on variable type
     - Provide examples from docs

3. **Run Interview**
   - Call InterviewEngine with dynamically generated questions
   - User answers each question

4. **Update Config**
   - Merge answers into project-config.json
   - Re-validate schema
   - Save updated config

5. **Re-run Customization**
   - Call TemplateCustomizer with updated config
   - Templates now complete (all variables resolved)

Example:
- Template contains {{DEPLOY_TARGET}}
- VariableResolver throws MissingVariableError(['DEPLOY_TARGET'])
- Generate question: "What is your deploy target? (staging, production, dev)"
- User answers: "production"
- Update config: {DEPLOY_TARGET: 'production'}
- Re-run customization: {{DEPLOY_TARGET}} â†’ 'production'

Related services:
- VariableResolver (SELF-001) - Detects missing variables
- InterviewEngine (SELF-004) - Runs dynamic interview
- ProjectConfigGenerator (SELF-002) - Updates config
- TemplateCustomizer (SELF-013) - Re-runs with updated config
"""

reasoning_chain = [
    "1. Create variableResolutionFlow.ts command",
    "2. Implement resolveVariables() method:",
    "   - Input: missingVariables array, projectConfig",
    "   - Output: updated projectConfig",
    "3. Step 1: Generate dynamic questions",
    "   - For each missing variable:",
    "     - Look up in variable-reference.md",
    "     - Extract valid values, examples, description",
    "     - Generate inquirer question",
    "4. Step 2: Run interview",
    "   - Call InterviewEngine.run(dynamicQuestions)",
    "   - Get user answers",
    "5. Step 3: Validate answers",
    "   - Validate against project-config-v1.json schema",
    "   - Re-prompt if invalid",
    "6. Step 4: Update config",
    "   - Merge answers into projectConfig",
    "   - Write updated config to .aetherlight/project-config.json",
    "7. Step 5: Re-run customization",
    "   - Call TemplateCustomizer.customizeAll(updatedConfig)",
    "   - All templates now complete",
    "8. Integration with TemplateCustomizer:",
    "   - TemplateCustomizer catches MissingVariableError",
    "   - Calls variableResolutionFlow.resolveVariables()",
    "   - Re-runs customization automatically",
    "9. Write tests (TDD):",
    "   - Mock missing variable â†’ question generated",
    "   - Mock user answer â†’ config updated",
    "   - Mock re-customization â†’ templates complete",
    "   - Mock invalid answer â†’ validation error",
    "10. Coverage: â‰¥85%, ready for Phase 4 integration (SELF-015)"
]

success_impact = """
After SELF-014 complete:
âœ… Variable resolution interview working
âœ… Dynamic question generation (from missing variables)
âœ… Config update with resolved values
âœ… Re-customization trigger (templates complete)
âœ… Error handling (invalid answers, validation)
âœ… Performance responsive (<100ms per question)
âœ… 85% test coverage (API standard)
âœ… Missing variables no longer block customization
"""

[tasks.SELF-015]
id = "SELF-015"
name = "Phase 4 integration tests"
phase = "phase-4-interview"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create end-to-end integration tests for Phase 4: Init command (SELF-012) + Interview flow (SELF-011) + Template customization (SELF-013) + Variable resolution (SELF-014). Test on greenfield projects and existing projects. Full self-configuration pipeline validation. Performance benchmarks. 85% coverage."
estimated_lines = 700
estimated_time = "8-10 hours"
dependencies = ["SELF-011", "SELF-012", "SELF-013", "SELF-014"]
agent = "infrastructure-agent"

deliverables = [
    "E2E test suite (init.test.ts)",
    "Greenfield project tests (empty workspace â†’ full config)",
    "Existing project tests (partial config â†’ complete config)",
    "Error case tests (invalid answers, missing templates)",
    "Performance benchmarks (<5s full flow)",
    "Coverage report (â‰¥85%)"
]

performance_target = "E2E test suite < 30s"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/e2e/init.test.ts"
]

validation_criteria = [
    "Run: npm test -- init.test.ts",
    "Output: All E2E tests pass",
    "Test: Greenfield project (empty workspace)",
    "Output: Detection â†’ interview â†’ config â†’ templates customized",
    "Test: Existing project (partial config)",
    "Output: Upgrade flow works, preserves customizations",
    "Test: Missing variable resolution",
    "Output: Dynamic interview triggered, variable resolved",
    "Test: Error handling (invalid answer)",
    "Output: Graceful error, user re-prompted",
    "Test: Performance benchmark",
    "Output: Full flow completes in <5s",
    "Check: Coverage report",
    "Output: â‰¥85% coverage for Phase 4 services"
]

why = """
End-to-end test of full initialization flow.

Without E2E tests: Individual components work but integration fails.
With E2E tests: Full self-configuration pipeline validated end-to-end.
"""

context = """
Phase 4 E2E tests validate the complete self-configuration pipeline from start to finish.

Test scenarios:
1. **Greenfield Project** (empty workspace)
   - Create empty directory
   - Run 'lumina init'
   - Detection: No files â†’ all interview questions asked
   - Interview: User answers 5 questions
   - Config: project-config.json generated
   - Templates: CLAUDE.md, skills, agents customized
   - Validation: All files created, variables resolved

2. **Existing Project** (already has some config)
   - Workspace has package.json, tsconfig.json
   - Run 'lumina init'
   - Detection: TypeScript + npm detected
   - Interview: Only 2 questions asked (build, deploy)
   - Config: Merges detection + interview
   - Templates: Customized with detected values
   - Validation: Existing config preserved

3. **Missing Variable Resolution**
   - Template contains {{CUSTOM_VAR}}
   - Init flow runs
   - VariableResolver throws MissingVariableError
   - Dynamic interview triggered
   - User provides CUSTOM_VAR value
   - Templates re-customized with new variable

4. **Error Cases**
   - Invalid answer â†’ Re-prompt
   - Missing template â†’ Skip, log warning
   - Detection failure â†’ Continue with interview only

5. **Performance Benchmarks**
   - Full flow (detection + interview + config + templates) < 5s
   - Detection pipeline < 500ms
   - Config generation < 100ms
   - Template customization < 200ms

Related services:
- Init command (SELF-012) - Orchestration
- Interview flow (SELF-011) - User input
- Template customization (SELF-013) - File generation
- Variable resolution (SELF-014) - Gap filling
"""

reasoning_chain = [
    "1. Create test/e2e/init.test.ts",
    "2. Setup test fixtures:",
    "   - Mock greenfield workspace (empty directory)",
    "   - Mock existing workspace (package.json, tsconfig.json)",
    "   - Mock templates in .aetherlight/templates/",
    "3. Test 1: Greenfield project",
    "   - Start with empty workspace",
    "   - Run init command",
    "   - Mock interview answers (TypeScript, npm, Jest)",
    "   - Assert: project-config.json created",
    "   - Assert: Templates customized (CLAUDE.md exists)",
    "   - Assert: All variables resolved",
    "4. Test 2: Existing project",
    "   - Start with workspace containing package.json",
    "   - Run init command",
    "   - Detection finds TypeScript + npm",
    "   - Mock interview (only build command asked)",
    "   - Assert: Config merges detection + interview",
    "   - Assert: Existing files preserved",
    "5. Test 3: Missing variable resolution",
    "   - Template contains {{DEPLOY_TARGET}}",
    "   - Run init command",
    "   - MissingVariableError thrown",
    "   - Mock dynamic interview",
    "   - Assert: Variable resolved, templates complete",
    "6. Test 4: Error handling",
    "   - Mock invalid interview answer",
    "   - Assert: Validation error, re-prompt",
    "   - Mock detection failure",
    "   - Assert: Falls back to interview only",
    "7. Test 5: Performance",
    "   - Run full flow 10 times",
    "   - Calculate average time",
    "   - Assert: <5s average",
    "8. Generate coverage report",
    "9. Assert: â‰¥85% coverage for Phase 4 services",
    "10. All tests pass â†’ Phase 4 complete"
]

success_impact = """
After SELF-015 complete:
âœ… Phase 4 E2E tests passing
âœ… Full self-configuration pipeline validated
âœ… Greenfield and existing project tests
âœ… Missing variable resolution tested
âœ… Error cases handled gracefully
âœ… Performance targets met (<5s full flow)
âœ… 85% coverage for Phase 4 services
âœ… Confidence HIGH for Phase 4 completion
âœ… Ready to proceed to Phase 5 (Migration & Upgrade)
"""

# =============================================================================
# PHASE 5: MIGRATION & UPGRADE - MEDIUM PRIORITY
# =============================================================================

[tasks.SELF-016]
id = "SELF-016"
name = "Implement upgrade command"
phase = "phase-5-migration"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement upgrade command for existing Ã†therLight users: detect current version (SELF-017), preview changes, require user confirmation, run config migration (SELF-018), create backup (SELF-019), apply upgrade, verify success. Manual upgrade only (no auto-upgrade). 90% coverage."
estimated_lines = 450
estimated_time = "8-10 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "Upgrade command (upgrade.ts)",
    "Version detection (current vs latest)",
    "Upgrade preview (show changes before applying)",
    "User confirmation flow (inquirer prompt)",
    "Backup creation (SELF-019 integration)",
    "Config migration (SELF-018 integration)",
    "Rollback on failure",
    "Unit tests (90% coverage)"
]

performance_target = "Upgrade check < 200ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/commands/upgrade.ts",
    "vscode-lumina/test/commands/upgrade.test.ts"
]

validation_criteria = [
    "Run: npm test -- upgrade.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Detect version (v1.0 â†’ v2.0 available)",
    "Output: Upgrade available message",
    "Test: Preview changes",
    "Output: List of config changes, new features",
    "Test: User confirms upgrade",
    "Output: Backup created, migration runs, upgrade succeeds",
    "Test: User cancels upgrade",
    "Output: No changes made, exit gracefully",
    "Test: Upgrade fails",
    "Output: Rollback to backup, error message",
    "Manual: Run 'lumina upgrade'",
    "Output: Interactive upgrade flow works"
]

why = """
Existing Ã†therLight users need migration path.
Manual upgrade required (user confirmation).

Without upgrade command: Users stuck on old version, can't benefit from self-config.
With upgrade command: Seamless migration, preserves customizations, safe rollback.
"""

context = """
Upgrade command migrates existing Ã†therLight installations to self-configuration system.

Workflow:
1. **Version Detection**
   - Check .aetherlight/version.json (SELF-017)
   - Compare with latest version
   - Determine upgrade path (v1.0 â†’ v2.0)

2. **Upgrade Preview**
   - Show current version vs new version
   - List config changes (old format â†’ new format)
   - List new features (detection, interview, templates)
   - Show files that will be modified

3. **User Confirmation**
   - Prompt: "Upgrade to v2.0? This will modify your config. [Y/n]"
   - If NO â†’ Exit, no changes
   - If YES â†’ Continue

4. **Backup Creation** (SELF-019)
   - Create .aetherlight/backups/pre-upgrade-v2.0/
   - Copy current config files
   - Backup successful â†’ Continue

5. **Config Migration** (SELF-018)
   - Load old config format
   - Convert to new format (project-config.json)
   - Preserve user customizations
   - Validate new config

6. **Apply Upgrade**
   - Write new config
   - Update version.json
   - Re-customize templates with new config

7. **Verify Success**
   - Run validation checks
   - If success â†’ Show success message
   - If failure â†’ Rollback to backup

Related services:
- VersionTracker (SELF-017) - Version detection
- ConfigMigration (SELF-018) - Config conversion
- BackupRollback (SELF-019) - Safety net
"""

reasoning_chain = [
    "1. Create upgrade.ts command",
    "2. Implement checkForUpgrade() method:",
    "   - Call VersionTracker.getCurrentVersion()",
    "   - Call VersionTracker.getLatestVersion()",
    "   - Compare versions",
    "   - Return upgrade available boolean",
    "3. Implement previewUpgrade() method:",
    "   - Load current config",
    "   - Generate preview of changes",
    "   - List new features",
    "   - Show files to be modified",
    "4. Implement confirmUpgrade() method:",
    "   - Use inquirer.confirm()",
    "   - Prompt user: 'Upgrade to v{version}? [Y/n]'",
    "   - Return user confirmation",
    "5. Implement runUpgrade() method:",
    "   - Step 1: Create backup (BackupRollback.backup())",
    "   - Step 2: Run migration (ConfigMigration.migrate())",
    "   - Step 3: Update version (VersionTracker.setVersion())",
    "   - Step 4: Re-customize templates",
    "   - Step 5: Verify success",
    "6. Implement rollback() method:",
    "   - If any step fails, call BackupRollback.rollback()",
    "   - Restore backup",
    "   - Show error message",
    "7. Write tests (TDD):",
    "   - Mock version check â†’ upgrade available",
    "   - Mock user confirms â†’ upgrade runs",
    "   - Mock user cancels â†’ no changes",
    "   - Mock upgrade fails â†’ rollback works",
    "8. Coverage: â‰¥90%",
    "9. Integration with VersionTracker, ConfigMigration, BackupRollback",
    "10. Ready for VersionTracker implementation (SELF-017)"
]

success_impact = """
After SELF-016 complete:
âœ… Upgrade command working (manual upgrade)
âœ… Version detection (current vs latest)
âœ… Upgrade preview (show changes before applying)
âœ… User confirmation flow (safe, manual)
âœ… Backup and rollback (data protection)
âœ… Config migration integration
âœ… 90% test coverage (infrastructure standard)
âœ… Existing users can migrate to self-config safely
"""

[tasks.SELF-017]
id = "SELF-017"
name = "Implement version tracking system"
phase = "phase-5-migration"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement VersionTracker service: track Ã†therLight version in user projects via .aetherlight/version.json. Detect current version, get latest version, compare versions. Enable upgrade detection and migration paths. Semantic versioning support. 90% coverage."
estimated_lines = 300
estimated_time = "4-6 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "VersionTracker service with version management",
    "version.json format (.aetherlight/version.json)",
    "getCurrentVersion() method",
    "getLatestVersion() method (fetch from registry)",
    "compareVersions() method (semver)",
    "setVersion() method (update version.json)",
    "Unit tests (90% coverage)"
]

performance_target = "Version check < 50ms (local), < 500ms (remote)"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/VersionTracker.ts",
    "vscode-lumina/test/services/VersionTracker.test.ts"
]

validation_criteria = [
    "Run: npm test -- VersionTracker.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Get current version from version.json",
    "Output: {version: '1.0.0', installed: '2025-01-15'}",
    "Test: Get latest version from npm registry",
    "Output: {version: '2.0.0', publishedAt: '2025-02-01'}",
    "Test: Compare versions (1.0.0 vs 2.0.0)",
    "Output: {updateAvailable: true, changeType: 'major'}",
    "Test: Set version (update version.json)",
    "Output: version.json updated with new version",
    "Test: Performance (local check)",
    "Output: <50ms"
]

why = """
Track Ã†therLight version in user project.
Enables upgrade detection and migration.

Without version tracking: Can't detect if upgrade needed, can't determine migration path.
With version tracking: Automatic upgrade detection, smart migration based on version diff.
"""

context = """
VersionTracker manages Ã†therLight version in user projects.

version.json format (.aetherlight/version.json):
```json
{
  "version": "1.0.0",
  "installedAt": "2025-01-15T10:30:00Z",
  "configVersion": "1",
  "schemaVersion": "v1"
}
```

Methods:
1. **getCurrentVersion()**: Read .aetherlight/version.json
2. **getLatestVersion()**: Fetch from npm registry or GitHub releases
3. **compareVersions()**: Use semver to compare (major/minor/patch)
4. **setVersion()**: Write new version to version.json
5. **needsUpgrade()**: currentVersion < latestVersion

Version comparison:
- 1.0.0 â†’ 1.0.1: Patch (bug fixes)
- 1.0.0 â†’ 1.1.0: Minor (new features, backward compatible)
- 1.0.0 â†’ 2.0.0: Major (breaking changes, migration required)

Related services:
- Upgrade command (SELF-016) - Uses version comparison
- ConfigMigration (SELF-018) - Uses version to determine migration path
"""

reasoning_chain = [
    "1. Create VersionTracker.ts service",
    "2. Implement getCurrentVersion() method:",
    "   - Read .aetherlight/version.json",
    "   - Parse JSON",
    "   - Return {version, installedAt, configVersion}",
    "   - If file missing, return null",
    "3. Implement getLatestVersion() method:",
    "   - Fetch from npm registry: npm view aetherlight version",
    "   - Or: Fetch from GitHub releases API",
    "   - Parse version",
    "   - Return {version, publishedAt}",
    "4. Implement compareVersions() method:",
    "   - Use semver library: semver.compare(v1, v2)",
    "   - Determine change type: major/minor/patch",
    "   - Return {updateAvailable, changeType, currentVersion, latestVersion}",
    "5. Implement setVersion() method:",
    "   - Create version.json object",
    "   - Write to .aetherlight/version.json",
    "   - Include timestamp, config version, schema version",
    "6. Implement needsUpgrade() method:",
    "   - Call getCurrentVersion() and getLatestVersion()",
    "   - Call compareVersions()",
    "   - Return boolean",
    "7. Write tests (TDD):",
    "   - Mock version.json â†’ getCurrentVersion() returns version",
    "   - Mock npm registry â†’ getLatestVersion() returns latest",
    "   - Mock version comparison â†’ compareVersions() accurate",
    "   - Mock setVersion() â†’ version.json updated",
    "8. Performance optimization:",
    "   - Cache latest version (5-minute TTL)",
    "   - Local check <50ms",
    "   - Remote check <500ms",
    "9. Coverage: â‰¥90%",
    "10. Ready for ConfigMigration integration (SELF-018)"
]

success_impact = """
After SELF-017 complete:
âœ… Version tracking system working
âœ… version.json format defined
âœ… Current version detection (local)
âœ… Latest version detection (npm registry)
âœ… Version comparison (semver)
âœ… Set version method (update tracking)
âœ… Performance targets met (<50ms local, <500ms remote)
âœ… 90% test coverage (infrastructure standard)
âœ… Foundation for upgrade detection and migration
"""

[tasks.SELF-018]
id = "SELF-018"
name = "Implement config migration service"
phase = "phase-5-migration"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement ConfigMigration service: migrate old Ã†therLight config formats to new self-config format (project-config.json). Preserve user customizations. Detect config version, apply version-specific migrations. Generate migration log. Validate migrated config against schema (SELF-003). 90% coverage."
estimated_lines = 600
estimated_time = "10-12 hours"
dependencies = ["SELF-002", "SELF-003"]
agent = "infrastructure-agent"

deliverables = [
    "ConfigMigration service with migrate() method",
    "Old format detection (v0, v1 config)",
    "New format conversion (project-config.json)",
    "User customization preservation",
    "Version-specific migrations (v0â†’v1, v1â†’v2)",
    "Migration log (.aetherlight/migration.log)",
    "Schema validation (post-migration)",
    "Unit tests (90% coverage)"
]

performance_target = "Migration < 500ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/ConfigMigration.ts",
    "vscode-lumina/test/services/ConfigMigration.test.ts"
]

validation_criteria = [
    "Run: npm test -- ConfigMigration.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Detect old config format (v1)",
    "Output: {version: 'v1', path: '.claude/config.json'}",
    "Test: Migrate v1 â†’ v2",
    "Output: project-config.json created, customizations preserved",
    "Test: Preserve user customizations",
    "Output: Custom BUILD_COMMAND preserved in new config",
    "Test: Validate migrated config",
    "Output: Schema validation passes",
    "Test: Generate migration log",
    "Output: migration.log contains migration details",
    "Test: Performance benchmark",
    "Output: Migration completes in <500ms"
]

why = """
Migrate old config format to new format.
Preserve user customizations during upgrade.

Without migration: Existing users lose customizations, must reconfigure manually.
With migration: Seamless upgrade, all customizations preserved automatically.
"""

context = """
ConfigMigration converts old Ã†therLight configs to new self-config format.

Old formats:
- v0: No config, manual setup
- v1: .claude/config.json (custom format)
- v2: project-config.json (self-config format) â† NEW

Migration paths:
1. **v0 â†’ v2**: No old config, run detection + interview (init flow)
2. **v1 â†’ v2**: Parse old config, map to new format, preserve customizations

Example v1 config (.claude/config.json):
```json
{
  "buildCommand": "npm run build:prod",
  "testCommand": "npm run test:unit",
  "language": "typescript"
}
```

Example v2 config (.aetherlight/project-config.json):
```json
{
  "language": "typescript",
  "BUILD_COMMAND": "npm run build:prod",
  "TEST_COMMAND": "npm run test:unit",
  "packageManager": "npm",
  "configVersion": "2",
  "schemaVersion": "v1"
}
```

Migration process:
1. Detect old config version
2. Load old config
3. Apply version-specific migration
4. Map old fields to new fields
5. Preserve user customizations
6. Validate against schema
7. Write new config
8. Generate migration log

Related services:
- ProjectConfigGenerator (SELF-002) - Generates new config
- project-config-v1.json schema (SELF-003) - Validates new config
- VersionTracker (SELF-017) - Determines migration path
"""

reasoning_chain = [
    "1. Create ConfigMigration.ts service",
    "2. Implement detectOldConfig() method:",
    "   - Check for .claude/config.json (v1)",
    "   - Check for other old formats",
    "   - Return {version, path, config}",
    "3. Implement migrate() method:",
    "   - Input: oldConfig, oldVersion",
    "   - Output: newConfig (project-config.json format)",
    "   - Determine migration path (v0â†’v2, v1â†’v2)",
    "   - Call version-specific migration",
    "4. Implement migrateV1ToV2() method:",
    "   - Parse old config.json",
    "   - Map fields:",
    "     - buildCommand â†’ BUILD_COMMAND",
    "     - testCommand â†’ TEST_COMMAND",
    "     - language â†’ language",
    "   - Add missing fields (packageManager, defaults)",
    "   - Validate against schema",
    "5. Implement preserveCustomizations() method:",
    "   - Identify user customizations (non-default values)",
    "   - Ensure customizations in new config",
    "   - Example: User set BUILD_COMMAND='custom' â†’ preserve",
    "6. Implement validateMigration() method:",
    "   - Call schema validator (SELF-003)",
    "   - Check required fields present",
    "   - Check value types correct",
    "   - Return validation result",
    "7. Implement generateMigrationLog() method:",
    "   - Log old config â†’ new config mapping",
    "   - Log preserved customizations",
    "   - Log any warnings/errors",
    "   - Write to .aetherlight/migration.log",
    "8. Write tests (TDD):",
    "   - Mock v1 config â†’ migrate to v2",
    "   - Mock user customizations â†’ preserved",
    "   - Mock invalid migration â†’ validation fails",
    "   - Performance test: migration <500ms",
    "9. Coverage: â‰¥90%",
    "10. Integration with Upgrade command (SELF-016)"
]

success_impact = """
After SELF-018 complete:
âœ… Config migration service working
âœ… Old format detection (v0, v1)
âœ… New format conversion (project-config.json)
âœ… User customizations preserved
âœ… Version-specific migrations (v1â†’v2)
âœ… Migration log generated
âœ… Schema validation (post-migration)
âœ… Performance target met (<500ms)
âœ… 90% test coverage (infrastructure standard)
âœ… Existing users can upgrade without losing customizations
"""

[tasks.SELF-019]
id = "SELF-019"
name = "Implement backup/rollback mechanism"
phase = "phase-5-migration"
assigned_engineer = "engineer_1"
status = "pending"
description = "Implement BackupManager service: create backups before upgrades (.aetherlight/backups/), rollback on failure, automatic cleanup of old backups. Copy config files, templates, customizations. Restore on rollback. 90% coverage."
estimated_lines = 400
estimated_time = "6-8 hours"
dependencies = []
agent = "infrastructure-agent"

deliverables = [
    "BackupManager service with backup() and rollback()",
    "Backup creation (.aetherlight/backups/pre-upgrade-vX.Y.Z/)",
    "Rollback mechanism (restore from backup)",
    "Automatic cleanup (keep last 5 backups)",
    "Backup verification (checksum validation)",
    "Unit tests (90% coverage)"
]

performance_target = "Backup creation < 200ms"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/src/services/BackupManager.ts",
    "vscode-lumina/test/services/BackupManager.test.ts"
]

validation_criteria = [
    "Run: npm test -- BackupManager.test.ts",
    "Output: All tests pass (90% coverage)",
    "Test: Create backup before upgrade",
    "Output: Backup created at .aetherlight/backups/pre-upgrade-v2.0.0/",
    "Test: Rollback to backup",
    "Output: Config files restored from backup",
    "Test: Automatic cleanup (6th backup triggers cleanup)",
    "Output: Oldest backup deleted, 5 backups remain",
    "Test: Backup verification",
    "Output: Checksums validate backup integrity",
    "Test: Performance benchmark",
    "Output: Backup creation <200ms"
]

why = """
Safety net: if upgrade fails, rollback to previous state.
User data protection.

Without backup: Upgrade failure loses user data, no recovery.
With backup: Safe upgrades, instant rollback on failure, zero data loss.
"""

context = """
BackupManager provides data protection during upgrades.

Backup structure (.aetherlight/backups/):
```
backups/
  â”œâ”€â”€ pre-upgrade-v2.0.0/
  â”‚   â”œâ”€â”€ project-config.json
  â”‚   â”œâ”€â”€ version.json
  â”‚   â”œâ”€â”€ CLAUDE.md
  â”‚   â””â”€â”€ checksums.json
  â”œâ”€â”€ pre-upgrade-v1.5.0/
  â””â”€â”€ pre-upgrade-v1.0.0/
```

Backup process:
1. Create timestamped backup directory
2. Copy all .aetherlight/ files
3. Copy customized templates
4. Generate checksums for validation
5. Write backup metadata

Rollback process:
1. Verify backup integrity (checksums)
2. Delete current config files
3. Restore files from backup
4. Verify restoration successful
5. Update version.json

Automatic cleanup:
- Keep last 5 backups
- Delete oldest when 6th backup created
- User can manually delete backups

Related services:
- Upgrade command (SELF-016) - Creates backup before upgrade
- ConfigMigration (SELF-018) - Triggers rollback on migration failure
"""

reasoning_chain = [
    "1. Create BackupManager.ts service",
    "2. Implement backup() method:",
    "   - Input: backupName (e.g., 'pre-upgrade-v2.0.0')",
    "   - Output: backupPath",
    "   - Create backup directory: .aetherlight/backups/{backupName}/",
    "   - Copy project-config.json",
    "   - Copy version.json",
    "   - Copy customized templates (CLAUDE.md, skills, agents)",
    "   - Generate checksums.json for validation",
    "   - Write backup metadata (timestamp, version, reason)",
    "3. Implement rollback() method:",
    "   - Input: backupName",
    "   - Output: success/failure",
    "   - Verify backup integrity (check checksums)",
    "   - Delete current config files",
    "   - Restore files from backup directory",
    "   - Verify restoration (compare checksums)",
    "   - Update version.json to backup version",
    "4. Implement listBackups() method:",
    "   - Scan .aetherlight/backups/",
    "   - Return array of backups sorted by timestamp",
    "5. Implement cleanupOldBackups() method:",
    "   - Keep last 5 backups",
    "   - Delete oldest backups if > 5",
    "6. Implement verifyBackup() method:",
    "   - Read checksums.json",
    "   - Calculate checksums of backup files",
    "   - Compare and return validation result",
    "7. Write tests (TDD):",
    "   - Mock backup creation â†’ files copied",
    "   - Mock rollback â†’ files restored",
    "   - Mock cleanup â†’ oldest backup deleted",
    "   - Mock verification â†’ checksums validated",
    "   - Performance test: backup <200ms",
    "8. Coverage: â‰¥90%",
    "9. Integration with Upgrade command",
    "10. Ready for Phase 5 integration tests (SELF-020)"
]

success_impact = """
After SELF-019 complete:
âœ… Backup/rollback mechanism working
âœ… Backup creation before upgrades
âœ… Rollback on failure (instant recovery)
âœ… Automatic cleanup (keep last 5)
âœ… Backup verification (checksum validation)
âœ… Performance target met (<200ms)
âœ… 90% test coverage (infrastructure standard)
âœ… Zero data loss during upgrades
"""

[tasks.SELF-020]
id = "SELF-020"
name = "Phase 5 integration tests"
phase = "phase-5-migration"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create end-to-end integration tests for Phase 5 Migration: Upgrade command (SELF-016) + VersionTracker (SELF-017) + ConfigMigration (SELF-018) + BackupManager (SELF-019). Test full upgrade flow v1â†’v2, rollback on failure, user confirmation. 85% coverage."
estimated_lines = 600
estimated_time = "6-8 hours"
dependencies = ["SELF-016", "SELF-017", "SELF-018", "SELF-019"]
agent = "infrastructure-agent"

deliverables = [
    "E2E upgrade test suite (upgrade.test.ts)",
    "Full upgrade flow tests (v1â†’v2)",
    "Rollback tests (upgrade fails, backup restored)",
    "User confirmation tests (cancel upgrade)",
    "Config preservation tests (customizations maintained)",
    "Performance benchmarks (upgrade <10s)",
    "Coverage report (â‰¥85%)"
]

performance_target = "Full upgrade flow < 10s"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/e2e/upgrade.test.ts"
]

validation_criteria = [
    "Run: npm test -- upgrade.test.ts",
    "Output: All E2E upgrade tests pass",
    "Test: Full upgrade flow (v1â†’v2)",
    "Output: Backup created, migration runs, config updated, version updated",
    "Test: Rollback on failure",
    "Output: Migration fails, backup restored, no data loss",
    "Test: User cancels upgrade",
    "Output: No changes made, exit gracefully",
    "Test: Customizations preserved",
    "Output: User's custom BUILD_COMMAND maintained after upgrade",
    "Test: Performance benchmark",
    "Output: Full upgrade completes in <10s",
    "Check: Coverage report",
    "Output: â‰¥85% coverage for Phase 5 services"
]

why = """
Test full upgrade flow end-to-end.

Without E2E tests: Upgrade components work alone but fail when integrated.
With E2E tests: Full upgrade pipeline validated, safe for production.
"""

context = """
Phase 5 E2E tests validate the complete upgrade pipeline.

Test scenarios:
1. **Successful Upgrade (v1â†’v2)**
   - Start with v1 config
   - Run 'lumina upgrade'
   - User confirms
   - Backup created
   - Migration runs
   - Config updated to v2 format
   - Version updated
   - Templates re-customized
   - Validation: All files correct, customizations preserved

2. **Rollback on Failure**
   - Start with v1 config
   - Run 'lumina upgrade'
   - User confirms
   - Backup created
   - Migration fails (simulate error)
   - Rollback triggered
   - Backup restored
   - Validation: Original config restored, no data loss

3. **User Cancels Upgrade**
   - Start with v1 config
   - Run 'lumina upgrade'
   - User cancels at confirmation
   - No changes made
   - Validation: Config unchanged

4. **Customizations Preserved**
   - Start with v1 config with custom BUILD_COMMAND
   - Run 'lumina upgrade'
   - Migration runs
   - Validation: Custom BUILD_COMMAND in v2 config

5. **Performance**
   - Run full upgrade 10 times
   - Average time < 10s

Related services:
- Upgrade command (SELF-016) - Orchestration
- VersionTracker (SELF-017) - Version detection
- ConfigMigration (SELF-018) - Config conversion
- BackupManager (SELF-019) - Data protection
"""

reasoning_chain = [
    "1. Create test/e2e/upgrade.test.ts",
    "2. Setup test fixtures:",
    "   - Mock v1 config (.claude/config.json)",
    "   - Mock v2 templates (.aetherlight/templates/)",
    "3. Test 1: Successful upgrade",
    "   - Create v1 config",
    "   - Run upgrade command",
    "   - Mock user confirms",
    "   - Assert: Backup created at .aetherlight/backups/",
    "   - Assert: Migration runs, v2 config created",
    "   - Assert: Version updated to v2",
    "   - Assert: Templates re-customized",
    "4. Test 2: Rollback on failure",
    "   - Create v1 config",
    "   - Run upgrade command",
    "   - Mock migration failure (throw error)",
    "   - Assert: Rollback triggered",
    "   - Assert: Backup restored",
    "   - Assert: v1 config intact",
    "5. Test 3: User cancels",
    "   - Create v1 config",
    "   - Run upgrade command",
    "   - Mock user cancels",
    "   - Assert: No changes made",
    "   - Assert: Config unchanged",
    "6. Test 4: Customizations preserved",
    "   - Create v1 config with custom BUILD_COMMAND='custom'",
    "   - Run upgrade",
    "   - Assert: v2 config has BUILD_COMMAND='custom'",
    "7. Test 5: Performance",
    "   - Run upgrade 10 times",
    "   - Calculate average time",
    "   - Assert: <10s average",
    "8. Generate coverage report",
    "9. Assert: â‰¥85% coverage for Phase 5 services",
    "10. All tests pass â†’ Phase 5 complete"
]

success_impact = """
After SELF-020 complete:
âœ… Phase 5 E2E tests passing
âœ… Full upgrade flow validated (v1â†’v2)
âœ… Rollback tested (backup restored on failure)
âœ… User confirmation tested (cancel works)
âœ… Customizations preserved (no data loss)
âœ… Performance target met (<10s upgrade)
âœ… 85% coverage for Phase 5 services
âœ… Confidence HIGH for Phase 5 completion
âœ… Ready to proceed to Phase 6 (Testing & Polish)
"""

# =============================================================================
# PHASE 6: TESTING & POLISH - MEDIUM PRIORITY
# =============================================================================

[tasks.SELF-021]
id = "SELF-021"
name = "Comprehensive integration tests"
phase = "phase-6-testing"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create comprehensive integration tests combining all 5 phases: Phase 2 (Foundation), Phase 3 (Detection), Phase 4 (Interview/Config), Phase 5 (Migration). Test matrix of 10+ project types (TypeScript, Rust, Python, Go, etc.). Full end-to-end system validation. 85% coverage."
estimated_lines = 900
estimated_time = "12-16 hours"
dependencies = ["SELF-005", "SELF-010", "SELF-015", "SELF-020"]
agent = "infrastructure-agent"

deliverables = [
    "Full system integration test suite (fullSystem.test.ts)",
    "Test matrix: 10+ project types (TypeScript, Rust, Python, Go, etc.)",
    "Cross-phase integration tests (detectionâ†’interviewâ†’configâ†’templates)",
    "Error resilience tests (graceful degradation)",
    "Performance benchmarks (full system <5s)",
    "Test report (HTML + JSON)",
    "Coverage report (â‰¥85%)"
]

performance_target = "Full system test suite < 2 minutes"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/integration/fullSystem.test.ts"
]

validation_criteria = [
    "Run: npm test -- fullSystem.test.ts",
    "Output: All integration tests pass",
    "Test: TypeScript + Jest + GitHub Actions project",
    "Output: Full self-config pipeline works",
    "Test: Rust + Cargo + cargo test project",
    "Output: Full self-config pipeline works",
    "Test: Python + pip + pytest project",
    "Output: Full self-config pipeline works",
    "Test: Error resilience (detection fails)",
    "Output: Falls back to interview, completes successfully",
    "Test: Performance benchmark",
    "Output: Full system <5s for all project types",
    "Check: Test report generated",
    "Output: HTML report with pass/fail matrix",
    "Check: Coverage report",
    "Output: â‰¥85% coverage across all phases"
]

why = """
Test all phases together as complete system.
Catch integration issues missed by unit tests.

Without full system tests: Individual phases work but system fails in production.
With full system tests: Complete pipeline validated, production-ready confidence.
"""

context = """
Comprehensive integration tests validate the entire self-configuration system.

Test matrix (10+ project types):
1. **TypeScript + npm + Jest**
2. **TypeScript + yarn + Vitest**
3. **JavaScript + npm + Mocha**
4. **Rust + Cargo + cargo test**
5. **Python + pip + pytest**
6. **Python + poetry + pytest**
7. **Go + go mod + go test**
8. **Java + Maven + JUnit**
9. **Monorepo (multiple sub-projects)**
10. **Greenfield (empty project)**
11. **Legacy (no config files)**

For each project type:
1. Run detection (Phase 3)
2. Run interview (Phase 4)
3. Generate config (Phase 4)
4. Customize templates (Phase 4)
5. Validate all variables resolved
6. Check performance (<5s)

Error resilience tests:
- Detection fails â†’ Falls back to interview
- Interview cancelled â†’ Uses detection + defaults
- Missing template â†’ Skips, logs warning
- Invalid config â†’ Validation error, clear message

Performance benchmarks:
- Detection: <500ms
- Interview: <4s (user interaction)
- Config generation: <100ms
- Template customization: <200ms
- Total: <5s (full pipeline)

Related phases:
- Phase 2 (SELF-005) - Foundation tests
- Phase 3 (SELF-010) - Detection tests
- Phase 4 (SELF-015) - Interview/Config tests
- Phase 5 (SELF-020) - Migration tests
"""

reasoning_chain = [
    "1. Create test/integration/fullSystem.test.ts",
    "2. Setup test fixtures for 10+ project types",
    "3. For each project type:",
    "   - Create fixture (package.json, tsconfig.json, etc.)",
    "   - Run init flow",
    "   - Assert: Detection correct",
    "   - Assert: Interview questions appropriate",
    "   - Assert: Config generated",
    "   - Assert: Templates customized",
    "   - Assert: All variables resolved",
    "   - Assert: Performance <5s",
    "4. Test 1: TypeScript + npm + Jest",
    "   - Fixture: package.json with jest, tsconfig.json",
    "   - Run init",
    "   - Assert: language='typescript', packageManager='npm', testFramework='jest'",
    "5. Test 2-10: Repeat for other project types",
    "6. Test error resilience:",
    "   - Mock detection failure â†’ Falls back to interview",
    "   - Mock interview cancelled â†’ Uses detection + defaults",
    "   - Mock missing template â†’ Skips, logs warning",
    "7. Generate test report:",
    "   - HTML matrix: project types Ã— test results",
    "   - JSON report for CI/CD",
    "8. Generate coverage report",
    "9. Assert: â‰¥85% coverage across all phases",
    "10. All tests pass â†’ System ready for production"
]

success_impact = """
After SELF-021 complete:
âœ… Comprehensive integration tests passing
âœ… Test matrix: 10+ project types validated
âœ… Cross-phase integration tested (all 5 phases)
âœ… Error resilience validated (graceful degradation)
âœ… Performance benchmarks met (<5s full system)
âœ… Test report generated (HTML + JSON)
âœ… 85% coverage across all phases
âœ… Production-ready confidence HIGH
âœ… Ready for real-world testing (SELF-022)
"""

[tasks.SELF-022]
id = "SELF-022"
name = "End-to-end tests (real projects)"
phase = "phase-6-testing"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create E2E tests on real open-source projects (not test fixtures): clone popular GitHub repos, run init flow, validate results. Test 5+ projects: TypeScript (VS Code Extension), Rust (ripgrep), Python (Flask), Go (Hugo), JavaScript (Express). Validates production readiness."
estimated_lines = 500
estimated_time = "8-10 hours"
dependencies = ["SELF-021"]
agent = "infrastructure-agent"

deliverables = [
    "E2E real project test suite (realProjects.test.ts)",
    "Test against 5+ popular open-source repos",
    "Clone and test automation",
    "Result validation (config correctness)",
    "All tests passing",
    "Real-world accuracy report"
]

performance_target = "Real project tests < 5 minutes total"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/e2e/realProjects.test.ts"
]

validation_criteria = [
    "Run: npm test -- realProjects.test.ts",
    "Output: All E2E real project tests pass",
    "Test: VS Code Extension (TypeScript)",
    "Output: Detected typescript, npm, jest correctly",
    "Test: ripgrep (Rust)",
    "Output: Detected rust, cargo, cargo-test correctly",
    "Test: Flask (Python)",
    "Output: Detected python, pip, pytest correctly",
    "Test: Hugo (Go)",
    "Output: Detected go, go-mod, go-test correctly",
    "Test: Express (JavaScript)",
    "Output: Detected javascript, npm, mocha correctly",
    "Manual: Review configs for accuracy",
    "Output: All configs match project reality"
]

why = """
Test on real open-source projects (not test fixtures).
Validates system works in production scenarios.

Without real project tests: System works on fixtures but fails on real code.
With real project tests: Production validation, high confidence deployment.
"""

context = """
E2E real project tests validate self-configuration on actual open-source projects.

Test projects (5+ popular repos):
1. **VS Code Extension** (TypeScript + npm + Jest)
   - Repo: microsoft/vscode-extension-samples
   - Expected: language='typescript', packageManager='npm', testFramework='jest'

2. **ripgrep** (Rust + Cargo + cargo test)
   - Repo: BurntSushi/ripgrep
   - Expected: language='rust', packageManager='cargo', testFramework='cargo-test'

3. **Flask** (Python + pip + pytest)
   - Repo: pallets/flask
   - Expected: language='python', packageManager='pip', testFramework='pytest'

4. **Hugo** (Go + go mod + go test)
   - Repo: gohugoio/hugo
   - Expected: language='go', packageManager='go-mod', testFramework='go-test'

5. **Express** (JavaScript + npm + mocha)
   - Repo: expressjs/express
   - Expected: language='javascript', packageManager='npm', testFramework='mocha'

Test workflow:
1. Clone repo to temp directory
2. Run init flow
3. Validate detection accuracy
4. Validate config correctness
5. Cleanup temp directory

Accuracy metrics:
- Language detection: 100% accuracy
- Package manager detection: 100% accuracy
- Test framework detection: â‰¥90% accuracy
- Build tool detection: â‰¥80% accuracy

Related tests:
- SELF-021: Integration tests (fixtures)
- This task: E2E tests (real projects)
"""

reasoning_chain = [
    "1. Create test/e2e/realProjects.test.ts",
    "2. Setup test automation:",
    "   - Clone GitHub repo to temp dir",
    "   - Run init flow",
    "   - Validate results",
    "   - Cleanup temp dir",
    "3. Test 1: VS Code Extension (TypeScript)",
    "   - Clone: microsoft/vscode-extension-samples",
    "   - Run init",
    "   - Assert: language='typescript', packageManager='npm', testFramework='jest'",
    "4. Test 2: ripgrep (Rust)",
    "   - Clone: BurntSushi/ripgrep",
    "   - Run init",
    "   - Assert: language='rust', packageManager='cargo', testFramework='cargo-test'",
    "5. Test 3: Flask (Python)",
    "   - Clone: pallets/flask",
    "   - Run init",
    "   - Assert: language='python', packageManager='pip', testFramework='pytest'",
    "6. Test 4: Hugo (Go)",
    "   - Clone: gohugoio/hugo",
    "   - Run init",
    "   - Assert: language='go', packageManager='go-mod', testFramework='go-test'",
    "7. Test 5: Express (JavaScript)",
    "   - Clone: expressjs/express",
    "   - Run init",
    "   - Assert: language='javascript', packageManager='npm', testFramework='mocha'",
    "8. Calculate accuracy metrics:",
    "   - Language detection: 5/5 = 100%",
    "   - Package manager detection: 5/5 = 100%",
    "   - Test framework detection: â‰¥4.5/5 = â‰¥90%",
    "9. Generate accuracy report",
    "10. All tests pass â†’ Production ready"
]

success_impact = """
After SELF-022 complete:
âœ… E2E real project tests passing
âœ… Tested on 5+ popular open-source repos
âœ… Language detection: 100% accuracy
âœ… Package manager detection: 100% accuracy
âœ… Test framework detection: â‰¥90% accuracy
âœ… Real-world accuracy validated
âœ… Production readiness confirmed
âœ… Confidence VERY HIGH for deployment
"""

[tasks.SELF-023]
id = "SELF-023"
name = "Performance optimization"
phase = "phase-6-testing"
assigned_engineer = "engineer_1"
status = "pending"
description = "Profile and optimize self-configuration init to meet <5s target. Identify bottlenecks in detection, interview, config generation. Implement caching strategies, lazy loading, parallel processing. Create performance benchmarks and regression tests. Ensure all performance targets met before release."
estimated_lines = 300
estimated_time = "8-10 hours"
dependencies = ["SELF-021"]
agent = "infrastructure-agent"

deliverables = [
    "Performance benchmarks (perf.test.ts)",
    "Optimization of hot paths (caching, lazy loading)",
    "Performance regression test suite",
    "Performance optimization report",
    "All performance targets met (<5s init)"
]

performance_target = "Self-configuration init completes < 5 seconds (including detection, interview, config generation)"

patterns = ["Pattern-TDD-001"]

files_to_modify = [
    "vscode-lumina/test/performance/init.perf.test.ts",
    "vscode-lumina/src/services/TechStackDetector.ts",
    "vscode-lumina/src/services/ToolDetector.ts",
    "vscode-lumina/src/services/WorkflowDetector.ts",
    "vscode-lumina/src/services/DomainDetector.ts",
    "vscode-lumina/src/commands/selfConfigInit.ts"
]

validation_criteria = [
    "Run: npm run test:perf -- init.perf.test.ts",
    "Output: Init completes < 5 seconds for all test scenarios",
    "Check: Performance baseline established (avg, p50, p95, p99)",
    "Check: Hot paths identified and optimized (profiling report)",
    "Check: Caching reduces repeat operations by 80%+",
    "Check: Lazy loading defers non-critical work",
    "Check: Parallel detection runs concurrently",
    "Check: Performance regression tests pass",
    "Check: No performance degradation from SELF-021 baseline"
]

why = """
Self-configuration init MUST complete in <5 seconds per user requirement.
Without optimization: detection might scan entire workspace (slow), interview blocks on I/O, config generation is synchronous.
With optimization: caching reduces redundant work, lazy loading defers non-critical tasks, parallel processing maximizes throughput.
Performance regression tests ensure future changes don't degrade performance.
"""

context = """
Self-configuration init involves multiple phases:
1. Detection (TechStackDetector, ToolDetector, WorkflowDetector, DomainDetector)
2. Interview (CLI prompts with inquirer.js)
3. Config generation (JSON + TOML output)
4. Template customization (variable resolution)

Each phase has potential bottlenecks:
- Detection: File scanning, regex matching, dependency parsing
- Interview: User input (not optimizable)
- Config generation: Template rendering, variable resolution
- File I/O: Reading templates, writing configs

Performance targets:
- Overall init: <5 seconds (hard constraint)
- Detection phase: <2 seconds
- Config generation: <1 second
- Template customization: <1 second
- Interview: Variable (user-dependent)

Optimization strategies:
- Caching: Cache workspace scans, dependency trees, detection results
- Lazy loading: Defer non-critical work (analytics, telemetry)
- Parallel processing: Run detectors concurrently
- Incremental updates: Only re-detect changed files
- Early exit: Stop detection once confident

Related tasks:
- SELF-021: Comprehensive integration tests (establishes baseline)
- SELF-022: E2E tests (validates real-world performance)
- SELF-025: Final validation (confirms targets met)
"""

reasoning_chain = [
    "1. Establish performance baseline (run SELF-021 tests, measure current performance)",
    "2. Profile init flow (identify hot paths using Node.js profiler)",
    "3. Analyze bottlenecks (which operations take most time?)",
    "4. Design optimization strategy (caching? lazy loading? parallelization?)",
    "5. Implement caching layer (workspace scan cache, dependency tree cache)",
    "6. Implement lazy loading (defer non-critical work until after init)",
    "7. Implement parallel detection (run all 4 detectors concurrently)",
    "8. Optimize file I/O (batch reads, minimize writes)",
    "9. Create performance benchmarks (perf.test.ts with various project sizes)",
    "10. Create regression tests (ensure optimizations don't break functionality)",
    "11. Measure optimized performance (compare to baseline)",
    "12. Generate optimization report (before/after metrics, strategy explanation)",
    "13. Verify all performance targets met (<5s init, <2s detection, etc.)",
    "14. Document optimization strategies (for future maintenance)"
]

success_impact = """
After SELF-023 complete:
âœ… Self-configuration init completes < 5 seconds (meets hard constraint)
âœ… Detection phase < 2 seconds (fast workspace analysis)
âœ… Config generation < 1 second (instant config output)
âœ… Performance baselines established (p50, p95, p99 metrics)
âœ… Hot paths optimized (caching, lazy loading, parallelization)
âœ… Performance regression tests prevent future degradation
âœ… Optimization report documents strategies and results
âœ… System ready for production performance requirements

Enables:
- Fast user experience (no waiting for init)
- Scalability to large workspaces (10k+ files)
- Competitive advantage (fastest project setup in industry)
- Confidence in production deployment (performance tested)
"""

[tasks.SELF-024]
id = "SELF-024"
name = "Documentation: Self-Configuration Guide"
phase = "phase-6-testing"
assigned_engineer = "engineer_1"
status = "pending"
description = "Create comprehensive documentation for self-configuration system. Write user guide covering init flow, customization, upgrade, troubleshooting. Document all project-config.json variables with examples. Create domain creation guide for extending language/framework support. Include screenshots, CLI output examples, and common workflows."
estimated_lines = 800
estimated_time = "8-10 hours"
dependencies = []
agent = "documentation-agent"

deliverables = [
    "Self-configuration user guide (docs/self-configuration-guide.md)",
    "Config reference documentation (docs/project-config-reference.md)",
    "Domain creation guide (docs/creating-domains-guide.md)",
    "Troubleshooting section with common issues",
    "Screenshots and CLI examples"
]

performance_target = "Documentation completeness score â‰¥ 95% (all public APIs documented)"

patterns = ["Pattern-DOCS-001"]

files_to_modify = [
    "docs/self-configuration-guide.md",
    "docs/project-config-reference.md",
    "docs/creating-domains-guide.md",
    "README.md"
]

validation_criteria = [
    "Check: Self-configuration guide covers all workflows (init, customize, upgrade)",
    "Check: project-config.json reference documents all 50+ variables",
    "Check: Each variable has: description, type, default, example, related variables",
    "Check: Domain creation guide includes step-by-step process",
    "Check: Troubleshooting section covers common issues (5+ scenarios)",
    "Check: Screenshots show CLI in action (3+ screenshots)",
    "Check: Code examples are tested and working",
    "Check: Links between docs are valid (no broken links)",
    "Check: Documentation follows Ã†therLight style guide"
]

why = """
Users need comprehensive documentation to understand self-configuration system.
Without docs: users struggle with init, can't customize, don't know upgrade path, frustrated by errors.
With docs: users confidently init projects, customize templates, upgrade smoothly, troubleshoot independently.
Good documentation reduces support burden and increases user satisfaction.
"""

context = """
Self-configuration documentation covers 3 main areas:

1. **Self-Configuration Guide** (docs/self-configuration-guide.md):
   - What is self-configuration? (purpose, benefits, philosophy)
   - Quick start (init command, basic flow)
   - Init workflow (detection â†’ interview â†’ config generation)
   - Customization (editing project-config.json, overriding templates)
   - Upgrade workflow (aetherlight upgrade, migration, rollback)
   - Advanced topics (custom domains, variable resolution, template syntax)
   - Troubleshooting (common issues, error messages, workarounds)

2. **Config Reference** (docs/project-config-reference.md):
   - project-config.json schema overview
   - All 50+ variables documented:
     - language.primary, language.version
     - build.command, build.output_dir
     - test.command, test.coverage_threshold
     - paths.src, paths.tests, paths.docs
     - framework.name, framework.version
     - tools.package_manager, tools.linter, tools.formatter
     - workflow.git_hooks, workflow.ci_cd
     - domain_specific (framework-specific variables)
   - Each variable:
     - Description (what it does)
     - Type (string, number, boolean, array)
     - Default value
     - Example (realistic usage)
     - Related variables (cross-references)
   - Variable resolution rules ({{VAR}} syntax, fallbacks)

3. **Domain Creation Guide** (docs/creating-domains-guide.md):
   - What are domains? (language/framework-specific configs)
   - Domain structure (detection rules, interview questions, templates)
   - Creating a new domain (step-by-step):
     1. Create domain directory (.aetherlight/domains/my-framework/)
     2. Define detection rules (how to identify this framework)
     3. Define interview questions (what to ask user)
     4. Create templates (CLAUDE.md, agent contexts, etc.)
     5. Test domain (validation, edge cases)
   - Example: Creating a "django" domain (Python web framework)
   - Contributing domains (submitting to Ã†therLight repo)

Documentation style:
- Conversational tone (approachable, not academic)
- Code examples for every concept
- Screenshots of CLI in action
- Tables for variable references
- Callouts for tips, warnings, gotchas
- Cross-references between docs (links)
- Searchable (good heading hierarchy)

Related tasks:
- SELF-003: project-config.json schema (documented in config reference)
- SELF-013: Template customization (documented in guide)
- SELF-016: Upgrade command (documented in guide)
"""

reasoning_chain = [
    "1. Review all completed SELF tasks (understand what needs documentation)",
    "2. Outline self-configuration guide structure (sections, flow)",
    "3. Write Quick Start section (get users productive quickly)",
    "4. Write Init Workflow section (detection, interview, config generation)",
    "5. Write Customization section (editing configs, overriding templates)",
    "6. Write Upgrade section (version migration, rollback)",
    "7. Write Troubleshooting section (common issues, error messages)",
    "8. Outline config reference structure (alphabetical? by category?)",
    "9. Document all 50+ variables (description, type, default, example)",
    "10. Create variable cross-reference table (related variables)",
    "11. Outline domain creation guide structure",
    "12. Write domain structure section (detection, interview, templates)",
    "13. Write step-by-step domain creation tutorial",
    "14. Create example domain (django or similar)",
    "15. Take screenshots of CLI in action (init, customize, upgrade)",
    "16. Test all code examples (ensure they work)",
    "17. Validate all links (no broken references)",
    "18. Review for completeness (all workflows covered?)"
]

success_impact = """
After SELF-024 complete:
âœ… Comprehensive self-configuration user guide (all workflows documented)
âœ… Complete config reference (all 50+ variables documented)
âœ… Domain creation guide (enables community extensions)
âœ… Troubleshooting section (reduces support burden)
âœ… Screenshots and examples (visual learners covered)
âœ… Documentation completeness â‰¥ 95% (all public APIs)
âœ… Users confidently use self-configuration system
âœ… Reduced support requests (self-service documentation)

Enables:
- User self-sufficiency (can learn without support)
- Community contributions (domain creation guide)
- Faster onboarding (good docs = quick start)
- Confidence in production (documented workflows)
- Open source readiness (external contributors can learn)
"""

[tasks.SELF-025]
id = "SELF-025"
name = "Final validation & release preparation"
phase = "phase-6-testing"
assigned_engineer = "engineer_1"
status = "pending"
description = "Final quality gate before v1.0.0 release. Run comprehensive validation across all areas: tests pass, performance targets met, documentation complete, security audit clean, no critical bugs. Generate final validation report. Draft v1.0.0 release notes. Confirm release readiness with all stakeholders. Tag release candidate."
estimated_lines = 200
estimated_time = "8-10 hours"
dependencies = ["SELF-021", "SELF-022", "SELF-023", "SELF-024"]
agent = "infrastructure-agent"

deliverables = [
    "Final validation report (comprehensive checklist)",
    "All quality gates passed (tests, performance, docs, security)",
    "Release readiness confirmed (stakeholder approval)",
    "v1.0.0 release notes drafted",
    "Release candidate tagged (git tag v1.0.0-rc.1)"
]

performance_target = "All performance targets met (init <5s, detection <2s, config generation <1s)"

patterns = ["Pattern-PUBLISH-001", "Pattern-PUBLISH-002", "Pattern-TDD-001"]

files_to_modify = [
    "internal/validation/SELF_CONFIG_VALIDATION_REPORT.md",
    "CHANGELOG.md",
    "RELEASE_NOTES_v1.0.0.md"
]

validation_criteria = [
    "Run: npm test -- all self-config tests",
    "Output: 100% tests passing (no failures, no skipped)",
    "Run: npm run test:coverage",
    "Output: Coverage â‰¥ 85% for self-config modules",
    "Run: npm run test:perf",
    "Output: Init <5s, detection <2s, config generation <1s (all targets met)",
    "Check: Documentation completeness â‰¥ 95% (all workflows documented)",
    "Check: Security audit clean (no critical vulnerabilities)",
    "Check: No P0/P1 bugs open in issue tracker",
    "Check: All SELF tasks completed (SELF-001 through SELF-024)",
    "Check: Release notes drafted with all features, fixes, breaking changes",
    "Check: Stakeholder approval obtained (user, team lead)",
    "Check: Release candidate tagged (git tag v1.0.0-rc.1)"
]

why = """
Final quality gate ensures v1.0.0 release meets production standards.
Without validation: critical bugs slip through, performance targets missed, documentation incomplete, users frustrated.
With validation: high confidence in release quality, all targets met, documentation complete, users satisfied.
Comprehensive validation prevents costly post-release hotfixes and support burden.
"""

context = """
Final validation covers 7 critical areas:

1. **Test Coverage & Quality**:
   - All tests passing (unit, integration, E2E, performance)
   - Coverage â‰¥ 85% for self-config modules
   - No skipped tests or TODOs
   - Test data covers edge cases

2. **Performance Targets**:
   - Init <5 seconds (hard constraint)
   - Detection <2 seconds
   - Config generation <1 second
   - Template customization <1 second
   - All benchmarks green

3. **Documentation Completeness**:
   - Self-configuration guide complete
   - Config reference complete (all 50+ variables)
   - Domain creation guide complete
   - Troubleshooting section complete
   - Screenshots and examples included
   - All links valid (no 404s)

4. **Security Audit**:
   - No critical vulnerabilities (npm audit)
   - No exposed secrets or credentials
   - No unsafe code patterns (SQL injection, XSS, etc.)
   - Dependencies up-to-date
   - Security best practices followed

5. **Bug Triage**:
   - No P0 (blocker) bugs open
   - No P1 (critical) bugs open
   - P2 (major) bugs documented in known issues
   - P3 (minor) bugs scheduled for next release

6. **Feature Completeness**:
   - All SELF-001 through SELF-024 tasks complete
   - Detection works for 5+ languages/frameworks
   - Interview flow intuitive (user testing)
   - Config generation accurate (validated against real projects)
   - Upgrade path works (v0.x â†’ v1.0.0)

7. **Release Preparation**:
   - CHANGELOG.md updated
   - Release notes drafted (features, fixes, breaking changes)
   - Version bumped to 1.0.0 (package.json, TOML)
   - Git tag created (v1.0.0-rc.1)
   - Stakeholder approval obtained

Validation process:
1. Run comprehensive test suite (all tests)
2. Run performance benchmarks (all targets)
3. Review documentation (completeness check)
4. Run security audit (npm audit, code scan)
5. Review bug tracker (triage P0/P1)
6. Review feature completeness (all SELF tasks)
7. Generate validation report (pass/fail per area)
8. Draft release notes (user-facing changes)
9. Obtain stakeholder approval (user, team lead)
10. Tag release candidate (git tag v1.0.0-rc.1)

Validation report format:
- Executive summary (overall readiness)
- Test results (pass/fail, coverage %)
- Performance results (benchmarks, targets met)
- Documentation results (completeness %, gaps)
- Security results (vulnerabilities, severity)
- Bug triage results (P0/P1 count, blockers)
- Feature completeness (% tasks complete)
- Recommendation (ship / no-ship)

Related patterns:
- Pattern-PUBLISH-001: Automated release pipeline
- Pattern-PUBLISH-002: Publishing enforcement
- Pattern-TDD-001: Test-driven development ratchet
"""

reasoning_chain = [
    "1. Run comprehensive test suite (npm test -- all self-config tests)",
    "2. Verify 100% tests passing (no failures, no skipped)",
    "3. Run coverage report (npm run test:coverage)",
    "4. Verify coverage â‰¥ 85% for self-config modules",
    "5. Run performance benchmarks (npm run test:perf)",
    "6. Verify all performance targets met (init <5s, detection <2s, config <1s)",
    "7. Review documentation completeness (guide, reference, domain creation)",
    "8. Verify documentation â‰¥ 95% complete (all workflows covered)",
    "9. Run security audit (npm audit --audit-level=critical)",
    "10. Verify no critical vulnerabilities (exit code 0)",
    "11. Review bug tracker (filter P0/P1 bugs)",
    "12. Verify no blocker/critical bugs open",
    "13. Review feature completeness (SELF-001 through SELF-024)",
    "14. Verify all SELF tasks completed",
    "15. Generate validation report (pass/fail per area)",
    "16. Draft release notes (features, fixes, breaking changes)",
    "17. Obtain stakeholder approval (user, team lead)",
    "18. Tag release candidate (git tag v1.0.0-rc.1)",
    "19. Make recommendation (ship / no-ship with reasoning)"
]

success_impact = """
After SELF-025 complete:
âœ… Final validation report generated (comprehensive checklist)
âœ… All quality gates passed (tests, performance, docs, security)
âœ… 100% tests passing with â‰¥85% coverage
âœ… All performance targets met (init <5s, detection <2s, config <1s)
âœ… Documentation â‰¥95% complete (all workflows covered)
âœ… Security audit clean (no critical vulnerabilities)
âœ… No blocker/critical bugs open (P0/P1 triaged)
âœ… All SELF tasks completed (SELF-001 through SELF-024)
âœ… Release notes drafted (user-facing changes documented)
âœ… Stakeholder approval obtained (confident to ship)
âœ… Release candidate tagged (v1.0.0-rc.1)
âœ… Recommendation: SHIP (high confidence)

Enables:
- v1.0.0 release (production-ready self-configuration)
- User confidence (thoroughly validated system)
- Support efficiency (comprehensive docs reduce burden)
- Future development (solid foundation for v1.x features)
- Open source readiness (quality bar for contributions)
"""

# =============================================================================
# PHASE 7: EXECUTE NORMALIZATION (AUTO-GENERATED) - FINAL
# =============================================================================

# PLACEHOLDER: This phase will be auto-generated by sprint-plan skill
# after TEMPLATE-006 completes.
#
# The sprint-plan skill will inject 27 normalized tasks here using
# SPRINT_TEMPLATE.toml, validating the template system works correctly.
#
# Expected tasks:
# - NORM-001 through NORM-027 (documentation, quality, security, etc.)
#
# This phase will be added dynamically after Phase 1 complete.

# =============================================================================
# PROGRESS TRACKING
# =============================================================================

[progress]
total_tasks = 40
completed_tasks = 1
in_progress_tasks = 0
pending_tasks = 39
completion_percentage = 3
